Improving Retrieval-Augmented Generation through
Multi-Agent Reinforcement Learning
Yiqun Chen
Renmin University of China
Beijing, China
chenyiqun990321@ruc.edu.cn
Lingyong Yan
Baidu Inc.
Beijing, China
lingyongy@gmail.com
Weiwei Sun
Carnegie Mellon University
Pittsburgh, USA
sunnweiwei@gmail.com
Xinyu Ma
Baidu Inc.
Beijing, China
xinyuma2016@gmail.com
Yi Zhang
Baidu Inc.
Beijing, China
zhangyi75@baidu.com
Shuaiqiang Wang
Baidu Inc.
Beijing, China
shqiang.wang@gmail.com
Dawei Yin
Baidu Inc.
Beijing, China
yindawei@acm.org
Yiming Yang
Carnegie Mellon University
Pittsburgh, USA
yiming@cs.cmu.edu
Jiaxin Maoâˆ—
Renmin University of China
Beijing, China
maojiaxin@gmail.com
Abstract
Retrieval-augmented generation (RAG) is extensively utilized to in-
corporate external, current knowledge into large language models,
thereby minimizing hallucinations. A standard RAG pipeline may
comprise several components, such as query rewriting, document
retrieval, document filtering, and answer generation. However,
these components are typically optimized separately through su-
pervised fine-tuning, which can lead to misalignments between
the objectives of individual modules and the overarching aim of
generating accurate answers in question-answering (QA) tasks.
Although recent efforts have explored reinforcement learning (RL)
to optimize specific RAG components, these approaches often fo-
cus on overly simplistic pipelines with only two components or
do not adequately address the complex interdependencies and col-
laborative interactions among the modules. To overcome these
challenges, we propose treating the RAG pipeline as a multi-agent
cooperative task, with each component regarded as an RL agent.
Specifically, we present MMOA-RAG, a Multi-Module joint Op-
timization Algorithm for RAG, which employs multi-agent rein-
forcement learning to harmonize all agentsâ€™ goals towards a unified
reward, such as the F1 score of the final answer. Experiments con-
ducted on various QA datasets demonstrate that MMOA-RAG im-
proves the overall pipeline performance and outperforms existing
baselines. Furthermore, comprehensive ablation studies validate
the contributions of individual components and the adaptability
of MMOA-RAG across different RAG components and datasets1.
1The code of MMOA-RAG is on https://github.com/chenyiqun/MMOA-RAG.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full
citation on the first page. Copyrights for components of this work owned by others
than the author(s) must be honored. Abstracting with credit is permitted. To copy
otherwise, or republish, to post on servers or to redistribute to lists, requires prior
specific permission and/or a fee. Request permissions from permissions@acm.org.
Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY
Â© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-XXXX-X/18/06
https://doi.org/XXXXXXX.XXXXXXX
CCS Concepts
â€¢ Computing methodologies â†’Natural language generation;
â€¢ Information systems â†’Question answering.
Keywords
Retrieval-Augmented Generation; Multi-Agent Cooperation; Multi-
Agent Reinforcement Learning; Multi-Module Joint Learning
ACM Reference Format:
Yiqun Chen, Lingyong Yan, Weiwei Sun, Xinyu Ma, Yi Zhang, Shuaiqiang
Wang, Dawei Yin, Yiming Yang, and Jiaxin Mao. 2018. Improving Retrieval-
Augmented Generation through Multi-Agent Reinforcement Learning. In
Proceedings of Make sure to enter the correct conference title from your rights
confirmation emai (Conference acronym â€™XX). ACM, New York, NY, USA,
12 pages. https://doi.org/XXXXXXX.XXXXXXX
1
Introduction
Large Language Models (LLMs) have been widely applied to tasks
such as question answering [1, 20], information retrieval [2, 38],
various forms of reasoning [11, 13], and evaluation [6, 9]. How-
ever, since LLMs cannot promptly update their internal knowledge
after pre-training, they remain prone to generating outdated or
fabricated responses [48]. To address these challenges, Retrieval-
Augmented Generation (RAG) enhances the generative capabil-
ities of LLMs by retrieving relevant information from external
knowledge sources. Recent RAG systems are often built as com-
plex pipelines comprising multiple interconnected modules [8],
including query rewriting [17, 25], first-stage retrieval [22, 34],
re-ranking [30, 31], document pre-processing [19, 24], and answer
generation [34, 37].
The complexity of RAG systems makes their optimization par-
ticularly challenging. Standard supervised fine-tuning (SFT) opti-
mizes each module independently using human-annotated data.
However, this often results in misalignment between the objec-
tives of individual components and the systemâ€™s overarching goal
of generating high-quality outputs. For example, retrieval mod-
ules are frequently trained on human-labeled relevance data to
arXiv:2501.15228v1  [cs.CL]  25 Jan 2025
Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY
Trovato et al.
optimize metrics such as nDCG[16], but this process fails to ad-
dress the disconnect between document relevance and response
qualityâ€”documents with high relevance scores do not always con-
tribute to generating accurate answers [4].
To address this issue, existing work on end-to-end optimization
for RAG, such as [10, 21, 22, 31] aims to propagate rewards from
the final output to intermediate modules using techniques like
attention distributions [15], generation probability [22, 46], and
expectation-maximization (EM) iterations [30, 36]. However, ear-
lier approaches primarily focus on simplified pipelines with only
two components-a retriever and a generator-and fail to provide a
generalizable framework for jointly optimizing complex systems
with multiple components and richer interdependencies. More
recent methods attempt to eliminate the need for module-specific
rewards by leveraging algorithms like Direct Preference Opti-
mization (DPO) [28] and Proximal Policy Optimization (PPO)[33].
Nonetheless, these methods still concentrate on optimizing individ-
ual RAG modules in isolation, without adequately modeling the col-
laborative dynamics between interacting components [19, 24, 25].
Effectively capturing interdependencies among multiple modules
and jointly optimizing complex RAG architectures remains an
open research challenge.
In this paper, we propose a novel approach called the Multi-
Module joint Optimization Algorithm (MMOA-RAG) to enable
joint optimization across multiple modules in a RAG system. Our
framework treats each intermediate component in the RAG pipeline
as an agent and formulates the optimization process as a multi-
agent collaborative reinforcement learning (RL) problem, where
the agents (i.e., modules) work together to maximize a shared
reward for the final outcome.
While MMOA-RAG is flexible with the choices reward function
and pipeline design, in this study, we define the final reward as
the correctness of the generated response, measured by the F1
score against the ground-truth answer, and apply MMOA-RAG to
a RAG pipeline comprising four modules: a query rewriter, a fixed
document retriever, a document selector, and an answer generator.
To achieve this, we employ the Multi-Agent PPO [45] algorithm,
enabling collaborative optimization within a fully cooperative
setting. In this setting, the optimization objectives of all modules
are aligned toward the ultimate goal of generating high-quality
answers.
Compared to previous approaches for RAG, MMOA-RAG allows
for the end-to-end optimization of complex RAG systems, ensuring
that each moduleâ€™s objectives align with the overarching goal of
producing accurate responses. Additionally, unlike recent methods
based on DPO [51, 52] or PPO [19, 25], our approach is more flexible
for various pipeline designs and excels at fostering collaboration
among multiple modules.
To demonstrate the effectiveness of the MMOA-RAG modeling
and optimization approach, we conducted experiments on three
publicly available QA datasets, HotpotQA [43], 2WikiMultihopQA
[12] and AmbigQA [26], based on Llama-3-8B-Instruct [5]. The
experimental results indicate that MMOA-RAG achieves better
performance than a series of existing optimization methods for
RAG. Additionally, we performed extensive ablation studies to
investigate the effectiveness and advantages of jointly optimizing
mulitple modules in the RAG system and the generalizability of
MMOA-RAG across different RAG pipelines.
Our main contributions are as follows:
â€¢ We innovatively model RAG as a multi-agent collaborative
task, treating multiple modules within the RAG pipeline as
individual agents.
â€¢ We employ a multi-agent cooperative reinforcement learning
algorithm to jointly optimize a sophisticated RAG system
with three modules: a query writer, a document selector, and
an answer generation.
â€¢ We conduct extensive experiments to verify and demonstrate
the effectiveness of the proposed framework.
2
Related Works
2.1
End-to-end optimization in OpenQA
ORQA [21] is an open-domain QA system that learns end-to-end
evidence retrieval and answer generation using only question-
answer pairs, enabled by pretraining with an Inverse Cloze Task.
REALM [10] is an end-to-end optimizing framework that enhances
language model pre-training with a retrieval-augmented approach.
Lewis et al. [22] introduces Retrieval-Augmented Generation as
RAG, a model that combines pre-trained language models with
non-parametric memory for improved performance on knowledge-
intensive NLP tasks. Izacard and Grave [15] propose a knowledge
distillation method to train retriever models using synthetic labels
derived from reader model attention scores. Stochastic RAG [46]
introduces a novel end-to-end optimization framework for RAG
through expected utility maximization.
2.2
RAG without parameters update
These methods typically involve designing a novel RAG mecha-
nism to enhance the performance of LLMs on question answering
tasks. For example, DSP [20] leverages sophisticated interactions
between retrieval and language models to address knowledge-
intensive NLP tasks. FLARE [18], an active retrieval augmented
generation method, enhances text generation by dynamically re-
trieving relevant information throughout the process, showing su-
perior performance across various long-form knowledge-intensive
tasks. ITER-RETGEN [34] is an iterative retrieval-generation syn-
ergy method that enhances retrieval-augmented large language
models by synergistically combining retrieval and generation in
an iterative manner. Search-in-the-Chain [41] is a framework that
interactively enhances Large Language Models with search capa-
bilities to improve performance on complex, knowledge-intensive
tasks. SELF-RAG [1] enhances language model quality and fac-
tuality through self-reflective retrieval and generation. DRAGIN
[37] is a dynamic RAG framework that addresses the real-time
information needs of LLMs during text generation, enhancing
performance on knowledge-intensive tasks. GenGround [35] syn-
ergizes large language model knowledge with external documents
to enhance multi-hop question answering through an iterative
process of generating answers and grounding them in evidence.
Astute RAG [39] is a approach that enhances the robustness of
Retrieval-Augmented Generation for Large Language Models by
adaptively integrating internal and external knowledge while re-
solving knowledge conflicts.
Improving Retrieval-Augmented Generation through Multi-Agent Reinforcement Learning
Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY
Initial question: É‡
Query Rewriter
Retriever
Selector
Generator
Shared Reward
Golden answer: í‘¨É„É‰í í¨í¥È†È‡È
È¬í‘¸È®
È¬È¯
È¬È£
Trainable
F1 Score
Individual Penalty
Sub-questions: É‰í’–È¸É‡
Question
Sub-question 1
Sub-question 2
Sub-question 3
Get documents set: È 
Get selected set: È í¬È‡í¥È‡È…È–È‡È†
Predicted answer: í‘¨É„É‰í©í«È‡È†È‹È…È–
Answer
È®í‘¸È®
È®È¯
È®È£
Adder
Reward of Each Agent
Freezed
ÈºÇ 
ÈºÇ 
ÈºÇ¢
ÈºÇ¡ ÈºÇ¢
ÈºÇ¡ ÈºÇ¢
ÈºÇ  ÈºÇ¡
ÈºÇ 
ÈºÇ 
ÈºÇ¢
ÈºÇ¡ ÈºÇ¢
ÈºÇ¡ ÈºÇ¢
ÈºÇ  ÈºÇ¡
ÈºÇ 
ÈºÇ¡ ÈºÇ¢
ÈºÇ¢
ÈºÇ¡
Figure 1: The overall framework of MMOA-RAG.
2.3
RAG with parameters update
2.3.1
Optimizing RAG with SFT. INFO-RAG [42], an unsupervised
training method, enhances the capacity of large language models
to integrate and refine information from retrieved texts. LongRAG
[47] introduces a dual-perspective retrieval-augmented genera-
tion system to enhance understanding of complex long-context
knowledge for improved performance in long-context question
answering tasks. In INSTRUCTRAG [40], generation accuracy and
trustworthiness are enhanced by explicitly denoising retrieved
information through self-synthesized rationales, outperforming
standard RAG approaches without additional supervision.
2.3.2
Optimizing RAG with RL. Some existing works use PPO [33]
algorithm to fine-tune LLMs. In Rewrite-Retrieve-Read framework
[25], a small language model is trained with reinforcement learning
to rewrite queries for RAG. BGM [19] proposes a novel bridge
mechanism between retrieval model and LLMs and uses PPO to
optimize the parameters of the bridge to filter for more helpful
documents. SMARTRAG [7] optimizes an iterative RAG framework
with reward, which includes a decision maker and a policy network.
RAG-Star [17] is a reasoning approach that combines Monte Carlo
Tree Search (MCTS) to improve the complex reasoning abilities of
LLMs.
Some other works use DPO [28] or similar alignment algorithms
to optimize LLMs. A noise-filtering method [52] is proposed by
optimizing mutual information between compressed data and out-
put while minimizing it with the retrieved passage. ATM [51], an
Adversarial Tuning Multi-agent system, enhances the robustness
and performance of retrieval-augmented generators in question
answering by iteratively tuning against an adversarial attacker
agent to better discriminate useful documents and resist fabricated
content. SEER [49] proposes a novel self-aligned evidence extrac-
tion learning framework aimed at enhancing RAG performance
by optimizing the extraction of high-quality, concise, and relevant
evidence. RAG-DDR [24] optimizes RAG systems by aligning data
preferences between modules through DDR, resulting in enhanced
performance on knowledge-intensive tasks.
3
Preliminary
3.1
Modeling RAG as Co-MARL
In this work, we conceptualize the RAG procedure within a cooper-
ative multi-agent reinforcement learning (Co-MARL) framework.
Within this framework, each module of the RAG pipeline functions
as an individual RL agent. The overarching objective of this multi-
agent system is to produce high-quality answers, which aligns
with the individual goals of each module.
In this context, we define the tuple âŸ¨G, O, A, RâŸ©, where G de-
notes the set of agents in the Co-MARL system, O represents the
observation information available to each agent, A constitutes the
action space accessible to each agent, and R is the reward shared
among all agents. The ultimate aim is to maximize this shared re-
ward, thereby achieving higher evaluation metrics and enhancing
the overall performance of the RAG system.
3.2
MAPPO Algorithm
In this paper we utilizes Multi-Agent PPO (MAPPO) [45], which
is an extension of the PPO algorithm [33] for multi-agent envi-
ronments, to optimize the policy for each agent in the Co-MARL
framework. In fully cooperative settings, unlike PPO, which fo-
cuses on single-agent scenarios with individual reward, MAPPO
employs a shared global reward to promote cooperation among
all agents. Additionally, a further distinction between MAPPO
and PPO lies in the input received by their critic models. In PPO,
the critic model is limited to the agentâ€™s observation information,
whereas MAPPOâ€™s global critic model can access comprehensive
global information, which enables the global critic model to more
accurately estimate the state-value function.
4
Method
4.1
Overall of MMOA-RAG
RAG systems typically follow a modular architecture composed
of multiple interconnected components. Figure 1 illustrates the
architecture of our MMOA-RAG framework, which consists of four
primary modules: the Query Rewriter, the Retriever, the Selector,
and the Generator:
Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY
Trovato et al.
â€¢ Query Rewriter reformulates the initial query ğ‘, which may
be too complex or ambiguous to resolve with a single retrieval,
into a set of sub-questions denoted as ğ‘ ğ‘¢ğ‘ğ‘.
â€¢ Retriever retrieves relevant documents from the corpus for
each sub-questions, respectively, and outputs a set of candidate
document ğ·.
â€¢ Selector further filters ğ·to obtain a subset of documents ğ·selected
that is useful for generating the final answer to the initial query
ğ‘.
â€¢ Generator leverages ğ·selected to generate the predicted answer
ğ´ğ‘›ğ‘ predict to the initial question.
Since the Query Rewriter, Selector, and Generator modules can
all be implemented using LLMs, they can be treated as RL agents
[27], enabling parameter updates through reward signals. To op-
timize computational efficiency, these three modules can share
the same LLM. Additionally, given the difficulty of modeling the
Retriever module as an RL agent, we consider the Retriever to be
part of the environment 2.
The focus of the MMOA-RAG framework is on the collaborative
optimization of multiple modules to align their individual optimiza-
tion objectives with the ultimate goal of generating high-quality
answers. We can use metrics derived from the Generatorâ€™s pre-
dicted answer ğ´ğ‘›ğ‘ predict, such as the F1 score, as a shared reward
ğ‘…shared. Given the fully cooperative nature of the modules in the
RAG system, the shared reward ğ‘…shared can be used to train all
agents, a common approach in existing MARL literature [3, 29, 45].
Additionally, to ensure training stability and accelerate conver-
gence in the multi-agent system, we design penalty terms ğ‘ƒğ‘„ğ‘…,
ğ‘ƒğ‘†, and ğ‘ƒğºfor each agent. A more detailed explanation will be
provided in Section 4.2.
4.2
Detailed Configuration for Each Agent
In this section, we will provide a detailed explanation of each ele-
ment in the tuple âŸ¨G, O, A, RâŸ©mentioned in Section 3.1. Here, G =
{Query Rewriter (QR), Selector (S), Generator (G)} represents all
agents. In the following, we introduce the essential elements for
each agent ğ‘–âˆˆG: the observation information ğ‘‚ğ‘–âˆˆO, the action
space ğ´ğ‘–âˆˆA, and the reward function ğ‘…ğ‘–.
4.2.1
Elements of Query Rewriter. Observation of Query Rewriter
is defined as Equation (1), which contains prompt of Query Rewriter
ğ‘ƒğ‘Ÿğ‘œğ‘šğ‘ğ‘¡ğ‘„ğ‘…and the initial question ğ‘.
ğ‘‚ğ‘„ğ‘…=

ğ‘ƒğ‘Ÿğ‘œğ‘šğ‘ğ‘¡ğ‘„ğ‘…,ğ‘
(1)
Action Space of Query Rewriter corresponds to the vocabulary
of LLMs V.
ğ´ğ‘„ğ‘…= V
(2)
Reward Function of the Query Rewriter is defined as shown
in Equation (3). Here, ğ‘…shared can be the metric for the final answer,
depicted as the yellow section in Figure 1. In this paper, we utilize
the F1 score of the predicted answer, ğ´ğ‘›ğ‘ predict, as the shared
reward. The term ğ‘ƒğ‘„ğ‘…serves as a penalty to discourage the Query
Rewriter from generating an excessive number of sub-questions
2Recent studies in generative IR (see Li et al. [23] for a survey) have explored using
generative models for retrieval. But we choose a more traditional dense retrieval
model [14] as the first-stage retriever and leave the optimization of the first-stage
retriever for future work.
during training. Specifically, ğ‘ƒğ‘„ğ‘…is assigned a value of -0.5 if the
number of sub-questions exceeds four, and it is set to 0 if the
number of sub-questions is four or fewer.
ğ‘…ğ‘„ğ‘…= ğ‘…shared + ğ‘ƒğ‘„ğ‘…
(3)
4.2.2
Elements of Selector. Observation of Selector is defined
as Equation (4), which contains prompt of Selector ğ‘ƒğ‘Ÿğ‘œğ‘šğ‘ğ‘¡ğ‘†, the
initial question ğ‘and the candidate documents set ğ·with ğ¾docu-
ments.
ğ‘‚ğ‘†= {ğ‘ƒğ‘Ÿğ‘œğ‘šğ‘ğ‘¡ğ‘†,ğ‘, ğ·}
(4)
Action Space of Selector only comprises of several words as
Equation (5). Since the function of the Selector is to output the IDs
of candidate documents helpful to answering the initial question
ğ‘, the action space is constrained to this limited set of words. This
constraint can significantly reduce the exploration space of the
Selector and provide a more stable training process.
ğ´ğ‘†= {"0", "1", . . . , "K-1", "Document", ","}
(5)
Reward Function of Selector also contains two terms, which
are ğ‘…shared and ğ‘ƒğ‘†. And ğ‘ƒğ‘†is a penalty term designed to prevent
the Selector from generating duplicate document IDs and from
outputting IDs that do not conform to the specified format (e.g.,
Document0,Document3,Document9). When the Selector outputs
duplicate document IDs or fails to adhere to the specified format,
ğ‘ƒğ‘†is set to -1; otherwise, ğ‘ƒğ‘†is set to 0.
ğ‘…ğ‘†= ğ‘…shared + ğ‘ƒğ‘†
(6)
4.2.3
Elements of Generator. Observation of Generator is in
Equation (7), which contains prompt of Generator ğ‘ƒğ‘Ÿğ‘œğ‘šğ‘ğ‘¡ğº, the
initial questionğ‘and the selected candidate documents set ğ·selected
given by Selector.
ğ‘‚ğº= {ğ‘ƒğ‘Ÿğ‘œğ‘šğ‘ğ‘¡ğº,ğ‘, ğ·selected}
(7)
Action Space of Generator ğ´ğºis the same as Query Rewriter.
ğ´ğº= ğ´ğ‘„ğ‘…= V
(8)
Reward Function of Generator contains ğ‘…shared and penalty
term ğ‘ƒğº, which is used to constrain the model from generating
excessively long content. When the generated answer exceeds a
certain length, ğ‘ƒğºis set to -0.5; otherwise, it is set to 0.
ğ‘…ğº= ğ‘…shared + ğ‘ƒğº
(9)
4.3
Warm Start with SFT
In preparation for joint optimization of multiple modules using
Multi-Agent PPO, it is essential to perform a warm start for each
trainable module. The warm start enables the model to better
adhere to instructions across diverse tasks and reduces the explo-
ration space during MARL joint training, thereby enhancing the
efficiency of exploration and exploitation. Within the MMOA-RAG
framework, there are three trainable modules: the Query Rewriter,
the Selector, and the Generator. Consequently, it is necessary to
construct training data for the SFT of each corresponding task.
Improving Retrieval-Augmented Generation through Multi-Agent Reinforcement Learning
Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY
Table 1: The prompt of Selector agent.
system: You are a helpful, respectful and honest assistant. Your
task is to output the IDs of the candidate Documents (0,1,2,...,K-1)
which are helpful in answering the Question.
assistant: Okay, I will provide the ID of candidate Documents
which are helpful in answering the Question.
user: Question: {content of Question}
Document0: {content of Document0}
...
Document(K-1): {content of Document(K-1)}
assistant: OK, I received the Question and the candidate
Documents.
user: Now, output the IDs of the candidate Documents (0,1,2,...,K-
1) which are helpful in answering the Question: {content
of Question}, for example, in the following format: Docu-
ment0,Document4,Document6,Document7.
4.3.1
Query Rewriter. In Rewrite-Retrieve-Read [25], a small lan-
guage model was trained using PPO to effectively rewrite queries
for RAG. Building on this approach, we utilize the publicly avail-
able query rewriting data from Rewrite-Retrieve-Read as the SFT
dataset to warm start the Query Rewriter in MMOA-RAG.
4.3.2
Selector. The task of the Selector is to choose a subset
ğ·selected that are helpful for answering a question from a given
set ğ·with ğ¾candidate documents. The output format of the Se-
lector is the IDs of the documents in ğ·selected (e.g., Document0,
Document4, Document6, Document7), as shown in the prompt
of Selector in Table 1. Therefore, to construct SFT data for the
Selector, the ground truth should be the IDs of documents that
are truly useful for answering the question. One method to obtain
the ground truth is to employ advanced LLMs, such as GPT-4o,
to provide the ground truth. However, we have found that this
approach does not yield results as good as expected. Additionally,
BGM [19] introduced and optimized a bridge module which is
similar to the Selector module. They proposed a method called
synthesis silver passage sequence (Synthesis SPS) to construct the
ground truth for SFT data. However, the Synthesis SPS method
requires examining each candidate document ğ‘‘ğ‘–,ğ‘—âˆˆğ·ğ‘–(candidate
documents of question ğ‘–), invoking the LLM for each check, and
comparing the utility values before and after the check, making it
a complex and costly method.
We propose a convenient heuristic approach for constructing
SFT data, aimed at LLMs to effectively follow instructions and
output in the desired format. As illustrated in Figure 2, for a given
questionğ‘ğ‘–and its golden answer, there are ğ¾candidate documents
denoted as ğ‘‘ğ‘–,ğ‘—, where ğ‘—âˆˆ{0, 1, Â· Â· Â· , ğ¾âˆ’1}. First, by removing
certain insignificant stop words and punctuation marks from ğ‘ğ‘–
and its golden answer, and converting the words to lowercase,
we obtain the set ğ‘†ğ‘’ğ‘¡ğ‘ğ‘–. Similarly, we perform the same operation
on the ğ¾candidate documents ğ‘‘ğ‘–,ğ‘—to obtain ğ‘†ğ‘’ğ‘¡ğ‘‘ğ‘–,ğ‘—. Finally, if
any word from ğ‘†ğ‘’ğ‘¡ğ‘ğ‘–appears in ğ‘†ğ‘’ğ‘¡ğ‘‘ğ‘–,ğ‘—, the ID of corresponding
Initial Question:
Who is younger, Roustam
Tariko or Dumitru Dediu?
Golden Answer:
Roustam Tariko
younger, roustam,
tariko, dumitru,
dediu
Doc 0: Dumitru Dediu (May
12, 1942 in Gala 2013 July
2013) was a cosmonaut of the
Romanian Air Force. ......
Doc 1: Roman Smishko is a
retired Ukrainian professional
footballer who played as a
goalkeeper. ......
Doc K-1: Roustam Tariko
(born March 17, 1962) is the
founder of Russian Standard
Vodka. ......
.
.
.
dumitru, dediu, may, 12,
1942, gala, 2013, july,
cosmonaut, romanian,
air, force
roman, smishko, retired,
ukrainian, professional,
footballer, played,
goalkeeper
roustam, tariko, born,
march, 17, 1962, founder,
russian, standard, vodka
í‘†Ç‰Ç˜Ç•Ç
í‘†Ç‰Ç˜ÇˆÇ,0
í‘†Ç‰Ç˜ÇˆÇ,1
í‘†Ç‰Ç˜ÇˆÇ,Æµâˆ’1
Ç•Ç
ÇˆÇ,0
ÇˆÇ,1
ÇˆÇ,Æµâˆ’1
answer
of Ç•Ç
Label of SFT:
Document0,DocumentK-1
.
.
.
Figure 2: The convenient approach to construct the SFT data
for Selector.
document ğ‘—is included in the final output as the Label of SFT.
With this approach, we can rapidly and cost-effectively construct
the Selectorâ€™s ground truth labels during the SFT stage. Given our
focus on the subsequent joint optimization of multiple modules,
this straightforward data construction method can adequately
meet our requirements.
4.3.3
Generator. The Generator is responsible for producing the
final answer, ğ´ğ‘›ğ‘ predict, based on the ğ·selected provided by the
Selector. Therefore, the ground truth for the SFT data of Generator
is the golden answer ğ´ğ‘›ğ‘ golden.
With these approaches, the SFT data used for the warm start
training of these three modulesâ€”Query Rewriter, Selector, and
Generatorâ€”can be obtained. All modules can be fine-tuned using
the typical loss function of SFT presented in Equation (10).
LSFT(ğœƒ) = âˆ’
ğ‘
âˆ‘ï¸
ğ‘›=1
log ğ‘ƒ(ğ‘Œğ‘–| ğ‘‹ğ‘–;ğœƒ)
(10)
In Equation (10), ğ‘represents the number of samples in the SFT
dataset, while ğœƒdenotes the parameters of the LLM. The variable
ğ‘‹ğ‘–corresponds to the input content of each module. Meanwhile,
ğ‘Œğ‘–signifies the output content of each module.
4.4
Multi-Agent Optimization
After undergoing SFT, the LLM demonstrates an improved abil-
ity to follow instructions while executing the functions of Query
Rewriter, Selector, and Generator. The RAG system also achieves
relatively satisfactory warm-start performance. To further enhance
the performance of the RAG system, which is modeled as a fully co-
operative multi-agent system, it is crucial to conduct joint training
of multiple agents to strengthen collaboration among them.
We adopt a setup similar to Multi-Agent PPO [45] in Star-
craft II, where multiple agents share a global reward, that is to
optimize G = {Query Rewriter (QR), Selector (S), Generator (G)}
with ğ‘…shared. To reduce computational overhead, we apply the
parameter-sharing mechanism among agents, allowing QR, S, and
G to utilize the same LLM.
Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY
Trovato et al.
In the multi-agent optimization process, there are three mod-
els to consider: the Actor model, the Critic model, and the SFT
model. The parameters for these models are denoted as ğœƒ, ğœ™, and
ğœƒSFT, respectively. The role of the Actor model is to provide the re-
sponse ğ´ğ‘›ğ‘ ğ‘¤ğ‘’ğ‘Ÿğ‘–based on the observation ğ‘‚ğ‘–for each agent ğ‘–. The
Critic model is responsible for estimating the state-value function
ğ‘‰ğ‘–,ğ‘¡
ğœ™, which is a classic setup in Actor-Critic architecture within
RL algorithms. The SFT model serves as a baseline for the Actor
model, similar to InstructGPT [27]. The objective is to update the
parameters of both the Actor and Critic models. The overall loss
function, L(ğœƒ,ğœ™), consists of two terms: LActor(ğœƒ) and LCritic(ğœ™):
L(ğœƒ,ğœ™) = LActor(ğœƒ) + ğ›¼âˆ—LCritic(ğœ™)
(11)
The Actor loss function presented in Equation (12) is similar
to that used in the typical single-agent PPO [33] algorithm. The
primary difference is that multiple agents are being optimized. In
Equation (12), ğ‘–âˆˆG denotes the three agents: Query Rewriter,
Selector, and Generator. The term ğ‘Ÿğ‘–
ğ‘¡in Equation (13) denotes the
importance sampling ratio, which measures the difference between
the new and old policies. The expression Ë†ğ´ğ‘–,ğ‘¡
ğœ‹ğœƒin Equation (14) is
the advantage function, estimated using Generalized Advantage
Estimation (GAE) [32]. The variable ğ›¿ğ‘–
ğ‘¡in Equation (15) is known
as the temporal difference (TD) error at time step ğ‘¡.
LActor(ğœƒ) =
âˆ‘ï¸
ğ‘–
âˆ‘ï¸
ğ‘¡
min

ğ‘Ÿğ‘–
ğ‘¡Ë†ğ´ğ‘–,ğ‘¡
ğœ‹ğœƒ, clip

ğ‘Ÿğ‘–
ğ‘¡, 1 âˆ’ğœ–, 1 + ğœ–

Ë†ğ´ğ‘–,ğ‘¡
ğœ‹ğœƒ

(12)
ğ‘Ÿğ‘–
ğ‘¡= ğœ‹ğœƒ(ğ‘ğ‘–
ğ‘¡| ğ‘ ğ‘–
ğ‘¡)
ğœ‹ğœƒold (ğ‘ğ‘–
ğ‘¡| ğ‘ ğ‘–
ğ‘¡)
(13)
Ë†ğ´ğ‘–,ğ‘¡
ğœ‹ğœƒ=
âˆ
âˆ‘ï¸
ğ‘™=0
(ğ›¾ğœ†)ğ‘™ğ›¿ğ‘–
ğ‘¡+ğ‘™
(14)
ğ›¿ğ‘–
ğ‘¡= ğ‘…(ğ‘ ğ‘–
ğ‘¡,ğ‘ğ‘–
ğ‘¡) + ğ›¾ğ‘‰ğœ™(ğ‘ ğ‘–
ğ‘¡+1) âˆ’ğ‘‰ğœ™(ğ‘ ğ‘–
ğ‘¡)
(15)
Similar to InstructGPT [27], the final reward function ğ‘…(ğ‘ ğ‘–
ğ‘¡,ğ‘ğ‘–
ğ‘¡)
is defined in Equation (16). The distinction is that our approach
does not require a trained reward model, as we use the evaluation
metric (F1 score) of the predicted answers ğ´ğ‘›ğ‘ predict of Generator
as the shared reward ğ‘…shared for all agents. The penalty term ğ‘ƒğ‘–can
also be easily obtained from the output of each agent, as introduced
in Section 4.2. The components ğ‘…ğ‘–in Equation (16) are defined
in Equations (3), (6), or (9). And ğ´ğ‘›ğ‘ ğ‘¤ğ‘’ğ‘Ÿğ‘–represents the output
generated by each agent ğ‘–based on its individual observation ğ‘‚ğ‘–.
ğ‘…(ğ‘ ğ‘–
ğ‘¡,ğ‘ğ‘–
ğ‘¡) =
ï£±ï£´ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£´ï£³
0,
if ğ‘¡< ğ‘‡
ğ‘…shared + ğ‘ƒğ‘–
|        {z        }
ğ‘…ğ‘–, and ğ‘–âˆˆG
âˆ’ğ›½log
 ğœ‹ğœƒ(Answerğ‘–|ğ‘‚ğ‘–)
ğœ‹ğœƒSFT (Answerğ‘–|ğ‘‚ğ‘–)

,
if ğ‘¡= ğ‘‡
(16)
The loss function of the Critic model, as shown in Equation
(17), employs a clipping operation similar to the Actor model.
Here, Î”ğ‘‰ğ‘–,ğ‘¡= ğ‘‰ğ‘–,ğ‘¡
ğœ™
âˆ’ğ‘‰ğ‘–,ğ‘¡
target, where ğ‘‰ğ‘–,ğ‘¡
ğœ™
= ğ‘‰ğœ™(ğ‘ ğ‘–
ğ‘¡). The term ğ‘‰ğ‘–,ğ‘¡
target
represents the cumulative return and ğ‘ ğ‘–
ğ‘¡is the state-value function.
LCritic(ğœ™) =
âˆ‘ï¸
ğ‘–
âˆ‘ï¸
ğ‘¡
max

(Î”ğ‘‰ğ‘–,ğ‘¡)2,

clip

ğ‘‰ğ‘–,ğ‘¡
ğœ™,ğ‘‰ğ‘–,ğ‘¡
ğœ™old Â± ğœ–

âˆ’ğ‘‰ğ‘–,ğ‘¡
target
2
(17)
Algorithm 1: The Training Process of Multi-Agent Opti-
mization
Initialize: The parameters of the Actor model ğœƒ, the Critic model
ğœ™, the SFT model ğœƒSFT, and a replay buffer M = âˆ….
Inputs: Dataset with initial questions ğ‘and corresponding golden
answers ğ´ğ‘›ğ‘ golden
for ğ‘’ğ‘ğ‘œğ‘â„â†1 to ğ‘_ğ‘’ğ‘ğ‘œğ‘â„do
for ğ‘ğ‘ğ‘¡ğ‘â„â†1 to ğ‘_ğ‘ğ‘ğ‘¡ğ‘â„do
// Collect Rollout
for each question ğ‘âˆˆğ‘ğ‘ğ‘¡ğ‘â„do
// Query Rewriter (QR)
Construct observation ğ‘‚ğ‘„ğ‘…according to Equation (1)
Get sub-questions ğ‘ ğ‘¢ğ‘ğ‘for initial question ğ‘
Calculate the penalty term of Query Rewriter ğ‘ƒğ‘„ğ‘…
// Retriever
Retrieve ğ¾candidate documents to construct ğ·
// Selector (S)
Construct observation ğ‘‚ğ‘†according to Equation (4)
Select ğ¼ğ·ğ‘ of helpful documents, and get ğ·selected
Calculate the penalty term of Selector ğ‘ƒğ‘†
// Generator (G)
Construct observation ğ‘‚ğºaccording to Equation (7)
Predict the ğ´ğ‘›ğ‘ predict to initial question ğ‘
Calculate the penalty term of Generator ğ‘ƒğº
// Getting Reward and Storing Tuple
Calculate the F1 score of ğ´ğ‘›ğ‘ predict as the shared
reward ğ‘…shared
Get reward for each agent ğ‘…ğ‘–,ğ‘–âˆˆ{ğ‘„ğ‘…,ğ‘†,ğº},
according Equation (3), (6) and (9)
Store tuple T =
 (ğ‘‚ğ‘„ğ‘…,ğ‘ ğ‘¢ğ‘ğ‘, ğ‘…ğ‘„ğ‘…), (ğ‘‚ğ‘†, ğ¼ğ·ğ‘ , ğ‘…ğ‘†), (ğ‘‚ğº,ğ´ğ‘›ğ‘ predict, ğ‘…ğº)
in the replay buffer M
// Policy and Value Optimization
for each question ğ‘âˆˆğ‘ğ‘ğ‘¡ğ‘â„do
Compute the advantage function Ë†ğ´ğ‘–,ğ‘¡
ğœ‹ğœƒusing GAE
Calculate the loss of the Actor LActor(ğœƒ) and Critic
model LCritic(ğœ™)
Update the parameters of models through the overall
loss function L(ğœƒ,ğœ™) in Equation (11)
Clear the replay buffer M to âˆ…
Output:Well-trained Actor model with parameters ğœƒtrained
The pseudocode for multi-agent optimization based on MAPPO
is shown in Algorithm 1, which corresponds to the overall frame-
work of MMOA-RAG depicted in Figure 1. For a specific ques-
tion, the first step is to execute the Collect Rollout process.
This process involves passing through the Query Rewriter, Re-
triever, Selector, and Generator, and the computed tuple T =

(ğ‘‚ğ‘„ğ‘…,ğ‘ ğ‘¢ğ‘ğ‘, ğ‘…ğ‘„ğ‘…), (ğ‘‚ğ‘†, ğ¼ğ·ğ‘ , ğ‘…ğ‘†), (ğ‘‚ğº,ğ´ğ‘›ğ‘ predict, ğ‘…ğº)

is stored in
the replay buffer M. Next, the Policy and Value Optimization
process is executed where the GAE is used to estimate the advan-
tage function Ë†ğ´ğ‘–,ğ‘¡
ğœ‹ğœƒ. Subsequently, the overall loss function L(ğœƒ,ğœ™)
is calculated, and the parameters of both the Actor and Critic
Improving Retrieval-Augmented Generation through Multi-Agent Reinforcement Learning
Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY
models are updated. Additionally, to accelerate the entire train-
ing process, we can run a minibatch in parallel. Ultimately, we
obtain a well-trained Actor model used for subsequent inference
and evaluation.
5
Experiments
Our experiments mainly aim to explore the following research
questions:
â€¢ RQ1. How does our MMOA-RAG perform compared to existing
RAG optimization methods?
â€¢ RQ2. What are the results of the ablation study on MMOA-
RAG, specifically, is joint optimization of multiple modules
more effective?
â€¢ RQ3. Does the joint optimization method of MMOA-RAG ex-
hibit generalizability across different RAG systems?
â€¢ RQ4. How does the MMOA-RAG method perform in out-of-
domain scenarios, i.e., what is its generalization capability?
5.1
Experimental Settings
5.1.1
Datasets and Evaluation. We conducted experiments using
MMOA-RAG alongside various baseline models across three open-
domain QA datasets: HotpotQA [43], 2WikiMultihopQA [12], and
AmbigQA [26]. The candidate documents are all retrieved from
Wikipedia passages for three datasets. We employ three key evalu-
ation metricsâ€”Accuracy, Exact Match (EM), and F1 scoreâ€”to assess
the performance of the RAG methods.
5.1.2
Baselines. The methods detailed below serve as baseline
models:
LLM w/o RAG: This approach answers questions solely based
on the internal knowledge embedded within LLMs, without em-
ploying any retrieval mechanisms.
Vanilla RAG w/o train: This method leverages a retrieval
model to obtain relevant documents, thereby augmenting the
LLMâ€™s internal knowledge with external sources. Here, the LLM
remains in a pre-trained state and has not undergone any fine-
tuning.
Vanilla RAG w SFT: Building on the Vanilla RAG framework,
this variant involves a fine-tuned LLM to improve the integration of
retrieved external knowledge with the LLMâ€™s internal knowledge,
potentially enhancing the quality of final answers.
SELF-RAG [1]: This innovative framework advances LLM per-
formance by incorporating both adaptive retrieval mechanisms
and self-reflection processes, aiming to produce precise and de-
pendable answers.
RetRobust [44]: This approach fortifies the RAG architecture
against irrelevant contexts, thereby boosting its effectiveness in
open-domain question-answering scenarios.
Rewrite-Retrieve-Read [25]: A small-scale query rewriter
model is trained using reinforcement learning, optimizing the
interaction between retrieval and answer generation.
BGM [19]: Utilizing PPO, this method trains a bridge component
to filter and identify documents that are more likely to be helpful,
thus refining the quality of the retrieved context.
5.1.3
Implementation Details. We utilize Contriever [14] as the
Retriever. Regardless of how many sub-questions ğ‘ ğ‘¢ğ‘ğ‘the Query
Rewriter generates from the initial question ğ‘, the Selector con-
sistently receives a fixed set of ğ¾= 10 documents as input. For
example, if the Query Rewriter yields 2 sub-questions, each sub-
question is used for retrieval, with the top-5 documents from each
retrieval being selected as part of the candidate documents ğ·for
the Selector. Furthermore, it is important to emphasize that we
do not utilize any support facts or positive passages 3 that come
with the official datasets to generate answers. Instead, we only use
the ğ¾candidate documents from the retrieval model as external
knowledge for answer generation.
Besides, we employ Llama-3-8B-Instruct [5] as the foundational
LLM for the baselines and MMOA-RAG. Building on the PPO code
from LLama-Factory4 [50], we have developed MMOA-RAG, which
optimizes the RAG multi-agent system using Multi-Agent PPO.
And the critical hyperparameters of MMOA-RAG are detailed in
Table 2.
Table 2: Key hyperparameters in the training process of
MMOA-RAG.
Name
Explanation
Values
ğ›½ğ‘šğ‘ğ‘¥
Maximum ğ›½in Equation (16)
0.2
ğ›½ğ‘šğ‘–ğ‘›
Minimum ğ›½in Equation (16)
0.06
ğ›¾
Key hyperparameter in GAE
1.0
ğœ†
Key hyperparameter in GAE
0.95
ğœ–
Clip range in MAPPO
0.2
ğ›¼
Coefficients in Equation (11)
0.1
ğ‘™ğ‘Ÿ
Maximum learning rate
2e-5
bueffer_size
Buffer size in MAPPO
128
lr_scheduler
Learning rate scheduler
cosine
top_p
Sampling parameters in training
0.9
5.2
Comparisons with Other Methods
We conducted a comparative analysis of MMOA-RAG against mul-
tiple baselines, with the results presented in Table 3. To ensure
fairness in comparison, all baselines were re-implemented accord-
ing to the settings delineated in Section 5.1.3. Each method utilized
Llama-3-8B-Instruct as the backbone architecture. Within these
methods, the untuned modules employed the pre-trained version of
Llama-3-8B-Instruct, whereas the trainable modules were derived
through specific SFT processes on Llama-3-8B-Instruct. Notably, in
the Rewrite-Retrieve-Read framework, the query rewrite module,
which is trainable, was optimized using the PPO algorithm ap-
plied to Llama-3-8B-Instruct; meanwhile, answer generation was
implemented based on the SFT-refined backbone. Regarding the
BGM method, we rebuilt the bridge to connect the retrieval model
and the generation model, leveraging Llama-3-8B-Instruct, with
this bridge being trained using the PPO algorithm. The genera-
tion model for BGM was similarly obtained from the SFT-refined
backbone.
3Some QA datasets inherently include supportive texts that aid in answering questions.
And some studies incorporate these supportive texts alongside retrieved candidate
documents as input to LLMs for answer prediction, which significantly enhances
answer quality. However, we adhere to the natural RAG process by using only the
candidate documents provided by the retriever for answer prediction, as annotated
supportive texts are not present in the natural RAG workflow.
4https://github.com/hiyouga/LLaMA-Factory
Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY
Trovato et al.
Table 3: Performance for different methods across datasets. All the results in this table are obtained using Llama-3-8B-Instruct
as the backbone. In each dataset, the highest baseline value is underscored. The final row Î” displays the improvement of
MMOA-RAG over the best baseline.
Methods
HotpotQA
2WikiMultihopQA
AmbigQA
Acc
EM
F1
Acc
EM
F1
Acc
EM
F1
LLM w/o RAG
25.08
21.31
31.18
27.78
23.68
29.47
27.21
20.96
33.42
Vanilla RAG w/o train
27.99
20.62
30.67
31.94
13.91
22.84
31.09
22.42
33.56
Vanilla RAG w SFT
36.18
32.30
44.49
39.47
38.28
43.36
34.41
30.74
44.36
SELF-RAG [1]
30.42
27.77
38.93
36.32
35.39
38.86
28.35
25.70
39.04
RetRobust [44]
37.69
34.60
46.49
41.02
39.73
44.51
35.13
32.37
44.78
Rewrite-Retrieve-Read [25]
38.03
33.93
46.32
40.40
39.17
44.17
35.94
31.90
45.92
BGM [19]
36.05
32.76
44.54
39.61
38.61
43.29
36.01
32.53
45.76
MMOA-RAG (ours)
39.15
36.15
48.29
42.73
41.52
46.40
38.85
34.75
48.59
Î”
+1.12
+1.55
+1.80
+1.71
+1.79
+1.89
+2.84
+2.22
+2.67
Firstly, as shown in Table 3, MMOA-RAG demonstrates supe-
rior performance across all metrics and datasets, highlighting its
effectiveness. Additionally, it is noteworthy that Vanilla RAG w/o
train achieves comparable results to LLM w/o RAG across various
metrics. This observation suggests that the pre-trained Llama-3-8B-
Instruct struggles to effectively leverage external knowledge for
answer generation, likely due to the absence of RAG-related tasks
in its pre-training process, which limits its external knowledge
utilization. In contrast, Vanilla RAG w SFT exhibits substantial
improvements over Vanilla RAG w/o train across all evaluation
metrics. This indicates that the SFT-enhanced Llama-3-8B-Instruct
is adept at utilizing external knowledge, successfully extracting
valuable information from noisy candidate documents to enhance
the quality of generated answers.
The Rewrite-Retrieve-Read and BGM approaches enhance Vanilla
RAG by respectively integrating a query rewrite module and a
bridge module, each of which is trained using the PPO algorithm.
As indicated in Table 3, on the multi-hop datasets HotpotQA and
2WikiMultihopQA, Rewrite-Retrieve-Read surpasses BGM, sug-
gesting that the inclusion of a query rewrite module is more effec-
tive than adding a bridge module for these multi-hop datasets. Con-
versely, on the single-hop dataset AmbigQA, the performance of
Rewrite-Retrieve-Read and BGM is relatively similar. Our MMOA-
RAG can be conceptualized as augmenting Vanilla RAG by integrat-
ing both a Query Rewriter and a Selector, whose roles are akin to
the query rewrite module in Rewrite-Retrieve-Read and the bridge
module in BGM. The primary advantage of MMOA-RAG lies in
its simultaneous optimization of the Query Rewriter, Selector, and
Generator modules. This is achieved by aligning the objectives
of these modules with the goal of generating higher-quality an-
swers via MAPPO. The experimental results presented in Table
3 further illustrate that MMOA-RAG significantly outperforms
Rewrite-Retrieve-Read, BGM, and other baselines.
The results in Table 3 and the analysis in Section 5.2 jointly
answer the RQ1.
5.3
Ablation Experiments on the Optimization
of Different Agents
To demonstrate the necessity of multi-agent joint optimization in
RAG systems, we present ablation experiments in this section. The
MMOA-RAG framework, depicted in Figure 1, consists of three
agents: ğ‘–âˆˆ{Query Rewriter (QR), Selector (S), Generator (G)}. In
Table 4, MMOA-RAG w/o ğ‘–denotes the variant where agent ğ‘–is
excluded from the complete optimization process of multi-agent
joint optimization.
As illustrated in Table 4, the complete version of MMOA-RAG,
where all three modules are jointly optimized, delivers the highest
performance. This underscores the effectiveness of multi-agent
joint optimization within the RAG system and validates the impor-
tance of optimizing multiple modules concurrently. Additionally,
the MMOA-RAG w/o S variant achieves the best performance
among the three ablation configurations. The Selectorâ€™s primary
function is to refine the candidate document set ğ·, yielding a
higher-quality subset ğ·selected, which enhances the Generatorâ€™s
ability to produce a superior answer ğ´ğ‘›ğ‘ predict. However, through
the joint optimization process with MAPPO, the Generator ac-
quires some denoising capabilities. Consequently, satisfactory re-
sults can be achieved even when the Selector is not optimized
during joint optimization.
0
20000
40000
60000
# Training Samples
0.38
0.42
0.46
0.50
F1 Score
MMOA-RAG w/o G
MMOA-RAG w/o S
MMOA-RAG w/o QR
MMOA-RAG
Figure 3: Ablation experiments on AmbigQA dataset. The
horizontal axis represents the number of training samples,
while the vertical axis denotes the shared reward ğ‘…shared (F1
score) during the training process.
We also present the trajectory of the shared reward ğ‘…shared dur-
ing the training process based on ablation experiments conducted
on the AmbigQA dataset, as shown in Figure 3. From Figure 3, it is
Improving Retrieval-Augmented Generation through Multi-Agent Reinforcement Learning
Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY
Table 4: Ablation about Optimizing Different Agents. In this table, MMOA-RAG w/o ğ‘–(ğ‘–âˆˆ{QR, S, G}) denote the variant where
agent ğ‘–is excluded from the complete optimization process of multi-agent joint optimization.
Methods
HotpotQA
2WikiMultihopQA
AmbigQA
Acc
EM
F1
Acc
EM
F1
Acc
EM
F1
MMOA-RAG w/o G
38.28
35.19
46.91
41.93
40.80
45.62
37.92
33.65
47.77
MMOA-RAG w/o S
38.81
35.27
47.61
42.47
40.99
45.95
38.69
34.38
48.23
MMOA-RAG w/o QR
37.26
34.66
46.05
42.20
40.82
45.70
38.64
34.34
48.14
MMOA-RAG
39.15
36.15
48.29
42.73
41.52
46.40
38.85
34.75
48.59
Table 5: Generality Experiments on RAG Systems with Varying Module Configurations. In the second column, SFT and
MAPPO refer to the current module configuration following the warm start training stage (Section 4.3) and the MAPPO joint
optimization training stage (Section 4.4), respectively. The symbol Î” signifies the enhancement achieved in the MAPPO stage
relative to the SFT stage.
Modules
Training Stage
& Delta
HotpotQA
2WikiMultihopQA
AmbigQA
Acc
EM
F1
Acc
EM
F1
Acc
EM
F1
QR+S+G
SFT
36.00
33.04
44.69
39.54
38.50
42.97
36.55
32.60
46.71
MAPPO
39.15
36.15
48.29
42.73
41.52
46.40
38.85
34.75
48.59
Î”
+3.15
+3.11
+3.60
+3.19
+3.02
+3.43
+2.30
+2.15
+1.88
S+G
SFT
34.25
32.18
43.14
38.93
37.97
42.40
35.85
32.35
45.82
MAPPO
38.23
34.85
47.07
41.79
40.57
45.25
37.60
33.90
47.19
Î”
+3.98
+2.67
+3.93
+2.86
+2.60
+2.85
+1.75
+1.55
+1.37
QR+G
SFT
36.76
32.78
45.00
39.15
37.89
42.91
35.50
31.50
45.31
MAPPO
38.90
35.89
47.94
42.43
41.01
46.19
37.65
33.50
47.53
Î”
+2.14
+3.11
+2.94
+3.28
+3.12
+3.28
+2.15
+2.00
+2.22
evident that the reward curve for MMOA-RAG demonstrates the
fastest convergence rate and achieves the highest final convergence
value. This underscores the effectiveness of joint optimization
across multiple modules in significantly and efficiently enhancing
the performance of the RAG system. Furthermore, the training
curve for MMOA-RAG w/o G in Figure 3 is noticeably slower com-
pared to other algorithms, and the test results for MMOA-RAG
w/o G on the AmbigQA dataset, as shown in Table 4, are the poor-
est. These findings suggest that the Generator module is the most
critical component for the single-hop AmbigQA dataset.
The results in Table 4 and Figure 3 answer the RQ.2 that it
is more effective to optimize multiple modules in a RAG system
simultaneously.
5.4
Generality Experiments on RAG Systems
with Varying Module Configurations
In this section, we evaluate the performance of MMOA-RAG in op-
timizing RAG systems with different numbers of agents, as detailed
in Table 5. In Table 5, QR+S+G represents the RAG framework
depicted in Figure 1, illustrating a multi-agent system composed
of three agent, Query Rewriter, Selector and Generator. The con-
figuration S+G results from omitting the Query Rewriter agent,
relying solely on the initial question ğ‘for retrieval, thereby con-
figuring the RAG system as a two-agent (Selector and Generator)
system. Conversely, QR+G denotes the exclusion of the Selector
agent, forming a RAG pipeline consisting of two agents, Query
Rewriter and Generator. The second column of Table 5 specifies
that SFT refers to the warm start of all agents in the correspond-
ing RAG system through supervised fine-tuning, while MAPPO
refers to the joint optimization of all agents built upon SFT utiliz-
ing the MAPPO framework. The notation Î” is used to denote the
performance enhancement achieved by MAPPO compared to SFT.
The experimental results in Table 5 reveal that RAG systems
optimized using joint MAPPO consistently outperform those using
only SFT across all datasets. This finding underscores the robust
generalizability of the MMOA-RAG joint optimization approach,
yielding significant performance improvements across diverse
RAG configurations. Notably, the performance gains from MAPPO
over SFT are approximately three percentage points on multi-hop
datasets such as HotpotQA and 2WikiMultihopQA, while improve-
ments on the single-hop dataset AmbigQA are around two percent-
age points. This difference may stem from the greater complexity
inherent to multi-hop datasets, which potentially exacerbates mis-
alignment among different modules during the SFT stage. These
results further highlight the necessity of multi-module joint opti-
mization, especially in the context of more challenging multi-hop
datasets.
The results presented in Table 5 demonstrate the effectiveness of
MMOA-RAG in optimizing various RAG systems across different
configurations, thereby answering RQ.3.
5.5
Out-of-Domain Experiments
We also conducted out-of-domain (OOD) experiments to evaluate
the generalization capabilities of MMOA-RAG compared to the
baselines. We trained LLM on HotpotQA dataset and evaluated
Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY
Trovato et al.
on the AmbigQA dataset. The experimental results are shown in
Table 6.
Table 6: Out-of-domain experimental results: The model is
trained on the HotpotQA dataset and tested on the AmbigQA
dataset.
Methods
Acc
EM
F1
SELF-RAG [1]
26.70
24.25
36.38
RetRobust [44]
34.19
31.75
44.08
Rewrite-Retrieve-Read [25]
33.91
30.73
43.61
BGM [19]
32.58
29.77
42.07
MMOA-RAG (ours)
35.45
32.43
45.62
From Table 6, it is evident that MMOA-RAG demonstrates su-
perior performance in the OOD experiments, underscoring its
notable generalization capabilities and effectively answering RQ.4.
Additionally, it is noteworthy that RetRobust outperforms all other
baselines. This can be attributed to its strategy of integrating both
relevant and irrelevant data during the SFT process, which signifi-
cantly enhances the robustness and generalization abilities of the
RAG system.
6
Conclusions
In this paper, we model the RAG system as a multi-agent collabo-
rative task, wherein we consider the Query Rewriter, Selector, and
Generator modules as learnable RL agents. We employ a multi-
agent reinforcement learning algorithm to jointly optimize these
agents, aligning the optimization goals of multiple modules with
the ultimate objective of generating high-quality answers.
Our experiments demonstrate the effectiveness of our modeling
approach and joint optimization method. Comprehensive ablation
studies confirm the necessity and generality of multi-module joint
optimization, establishing MMOA-RAG as an effective approach
for optimizing RAG systems.
Improving Retrieval-Augmented Generation through Multi-Agent Reinforcement Learning
Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY
References
[1] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi.
2023. Self-rag: Learning to retrieve, generate, and critique through self-reflection.
arXiv preprint arXiv:2310.11511 (2023).
[2] Yiqun Chen, Qi Liu, Yi Zhang, Weiwei Sun, Daiting Shi, Jiaxin Mao, and Dawei
Yin. 2024. TourRank: Utilizing Large Language Models for Documents Ranking
with a Tournament-Inspired Strategy. arXiv preprint arXiv:2406.11678 (2024).
[3] Yiqun Chen, Hangyu Mao, Tianle Zhang, Shiguang Wu, Bin Zhang, Jianye Hao,
Dong Li, Bin Wang, and Hongxing Chang. 2022. Ptde: Personalized training
with distillated execution for multi-agent reinforcement learning. arXiv preprint
arXiv:2210.08872 (2022).
[4] Florin Cuconasu, Giovanni Trappolini, Federico Siciliano, Simone Filice, Cesare
Campagnano, Yoelle Maarek, Nicola Tonellotto, and Fabrizio Silvestri. 2024. The
power of noise: Redefining retrieval for rag systems. In Proceedings of the 47th
International ACM SIGIR Conference on Research and Development in Information
Retrieval. 719â€“729.
[5] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad
Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan,
et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783 (2024).
[6] Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. Gptscore:
Evaluate as you desire. arXiv preprint arXiv:2302.04166 (2023).
[7] Jingsheng Gao, Linxu Li, Weiyuan Li, Yuzhuo Fu, and Bin Dai. 2024. SmartRAG:
Jointly Learn RAG-Related Tasks From the Environment Feedback. arXiv preprint
arXiv:2410.18141 (2024).
[8] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai,
Jiawei Sun, and Haofen Wang. 2023. Retrieval-augmented generation for large
language models: A survey. arXiv preprint arXiv:2312.10997 (2023).
[9] Peiyuan Gong and Jiaxin Mao. 2023. CoAScore: Chain-of-Aspects Prompting
for NLG Evaluation. arXiv preprint arXiv:2312.10355 (2023).
[10] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang.
2020. Retrieval augmented language model pre-training. In International confer-
ence on machine learning. PMLR, 3929â€“3938.
[11] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang,
and Zhiting Hu. 2023. Reasoning with language model is planning with world
model. arXiv preprint arXiv:2305.14992 (2023).
[12] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020.
Constructing a multi-hop QA dataset for comprehensive evaluation of reasoning
steps. arXiv preprint arXiv:2011.01060 (2020).
[13] Jie Huang and Kevin Chen-Chuan Chang. 2022. Towards reasoning in large
language models: A survey. arXiv preprint arXiv:2212.10403 (2022).
[14] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bo-
janowski, Armand Joulin, and Edouard Grave. 2021. Unsupervised dense in-
formation retrieval with contrastive learning. arXiv preprint arXiv:2112.09118
(2021).
[15] Gautier Izacard and Edouard Grave. 2020. Distilling knowledge from reader to
retriever for question answering. arXiv preprint arXiv:2012.04584 (2020).
[16] Kalervo JÃ¤rvelin and Jaana KekÃ¤lÃ¤inen. 2002. Cumulated gain-based evaluation
of IR techniques. ACM Transactions on Information Systems (TOIS) 20, 4 (2002),
422â€“446.
[17] Jinhao Jiang, Jiayi Chen, Junyi Li, Ruiyang Ren, Shijie Wang, Wayne Xin Zhao,
Yang Song, and Tao Zhang. 2024. RAG-Star: Enhancing Deliberative Reason-
ing with Retrieval Augmented Verification and Refinement. arXiv preprint
arXiv:2412.12881 (2024).
[18] Zhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-
Yu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Active retrieval
augmented generation. arXiv preprint arXiv:2305.06983 (2023).
[19] Zixuan Ke, Weize Kong, Cheng Li, Mingyang Zhang, Qiaozhu Mei, and Michael
Bendersky. 2024. Bridging the preference gap between retrievers and llms. arXiv
preprint arXiv:2401.06954 (2024).
[20] Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang,
Christopher Potts, and Matei Zaharia. 2022. Demonstrate-search-predict: Com-
posing retrieval and language models for knowledge-intensive nlp.
arXiv
preprint arXiv:2212.14024 (2022).
[21] Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval
for weakly supervised open domain question answering.
arXiv preprint
arXiv:1906.00300 (2019).
[22] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir
Karpukhin, Naman Goyal, Heinrich KÃ¼ttler, Mike Lewis, Wen-tau Yih, Tim Rock-
tÃ¤schel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp
tasks. Advances in Neural Information Processing Systems 33 (2020), 9459â€“9474.
[23] Xiaoxi Li, Jiajie Jin, Yujia Zhou, Yuyao Zhang, Peitian Zhang, Yutao Zhu, and
Zhicheng Dou. 2024. From matching to generation: A survey on generative
information retrieval. arXiv preprint arXiv:2404.14851 (2024).
[24] Xinze Li, Sen Mei, Zhenghao Liu, Yukun Yan, Shuo Wang, Shi Yu, Zheni Zeng,
Hao Chen, Ge Yu, Zhiyuan Liu, et al. 2024. RAG-DDR: Optimizing Retrieval-
Augmented Generation Using Differentiable Data Rewards. arXiv preprint
arXiv:2410.13509 (2024).
[25] Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. 2023. Query
rewriting for retrieval-augmented large language models.
arXiv preprint
arXiv:2305.14283 (2023).
[26] Sewon Min, Julian Michael, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2020.
AmbigQA: Answering ambiguous open-domain questions.
arXiv preprint
arXiv:2004.10645 (2020).
[27] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela
Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022.
Training language models to follow instructions with human feedback. Advances
in neural information processing systems 35 (2022), 27730â€“27744.
[28] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano
Ermon, and Chelsea Finn. 2024. Direct preference optimization: Your language
model is secretly a reward model. Advances in Neural Information Processing
Systems 36 (2024).
[29] Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar,
Jakob Foerster, and Shimon Whiteson. 2018. Qmix: Monotonic value func-
tion factorisation for deep multi-agent reinforcement learning. In International
conference on machine learning. PMLR, 4295â€“4304.
[30] Alireza Salemi and Hamed Zamani. 2024. Learning to Rank for Multiple Retrieval-
Augmented Models through Iterative Utility Maximization.
arXiv preprint
arXiv:2410.09942 (2024).
[31] Alireza Salemi and Hamed Zamani. 2024. Towards a search engine for machines:
Unified ranking for multiple retrieval-augmented large language models. In
Proceedings of the 47th International ACM SIGIR Conference on Research and
Development in Information Retrieval. 741â€“751.
[32] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel.
2015. High-dimensional continuous control using generalized advantage esti-
mation. arXiv preprint arXiv:1506.02438 (2015).
[33] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347
(2017).
[34] Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu
Chen. 2023. Enhancing retrieval-augmented large language models with iterative
retrieval-generation synergy. arXiv preprint arXiv:2305.15294 (2023).
[35] Zhengliang Shi, Weiwei Sun, Shen Gao, Pengjie Ren, Zhumin Chen, and
Zhaochun Ren. 2024. Generate-then-ground in retrieval-augmented generation
for multi-hop question answering. arXiv preprint arXiv:2406.14891 (2024).
[36] Devendra Singh, Siva Reddy, Will Hamilton, Chris Dyer, and Dani Yogatama.
2021. End-to-end training of multi-document reader and retriever for open-
domain question answering. Advances in Neural Information Processing Systems
34 (2021), 25968â€“25981.
[37] Weihang Su, Yichen Tang, Qingyao Ai, Zhijing Wu, and Yiqun Liu. 2024. Dragin:
Dynamic retrieval augmented generation based on the real-time information
needs of large language models. arXiv preprint arXiv:2403.10081 (2024).
[38] Weiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang Wang, Pengjie Ren, Zhumin
Chen, Dawei Yin, and Zhaochun Ren. 2023.
Is ChatGPT good at search?
investigating large language models as re-ranking agents.
arXiv preprint
arXiv:2304.09542 (2023).
[39] Fei Wang, Xingchen Wan, Ruoxi Sun, Jiefeng Chen, and Sercan Ã– ArÄ±k. 2024. As-
tute rag: Overcoming imperfect retrieval augmentation and knowledge conflicts
for large language models. arXiv preprint arXiv:2410.07176 (2024).
[40] Zhepei Wei, Wei-Lin Chen, and Yu Meng. [n. d.]. InstructRAG: Instructing
Retrieval Augmented Generation via Self-Synthesized Rationales. In Adaptive
Foundation Models: Evolving AI for Personalized and Efficient Learning.
[41] Shicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng, and Tat-Seng Chua.
2024. Search-in-the-Chain: Interactively Enhancing Large Language Models
with Search for Knowledge-intensive Tasks. In Proceedings of the ACM on Web
Conference 2024. 1362â€“1373.
[42] Shicheng Xu, Liang Pang, Mo Yu, Fandong Meng, Huawei Shen, Xueqi Cheng,
and Jie Zhou. 2024.
Unsupervised Information Refinement Training of
Large Language Models for Retrieval-Augmented Generation. arXiv preprint
arXiv:2402.18150 (2024).
[43] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan
Salakhutdinov, and Christopher D Manning. 2018. HotpotQA: A dataset for di-
verse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600
(2018).
[44] Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. 2023.
Making
retrieval-augmented language models robust to irrelevant context. arXiv preprint
arXiv:2310.01558 (2023).
[45] Chao Yu, Akash Velu, Eugene Vinitsky, Jiaxuan Gao, Yu Wang, Alexandre Bayen,
and Yi Wu. 2022. The surprising effectiveness of ppo in cooperative multi-agent
games. Advances in Neural Information Processing Systems 35 (2022), 24611â€“
24624.
[46] Hamed Zamani and Michael Bendersky. 2024.
Stochastic rag: End-to-end
retrieval-augmented generation through expected utility maximization. In Pro-
ceedings of the 47th International ACM SIGIR Conference on Research and Devel-
opment in Information Retrieval. 2641â€“2646.
[47] Qingfei Zhao, Ruobing Wang, Yukuo Cen, Daren Zha, Shicheng Tan, Yuxiao
Dong, and Jie Tang. 2024. LongRAG: A Dual-Perspective Retrieval-Augmented
Generation Paradigm for Long-Context Question Answering. arXiv preprint
arXiv:2410.18050 (2024).
Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY
Trovato et al.
[48] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,
Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey
of large language models. arXiv preprint arXiv:2303.18223 (2023).
[49] Xinping Zhao, Dongfang Li, Yan Zhong, Boren Hu, Yibin Chen, Baotian Hu, and
Min Zhang. 2024. Seer: Self-aligned evidence extraction for retrieval-augmented
generation. arXiv preprint arXiv:2410.11315 (2024).
[50] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi
Feng, and Yongqiang Ma. 2024. Llamafactory: Unified efficient fine-tuning of
100+ language models. arXiv preprint arXiv:2403.13372 (2024).
[51] Junda Zhu, Lingyong Yan, Haibo Shi, Dawei Yin, and Lei Sha. 2024. ATM:
Adversarial Tuning Multi-agent System Makes a Robust Retrieval-Augmented
Generator. arXiv preprint arXiv:2405.18111 (2024).
[52] Kun Zhu, Xiaocheng Feng, Xiyuan Du, Yuxuan Gu, Weijiang Yu, Haotian Wang,
Qianglong Chen, Zheng Chu, Jingchang Chen, and Bing Qin. 2024. An Informa-
tion Bottleneck Perspective for Effective Noise Filtering on Retrieval-Augmented
Generation. arXiv preprint arXiv:2406.01549 (2024).
