Improving Retrieval Augmented Language Model with Self-Reasoning
Yuan Xia1, Jingbo Zhou2,*, Zhenhui Shi1, Jun Chen1, Haifeng Huang1
1Baidu Inc., China 2Baidu Research, China
{xiayuan,zhoujingbo,shizhenhui,chenjun22,huanghaifeng}@baidu.com
Abstract
The Retrieval-Augmented Language Model (RALM) has
demonstrated
remarkable
performance
on
knowledge-
intensive tasks by integrating external knowledge during
inference, which mitigates the factual hallucinations in-
herited in large language models (LLMs). Despite these
advancements, challenges persist in the implementation of
RALMs, particularly in terms of reliability and traceability.
Specifically, the irrelevant document retrieval may result
in unhelpful responses or even deteriorate the performance
of LLMs, while the lack of appropriate citations in outputs
complicates efforts to verify the trustworthiness of the
models. To this end, we propose a novel self-reasoning
framework aimed at improving the reliability and traceability
of RALMs, whose core idea is to leverage reasoning
trajectories generated by the LLM itself. The framework
involves constructing self-reasoning trajectories through
three processes: a relevance-aware process, an evidence-
aware selective process, and a trajectory analysis process.
We evaluated our framework across four public datasets (two
short-form QA datasets, one long-form QA dataset, and one
fact verification dataset) to demonstrate its superiority. Our
method can outperform existing state-of-the-art models and
achieve performance comparable with GPT-4, using only
2,000 training samples.
Introduction
The Retrieval-Augmented Language Model (RALM), also
known as Retrieval-Augmented Generation (RAG), has be-
come a crucial enhancement for Large Language Models
(LLMs) by integrating external knowledge during inference.
Despite their advanced capabilities in language understand-
ing and generation (Brown et al. 2020; Touvron et al. 2023),
LLMs are prone to producing hallucinated and inaccurate
content, especially in knowledge-intensive tasks (Ji et al.
2023). Augmenting LLMs with relevant information ob-
tained from external sources like Wikipedia and search en-
gines has proven effective in reducing these inaccuracies
(Guu et al. 2020; Lewis et al. 2020; Borgeaud et al. 2022;
Izacard et al. 2022; Asai et al. 2024). This approach has
proven effective in mitigating the factual hallucinations that
*Jingbo Zhou is the corresponding author.
Copyright © 2025, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.
Who painted the ceiling of the Florence Cathedral?
3
Question
Relevant-Aware Process
1
The provide documents are relevant with question
Evidence-Aware Selective Process
2
Document 1: … painted surface, was started in
1568 by Giorgio Vasari and Federico Zuccari and
would last till 1579. The upper portion, near the
lantern, representing The 24 Elders of Apocalypse
was finished by Vasari before his death …
Trajectory Analysis Process
The ceiling of the Florence Cathedral was painted
by Federico Zuccari and his collaborators, beginning
in 1568 and completing in 1579. The work was
started by Giorgio Vasari and later continued by
Federico Zuccari, with contributions from various
artists, including Domenico Cresti [1][3]
This piece indicates that the painting of the Florence
Cathedral was started in 1568 by Giorgio Vasari and
Federico Zuccari, which directly answers the
question.
Giorgio Vasari, Federico Zuccari
Final Answer
SELF-REASONING
Documents
Figure 1: An example of how SELF-REASONING framework
generates reasoning trajectories.
are inherent in LLMs (Kwiatkowski et al. 2019; Petroni et al.
2021; Ram et al. 2023).
Nevertheless, there are still limitations associated with
RALMs, particularly concerning reliability and traceability.
Firstly, the reliability of the retrieved information remains a
substantial concern. Previous studies have shown that noisy
retrieval can adversely affect the performance of an LLM
(Menick et al. 2022; Li et al. 2023), as irrelevant data can
lead to misguided responses and disturb the model’s abil-
ity to leverage its intrinsic knowledge effectively. Secondly,
the interpretability and traceability of outputs generated by
RALMs need to be improved. Although RALMs incorporate
retrieved documents during both the training and inference
arXiv:2407.19813v3  [cs.CL]  19 Dec 2024
phases, they may fail to explicitly cite these documents, thus
complicating the process of tracing and verifying the claims
made by LLMs. To improve the retrieval robustness, recent
studies have explored incorporating external tools such as
natural language inference (NLI) models (Honovich et al.
2022) and document summarization models during infer-
ence (Yoran et al. 2023; Xu et al. 2024). However, the effec-
tiveness of these external tools largely influences the overall
performance of RALMs. Additionally, training and optimiz-
ing these auxiliary models require additional costs. Conse-
quently, identifying the most appropriate training and selec-
tion methods for NLI and summarization models remains a
critical challenge in leveraging these approaches.
To address the above limitations, we propose a novel end-
to-end SELF-REASONING framework to improve the per-
formance of RALMs. For convenience, we will also re-
fer to this framework as SELF-REASONING RAG and use
the terms interchangeably. Our intuition is that the explicit
self-reasoning trajectory crafted by LLMs can improve both
the retrieval robustness and accuracy in question answering.
During the pre-training phase, while an LLM primarily fo-
cuses on knowledge acquisition, it does not learn to reason
from retrieved documents to generate answers. To address
this, a feasible approach is to incorporate reasoning trajecto-
ries into a post-training phase. Such an approach could po-
tentially teach the model to reason and distinguish relevant
and irrelevant documents, thereby enhancing its query re-
sponse accuracy. An example of how our SELF-REASONING
framework generates reasoning trajectories is illustrated in
Figure 1. In contrast, as shown in the middle part of Figure
2, the conventional RALM methods gather all documents
in a non-selective manner, leading to the distraction of the
LLM by irrelevant content and consequently resulting in the
generation of erroneous answers.
Our framework constructs self-reasoning trajectories
comprising three processes: 1) a Relevance-Aware Process
(RAP), which instructs the LLM to judge the relevance
between the retrieved documents and the question, 2) an
Evidence-Aware Selective Process (EAP), which directs the
LLM to choose and cite relevant documents, and then auto-
matically select snippets of key sentences as evidence from
the cited documents, 3) a Trajectory Analysis Process (TAP),
which requires the LLM to synthesize a concise analysis
based on all gathered self-reasoning trajectories generated
by previous two processes and subsequently provide the
final inferred answer. Furthermore, we propose a gradual
training method by employing stage-wise masking strategies
to enhance the performance of our framework. We summa-
rize our contributions as follows:
• We propose a novel end-to-end
SELF-REASONING
framework that improves the robustness of RALMs by
leveraging reasoning trajectories generated by the LLM
itself, without the need for external tools.
• We carefully design three processes to enhance the in-
terpretability and traceability of RALMs by requiring
LLMs to explicitly generate snippets and citations from
documents, and further explain the reason why cited doc-
uments can help answer the question.
• We evaluate our framework on four public datasets (two
short-form QA, one long-form QA, and one fact verifica-
tion), demonstrating that our method surpasses existing
state-of-the-art models in performance using only 2,000
training samples.
Related Work
Retrieval-augmented LMs
Many studies have investigated augmenting the performance
of LLMs with externally retrieved information (Izacard et al.
2022; Guu et al. 2020; Borgeaud et al. 2022) and some of
them pre-train language models with retrieved passages. For
works focusing on RALMs with citations, Menick et al.
(2022); Nakano et al. (2021) instruct or train an LLM to
answer questions with retrieved documents while providing
citations. Gao et al. (2023b) proposes an end-to-end system
to retrieve supporting evidence and generate answers with
citations, while only focusing on prompting without updat-
ing their model weights. Other works instruct or fine-tune
LLMs to use external tools to retrieve dynamically (Schick
et al. 2023; Yao et al. 2023; Jiang et al. 2023), which of-
fers an adaptive method of when and what to search. Gao
et al. (2023a) improves the attribution and factuality of lan-
guage models by taking outputs of LLMs and applying a
post-process retrieve-and-edit approach.
Robustness for RALMs
To improve the robustness of RALMs, previous works can
be divided into two categories. The first category utilizes re-
trieved documents to enhance the Chain of Thought (CoT).
For example, IRCoT (Trivedi et al. 2023) iteratively uses re-
trieved documents to generate CoT, which is then used to
retrieve further documents in subsequent steps. ReAct (Yao
et al. 2023) introduces an iterative CoT paradigm that in-
tegrates reasoning with search results. However, irrelevant
retrievals may produce misguided CoT, adversely affecting
LLM performance (Menick et al. 2022; Li et al. 2023).
To address the issue of irrelevant retrieval information, the
second category proposes using external modules to process
retrieved documents during inference. For instance, Yoran
et al. (2023) utilize a natural language inference model to
filter out irrelevant documents, Yan et al. (2024) employ a
retrieval evaluator to classify documents based on their qual-
ity, and Xu et al. (2024) and Yu et al. (2023) apply models
to filter out or compress retrieved documents. Baek et al.
(2023) deploy a separate small language model as a veri-
fier to detect and correct errors in LLMs during retrieval.
A method presented by Asai et al. (2024), which appears
most similar to our approach, develops a technique that in-
structs models to retrieve information using specifically de-
signed reflection tokens. However, this approach needs to
train extra critic models and generator models to predict the
reflection tokens, which requires tens of thousands of extra
training samples.
Unlike the second group of works, which rely on exter-
nal tools or additional modules to eliminate irrelevant infor-
mation, the SELF-REASONING RAG method integrates self-
reasoning directly into the model’s architecture, thereby en-
Cite content:
[1] … the original start date was
January 2002, but was pushed to
February 7 in Los Angeles, …
Reason to cite:
This piece provides information on
the commencement and location of
filming for 'Catch Me If You Can',
indicating that it started in April 2002
When was Catch Me
If You Can made?
Answer
Generation
Retrieved documents
(Relevant Aware Process）
(Evidence Aware Selective Process）
(Trajectory Analysis Process）
Basic LLMs
Retrieval Augmented  LLMs
1989
2000
2002
Relevant:
True
Relevant Reason:
The provided documents
are relevant with question.
Cite content:
[1] … the original start date was
January 2002, but was pushed to
February 7 in Los Angeles, …
Reason to cite:
This piece provides information on
the commencement and location of
filming for 'Catch Me If You Can',
indicating that it started in April 2002.
Analysis:
The film 'Catch Me If You Can' was made in
2002. It started filming in April 2002 in Park
Avenue, just outside the Waldorf-Astoria
Hotel, and moved to Orange, New Jersey,
before returning to Brooklyn for bank and
courthouse scenes [1].
(Self-Reasoning Short Answer）
(RAG Answer）
(Raw LLM Answer）
Self-Reasoning with Trajectories
the film due to her busy schedule.
The original start date was
January 2002, but was pushed to
February 7 in Los Angeles,
California. Locations …
Input Question
Output
❌
❌
✅
Figure 2: An illustration of the SELF-REASONING framework. The upper is the basic LLMs which answer the question by
inherent knowledge. The middle is the standard retrieval augmented LMs, which use retrieved documents to help answer the
question. The bottom is our SELF-REASONING framework which uses self-generated reason trajectories to output answers.
hancing the performance of LLMs and providing a more ef-
ficient and scalable solution. Further related works on LLMs
for reasoning are discussed in the Appendix.
Preliminary
We formally define the problem of retrieval augmented gen-
eration with self-reasoning. Given a query q and a corpus of
documents D, an LLM-generated answer with m statements
and n tokens can be defined as y = (s1, s2, · · · , sm) =
(w1, w2, · · · , wn), where si is the i-th statement and wj is
the j-th token in the generated answer. In addition, for long-
form QA settings, each statement si should cite a list of doc-
uments Ci = {c(1)
i , c(2)
i , ...}, where c(k)
i
∈D. In our work,
we train an LLM (e.g. LLaMA2) to first generate reasoning
trajectories τ through self-reasoning and then to generate an-
swers y∗(short-form answers) on condition of τ. The model
output is y = concat(τ, y∗), which is the concatenation of
τ and y∗. Note that the generations of τ and y∗are done in
a single pass within the SELF-REASONING framework.
Method
Here we provide a detailed implementation of the self-
reasoning process which involves three processes: 1) a
Relevance-Aware Process (RAP), 2) an Evidence-Aware Se-
lective Process (EAP), and 3) a Trajectory Analysis Process
(TAP). An illustration of our SELF-REASONING framework
is shown in Figure 2. Additionally, we outline the process of
data generation and quality control, and present the specifics
of model training.
Relevance-Aware Process
In this work, we choose DPR (Karpukhin et al. 2020) and
Contriever (Izacard et al. 2021) as default retrievers R to
recall the top-k relevant documents. When presented with
a question and a set of documents, people can determine
whether the question is relevant to the retrieved documents.
Therefore, we first instruct the model to judge the relevance
between the retrieved documents D and the given question q.
We further request the model to explicitly generate reasons
explaining why given documents are identified as relevant.
The output should include two fields as relevant and rele-
vant reason, as depicted in Figure 2. If all of the retrieved
documents are irrelevant, the model should provide an an-
swer based on the internal knowledge acquired during its
pre-training phase. We define the self-reasoning trajectories
generated by RAP as τr.
Evidence-Aware Selective Process
When answering a question, people generally first identify
the crucial sentences from the provided documents and then
cite or highlight them as key points. This process of cit-
ing the document facilitates reading comprehension and can
serve as a technique for combining multiple short answers
to address various aspects. While people may carry out this
selective process and citation instantaneously, LLMs need to
formulate the self-reasoning trajectories explicitly.
In our work, we require the LLM to explicitly state the
reason why the selected sentence is supportive and plausible
in answering the question. We define the selected sentence
as evidence in our paper. Specifically, after retrieving the
top-k documents, the self-reasoning method for Evidence-
Aware Selective Process can be formulated as follows: First,
we instruct the LLM to choose relevant documents and au-
tomatically select snippets of key sentences for the selected
documents. Then, we request the LLM to output the reason
why the selected snippets can answer the question. The in-
termediate output is a list containing multiple contents, each
content should include two fields, as cite content and rea-
son for cite, which is illustrated in Figure 2. We define the
self-reasoning trajectories generated by EAP as τe.
Trajectory Analysis Process
Finally, we consolidate all the self-reasoning trajectories (τr
and τe) in the previous processes together to form a chain
of reasoning snippets, thereby enhancing the overall perfor-
mance of the retrieval augmentation generation. Specifically,
we ask the LLM to analyze the reasoning trajectories within
itself and ultimately to output a concise analysis and a short
answer. We instruct the LLM to output content with two
fields as analysis and answer, which is shown in Figure 2.
We define the self-reasoning trajectories generated by TAP
as τa. In this work, the analysis output is defined as a long-
form answer, and the answer output is defined as a short-
form answer. In the experiment section, we further explored
the performance of long-form and short-form QA settings.
Data Generation and Quality Control
Training Data Generation.
For the Relevance-Aware
Process data generation, as manually labeling the relevant
and irrelevant documents is label-intensive, we request GPT-
4 (OpenAI 2023) to generate answers as ground truth.
Specifically, we instruct GPT-4 to generate labels regarding
irrelevant fields, and further to output the reasons why the
given documents cannot answer the question. We concate-
nate the given question and the retrieved documents as pos-
itive samples. For negative samples, we randomly select a
different question from the training set and retrieve the top-k
documents related to it. These documents are then concate-
nated with the initial question to form negative samples. To
avoid order bias in the training data, we shuffle the order of
the documents.
For the EAP and TAP data generation, manually annotat-
ing the citation and writing the self-reasoning process for
each question is not feasible in practice. Therefore, we fol-
low a similar process to RAP, we first instruct GPT-4 to
generate a snippet of selected documents and subsequently
output the reasoning process as trajectories. The method for
constructing the EAP training data is the same as RAP ex-
cept that the instructions given to GPT-4 are different. The
details of the instructions are shown in the Appendix.
Data Quality Control.
For training data generation, cor-
rect and comprehensive reasoning trajectories are very im-
portant. When training an LLM, the quality of the train-
ing samples is more important than the quantity (Zhou
et al. 2023). As we cannot guarantee the correctness of self-
reasoning trajectories and citations by GPT-4, we develop
two efficient methods to control the quality of data genera-
tion: 1) The first method is to use the off-the-shelf tools 1 in
Gao et al. (2023b) to automatically verify the performance
of data generation for document citations. We calculate the
citation precision and recall score for each training sample
and filter out scores lower than our pre-defined thresholds
δp and δr, for citation precision and recall, respectively. 2)
Second, though the validation of self-reasoning trajectories
and citations generated by GPT-4 is challenging, verifying
the correctness of the final answer is straightforward. There-
fore, we filter out the trajectories that lead to the incorrect
answers and only keep the correct ones. We totally generate
10,000 training samples by GPT-4, after the filtering strat-
egy by quality control, we finally keep 2,000 training sam-
ples with high quality. More details and pseudo-codes can
be found in the Appendix.
Model Training
We train the self-reasoning RAG model ϕ by our constructed
corpus which is augmented with self-reasoning trajectories
τ using the standard language modeling objective, maximiz-
ing likelihood:
max
ϕ
E(q,τ,y)∼Dsr log pϕ(y | τ, q)pϕ(τ | q)
(1)
where τ = τr ⊕τe ⊕τa are the self-reasoning trajectories, ⊕
is a concatenation operator, τr, τe, τa are trajectories gener-
ated by above three processes respectively. q is the provided
question, and y is the model output, including the intermedi-
ate reason trajectories and the final answer. Dsr is the train-
ing corpus augmented with self-reasoning trajectories.
During training, we observed that it is more challenging
to ensure the correctness of an LLM with 13B parameters
when generating long reasoning trajectories than short ones.
We hypothesize that an LLM’s effective reasoning length is
limited and exceeding this limit might lead to error accumu-
lation during the inference stage. Therefore, we propose a
gradual training method by employing stage-wise masking
strategies to gradually learn to generate long trajectories.
Specifically, we propose a stage-wise training process
while we train the LLM stage by stage. In the first stage, we
mask the trajectories produced by the next two stages (EAP
and TAP) and train the model with a learning rate ra. Then
in the second stage, we only mask the trajectories generated
by TAP and train the model with a learning rate rb. Finally,
we concatenate the reasoning trajectories from all stages and
put them into a self-reasoning LLM for end-to-end training
with a learning rate rc. Hyper-parameters for training are
described in the Appendix.
Experiments
Datasets and Settings
To demonstrate the effectiveness of our proposed SELF-
REASONING framework, we conduct an extensive experi-
mental evaluation on two short-form QA datasets (Natu-
ralQuestion (Kwiatkowski et al. 2019) and PopQA (Mallen
et al. 2023)), one long-form QA dataset (ASQA (Stelmakh
1Tools are available at https://github.com/princeton-nlp/ALCE/
tree/main
Models
NaturalQuestion
PopQA
FEVER
ASQA
(acc)
(acc)
(acc)
(em-recall)
(precision)
(recall)
Baselines without retrieval
LLaMA27B
19.2
18.4
23.2
10.2
-
-
LLaMA213B
24.0
22.6
25.3
15.3
-
-
LLaMA27B-chat
20.2
21.5
26.5
16.3
-
-
LLaMA213B-chat
23.2
25.9
28.4
18.3
-
-
Baselines with retrieval
LLaMA27B
27.8
47.8
39.8
28.5
13.6
9.59
LLaMA213B
34.0
48.1
35.2
26.8
21.8
16.3
LLaMA27B-chat
27.4
52.9
43.4
25.3
34.5
33.2
LLaMA213B-chat
32.7
53.5
53.4
26.4
39.4
38.4
Vicuna7B (Chiang et al. 2023)
28.0
55.2
62.4
24.3
45.7
40.8
Vicuna13B (Chiang et al. 2023)
35.4
56.1
60.6
27.3
51.3
50.2
LLaMA2-FT7B
36.8
54.4
67.5
28.5
47.2
45.4
ReAct (Yao et al. 2023)
-
-
64.6
-
-
-
RECOMP (Xu et al. 2024)
38.4
-
-
-
-
-
Self-RAG7B (Asai et al. 2024)
37.2
54.9
70.2
30.0
66.9
67.8
Self-RAG13B (Asai et al. 2024)
38.8
55.8
72.1
31.7
70.3
71.3
SELF-REASONING7B
38.0
54.2
78.6
33.9
66.3
70.8
SELF-REASONING13B
41.4
57.3
83.9
35.2
71.2
72.3
GPT-4
46.6
62.5
87.7
41.3
75.6
68.5
Table 1: Performance comparisons with different baseline models on two short-form QA datasets, a long-form QA dataset, and
a fact verification dataset. The numbers with bold black represent the best results excluding GPT-4. The results are averaged
over five runs, and presented with standard variance values omitted (all ≤2%).
et al. 2022)), and one fact verification dataset (FEVER
(Thorne et al. 2018)). Detailed descriptions of the datasets
can be found in the Appendix. We explore off-the-shelf
retrievers. We use DPR (Karpukhin et al. 2020) and
Contriever-MS MARCO (Izacard et al. 2021) to retrieve the
top five documents from Wikipedia.
By default, we use DPR as a retriever for the NQ, as DPR
has been fine-tuned on the high-quality NQ data. On the
PopQA, where question and answer pairs are created based
on Wikipedia in 2022, therefore, for the PopQA, we use the
December 2020 preprocessed Wikipedia corpus provided by
(Izacard et al. 2022) and use Contriever as a retriever. For the
ASQA dataset, we use GTR (Ni et al. 2022) as a retrieval
that corresponds to the experimental settings in (Gao et al.
2023b). More settings can be found in the Appendix.
Evaluation Metrics
We use different evaluation metrics for short-form QA, long-
form QA, and fact verification tasks.
Short-form QA metrics.
We report accuracy for short-
form QA tasks, which is based on whether ground-truth
answers are included in the model predictions instead of
strictly requiring exact matching, following Mallen et al.
(2023); Schick et al. (2023).
Long-form QA metrics.
For long-form QA tasks, we re-
port the EM recall as a correctness metric, and the citation
recall and the citation precision for citation quality, which
are the same as the metrics in (Gao et al. 2023b).
Fact verification metrics.
For the fact verification task,
we report the accuracy as a metric, which is a three-class
classification accuracy, following Thorne et al. (2018).
Baseline Models
Baseline models without retrieval.
We evaluate strong
open-source pre-trained LLMs as baseline models. For basic
LLMs, we test LLaMA2-7B, LLaMA2-13B (Touvron et al.
2023) and its instruction-tuned chat version LLaMA2-Chat-
7B, LLaMA2-Chat-13B.
Baseline models with retrieval.
First, we benchmark the
models using the LLaMA2 and the Vicuna (Chiang et al.
2023) series models for baselines. Additionally, for a fair
comparison, we also include LLaMA2-FT, where LLaMA2
is fine-tuned on all the training samples generated by GPT-
4 except the self-reasoning trajectories. To establish strong
baselines, we compare our method against RECOMP (Xu
et al. 2024), ReAct (Yao et al. 2023), and Self-RAG (Asai
et al. 2024), all of which are trained with extra GPT-4 gener-
ated samples or external tools. We also compare our frame-
work with GPT-4 (OpenAI 2023). We include categorical
comparisons with the baseline models in the Appendix.
Main Results
Table 1 shows the performance comparisons with different
methods on the four public datasets. For short-form QA
evaluations, the performance of LLMs with augmented re-
trieval is consistently better than that of basic ones, affirm-
ing the effectiveness of the augmented approach. Notably,
Figure 3: Noise robustness experiment results on three different datasets: (a) On the left is the NQ dataset, (b) in the middle is
the PopQA dataset, and (c) on the right is the FEVER dataset. The Self-RAG and Vicuna are 13B parameter size models.
under the same order of magnitude parameters, our SELF-
REASONING framework outperforms most of the strong
baseline LLMs. Specifically, compared to the Self-RAG, our
framework is an end-to-end system trained with only 2,000
self-reasoning trajectory samples. In contrast, the Self-RAG
requires training additional critic LMs to predict reflection
tokens using an additional 46,000 instances generated by
GPT-4. This efficiency not only simplifies the training pro-
cess but also significantly reduces resource consumption.
In the context of long-form QA evaluations, for the met-
rics of EM recall, it needs to comprehend multiple docu-
ments and merge answers. The EAP and TAP are specifi-
cally designed for multi-document reading comprehension,
enabling our performance to surpass other baselines. In
terms of citation evaluation metrics, the SELF-REASONING
RAG can achieve better results than GPT-4 in ASQA cita-
tion recall metrics (72.3 vs. 68.5). This is largely due to the
reasoning trajectories generated in the EAP, which can en-
hance the recall and precision of citation evaluation, leading
to more interpretable and traceable generations.
For fact verification evaluations, we observed that SELF-
SEASONING is dominantly superior to all baseline models.
Our method achieves a much higher accuracy rate than the
Self-RAG model (83.9 vs. 72.1). The RAP in our framework
is designed to judge the relevance between the retrieved doc-
uments and the question, which leads to a notable enhance-
ment in accuracy for this fact verification task.
To clearly demonstrate the practical applications and ben-
efits of our SELF-REASONING framework, we provide a case
study for a more in-depth analysis in Appendix, which illus-
trates how our framework operates in real-world scenarios.
Analysis
Ablation Study
We conduct an ablation study on two short-form QA datasets
and a fact verification dataset to analyze the individual
contributions of each process within our proposed SELF-
REASONING framework. We further explore the effective-
ness of the gradual learning (GL) method and the qual-
Models
NQ
PopQA
FEVER
(acc)
(acc)
(acc)
ORIGIN
41.4
57.3
83.9
w/o (RAP)
39.9
54.3
72.2
w/o (EAP)
37.2
53.2
78.4
w/o (TAP)
38.2
53.4
81.2
w/o (GL)
39.5
55.3
81.2
w/o (QC)
37.7
54.2
80.8
Table 2: The ablation study on two short-form QA datasets
and a fact verification dataset with 13B parameter size mod-
els. In the table, the ORIGIN represents our self-reasoning
model enhanced with self-generated trajectories.
Models
NQ
PopQA
FEVER
(acc)
(acc)
(acc)
LLaMA2
32.7
53.5
53.4
+ trajectory
38.3
54.2
79.2
Vicuna
35.4
56.1
60.6
+ trajectory
38.5
56.4
79.6
Table 3: The analysis on the effectiveness of self-reasoning
trajectories with 13B parameter size models. In the table,
the +trajectory indicates the result of the baseline model is
enhanced with self-generated trajectories by our framework.
ity control (QC) of data generation (a detailed analysis de-
scribed in the Appendix). The main ablation study results
are shown in Table 2 and Table 3.
Effectiveness of RAP.
First, we evaluate the effect of
the RAP. The removal of the RAP causes the overall per-
formance to drop in two short-form QA datasets and a
fact verification dataset, suggesting that preliminary con-
sideration of the relevance between questions and retrieved
documents can help improve performance. We notice that
the performance declines most significantly in the FEVER
dataset. Detecting irrelevant documents is critical in the fact-
verification task. Our model will immediately output NotE-
noughInfo if it detects that all documents are irrelevant.
Effectiveness of EAP.
Then we evaluate the effect of the
EAP. Removing the EAP causes the overall performance of
the average accuracy to decline from 60.9 to 56.3 in three
short-form QA datasets, which indicates that snippets of key
sentences and document citations generated through self-
reasoning are instrumental in boosting accuracy.
Effectiveness of TAP.
Finally, we evaluate the effect of
the TAP. When excluding the TAP, we can observe a per-
formance decline on all three datasets, demonstrating that
self-analysis based on two previous processes generated tra-
jectories can also improve the performance of LLMs. Note
that the analysis content generated by TAP is indispensable
for the long-form QA evaluation.
Effectiveness of Self-Reasoning Trajectory.
To verify
whether the trajectories generated by the self-reasoning
framework are truly effective, we put the trajectories gener-
ated by our SELF-REASONING framework into the original
baseline models as input prompts, and then use the baseline
models to regenerate the answers. We observe that incorpo-
rating self-generated trajectories can significantly enhance
performance in short QA tasks and fact verification tasks.
Retrieval Robustness Analysis
Retrievers are not perfect and past work has shown that noisy
retrieval can have negative effects on the performance of
LLMs (Petroni et al. 2020; Li et al. 2023). In this section,
we design two kinds of settings to validate the robustness
of RALMs. In the first setting, we test whether the order of
the retrieved documents will affect the performance of the
RALMs. Specifically, after retrieving the top-k documents
using retrievals with a descending relevance score, we ran-
domly shuffle the order of the retrieved documents and then
input them to an LLM. In the second setting, we test how
noisy documents impact the performance of LLMs. When
retrieving the top-k documents from the given question, we
randomly replace 50% of the retrieved documents with other
documents sampled from a different question in the dataset.
Figure 3 shows the noise robustness experiment results
on three datasets. Our SELF-REASONING framework con-
sistently outperforms the Self-RAG and Vicuna models. We
observe that random shuffling of retrieved documents has a
minimal impact on the performance of RALMs. If the pro-
vided documents are supportive, it is trivial for a RALM
to determine the correct answer. However, when presented
with noisy documents, all models experience a decline in
performance. The performance drop in our self-reasoning
framework is relatively minimal, demonstrating the robust-
ness of our method even when handling noisy documents.
Citation Analysis
As the automatic evaluation by the NLI model cannot de-
tect partially supported citations, we discuss the analysis of
citations with human evaluation in this section. Similarly to
Liu, Zhang, and Liang (2023), we conduct a human eval-
uation on two dimensions: 1) citation recall: annotators are
Figure 4: Human citation quality evaluation vs. automatic
citation evaluation on the long-form ASQA dataset.
given a statement and all documents that the statement refers
to and are asked to judge whether the documents fully sup-
port the given statement; 2) citation precision: given a state-
ment and one of its citations, annotators are asked to vali-
date whether the citation fully supports, partially supports
or does not support the statement. As shown in Figure 4, the
relative rankings by human evaluation align well with those
from the automatic evaluation, and the human evaluation of-
ten yields a closely higher score when compared with the
automatic evaluation. Details of human annotation can be
found in the Appendix.
Latency Analysis
We
also
compared
the
inference
latency
of
SELF-
REASONING RAG with that of Self-RAG and GPT-4. The
results show that our method maintains comparable latency
to Self-RAG while delivering substantial performance gains.
Detailed results are available in the Appendix.
Conclusion
RALMs can effectively enhance the performance of LLMs
in handling knowledge-intensive tasks. Despite their effec-
tiveness, notable concerns about their reliability and trace-
ability persist. To address these limitations, we propose a
novel SELF-REASONING framework to improve the perfor-
mance of RALMs by using reasoning trajectories generated
by the LLM itself. It is comprised of a relevance-aware pro-
cess, an evidence-aware selective process, and a trajectory
analysis process. We conduct extensive experiments on four
public datasets to demonstrate the superiority of our frame-
work over existing state-of-the-art models.
References
Asai, A.; Wu, Z.; Wang, Y.; Sil, A.; and Hajishirzi, H.
2024. Self-RAG: Learning to Retrieve, Generate, and Cri-
tique through Self-Reflection. In The Twelfth International
Conference on Learning Representations.
Baek, J.; Jeong, S.; Kang, M.; Park, J.; and Hwang, S. 2023.
Knowledge-Augmented Language Model Verification.
In
Bouamor, H.; Pino, J.; and Bali, K., eds., Proceedings of
the 2023 Conference on Empirical Methods in Natural Lan-
guage Processing, 1720–1736. Singapore.
Borgeaud, S.; Mensch, A.; Hoffmann, J.; Cai, T.; Ruther-
ford, E.; Millican, K.; Van Den Driessche, G. B.; Lespiau,
J.-B.; Damoc, B.; Clark, A.; et al. 2022. Improving language
models by retrieving from trillions of tokens. In Interna-
tional conference on machine learning, 2206–2240. PMLR.
Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;
Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,
A.; et al. 2020. Language models are few-shot learners. Ad-
vances in neural information processing systems, 33: 1877–
1901.
Chiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y.; Wu, Z.; Zhang, H.;
Zheng, L.; Zhuang, S.; Zhuang, Y.; Gonzalez, J. E.; Stoica,
I.; and Xing, E. P. 2023. Vicuna: An Open-Source Chatbot
Impressing GPT-4 with 90%* ChatGPT Quality.
Gao, L.; Dai, Z.; Pasupat, P.; Chen, A.; Chaganty, A. T.; Fan,
Y.; Zhao, V.; Lao, N.; Lee, H.; Juan, D.-C.; and Guu, K.
2023a. RARR: Researching and Revising What Language
Models Say, Using Language Models. In Rogers, A.; Boyd-
Graber, J.; and Okazaki, N., eds., Proceedings of the 61st
Annual Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), 16477–16508. Toronto,
Canada.
Gao, T.; Yen, H.; Yu, J.; and Chen, D. 2023b.
Enabling
Large Language Models to Generate Text with Citations.
In Bouamor, H.; Pino, J.; and Bali, K., eds., Proceedings
of the 2023 Conference on Empirical Methods in Natural
Language Processing, 6465–6488. Singapore.
Guu, K.; Lee, K.; Tung, Z.; Pasupat, P.; and Chang, M.-W.
2020. REALM: retrieval-augmented language model pre-
training. In Proceedings of the 37th International Confer-
ence on Machine Learning, ICML’20. JMLR.org.
Honovich, O.; Aharoni, R.; Herzig, J.; Taitelbaum, H.; Kuk-
liansy, D.; Cohen, V.; Scialom, T.; Szpektor, I.; Hassidim,
A.; and Matias, Y. 2022. TRUE: Re-evaluating factual con-
sistency evaluation. arXiv preprint arXiv:2204.04991.
Izacard, G.; Caron, M.; Hosseini, L.; Riedel, S.; Bojanowski,
P.; Joulin, A.; and Grave, E. 2021. Unsupervised dense in-
formation retrieval with contrastive learning. arXiv preprint
arXiv:2112.09118.
Izacard, G.; Lewis, P.; Lomeli, M.; Hosseini, L.; Petroni, F.;
Schick, T.; Dwivedi-Yu, J.; Joulin, A.; Riedel, S.; and Grave,
E. 2022. Few-shot learning with retrieval augmented lan-
guage models. arXiv preprint arXiv:2208.03299.
Ji, Z.; Lee, N.; Frieske, R.; Yu, T.; Su, D.; Xu, Y.; Ishii, E.;
Bang, Y. J.; Madotto, A.; and Fung, P. 2023. Survey of hal-
lucination in natural language generation. ACM Computing
Surveys, 55(12): 1–38.
Jiang, Z.; Xu, F.; Gao, L.; Sun, Z.; Liu, Q.; Dwivedi-Yu, J.;
Yang, Y.; Callan, J.; and Neubig, G. 2023. Active Retrieval
Augmented Generation. In Bouamor, H.; Pino, J.; and Bali,
K., eds., Proceedings of the 2023 Conference on Empirical
Methods in Natural Language Processing, 7969–7992. Sin-
gapore.
Karpukhin, V.; Oguz, B.; Min, S.; Lewis, P.; Wu, L.; Edunov,
S.; Chen, D.; and Yih, W.-t. 2020.
Dense Passage Re-
trieval for Open-Domain Question Answering. In Webber,
B.; Cohn, T.; He, Y.; and Liu, Y., eds., Proceedings of the
2020 Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), 6769–6781. Online.
Kwiatkowski, T.; Palomaki, J.; Redfield, O.; Collins, M.;
Parikh, A.; Alberti, C.; Epstein, D.; Polosukhin, I.; Devlin,
J.; Lee, K.; Toutanova, K.; Jones, L.; Kelcey, M.; Chang, M.-
W.; Dai, A. M.; Uszkoreit, J.; Le, Q.; and Petrov, S. 2019.
Natural Questions: A Benchmark for Question Answering
Research.
Transactions of the Association for Computa-
tional Linguistics, 7: 452–466.
Kwon, W.; Li, Z.; Zhuang, S.; Sheng, Y.; Zheng, L.; Yu,
C. H.; Gonzalez, J. E.; Zhang, H.; and Stoica, I. 2023. Ef-
ficient Memory Management for Large Language Model
Serving with PagedAttention. In Proceedings of the ACM
SIGOPS 29th Symposium on Operating Systems Principles.
Lewis, P.; Perez, E.; Piktus, A.; Petroni, F.; Karpukhin, V.;
Goyal, N.; K¨uttler, H.; Lewis, M.; Yih, W.-t.; Rockt¨aschel,
T.; et al. 2020.
Retrieval-augmented generation for
knowledge-intensive nlp tasks. Advances in Neural Infor-
mation Processing Systems, 33: 9459–9474.
Li, D.; Rawat, A. S.; Zaheer, M.; Wang, X.; Lukasik, M.;
Veit, A.; Yu, F.; and Kumar, S. 2023.
Large Language
Models with Controllable Working Memory. In Rogers, A.;
Boyd-Graber, J.; and Okazaki, N., eds., Findings of the As-
sociation for Computational Linguistics: ACL 2023, 1774–
1793. Toronto, Canada.
Liu, N.; Zhang, T.; and Liang, P. 2023. Evaluating Verifia-
bility in Generative Search Engines. In Bouamor, H.; Pino,
J.; and Bali, K., eds., Findings of the Association for Compu-
tational Linguistics: EMNLP 2023, 7001–7025. Singapore.
Mallen, A.; Asai, A.; Zhong, V.; Das, R.; Khashabi, D.;
and Hajishirzi, H. 2023.
When Not to Trust Language
Models: Investigating Effectiveness of Parametric and Non-
Parametric Memories. In Rogers, A.; Boyd-Graber, J.; and
Okazaki, N., eds., Proceedings of the 61st Annual Meeting
of the Association for Computational Linguistics (Volume 1:
Long Papers), 9802–9822. Toronto, Canada.
Menick, J.; Trebacz, M.; Mikulik, V.; Aslanides, J.; Song,
F.; Chadwick, M.; Glaese, M.; Young, S.; Campbell-
Gillingham, L.; Irving, G.; et al. 2022. Teaching language
models to support answers with verified quotes.
arXiv
preprint arXiv:2203.11147.
Min, S.; Michael, J.; Hajishirzi, H.; and Zettlemoyer, L.
2020.
AmbigQA: Answering Ambiguous Open-domain
Questions. In Webber, B.; Cohn, T.; He, Y.; and Liu, Y., eds.,
Proceedings of the 2020 Conference on Empirical Methods
in Natural Language Processing (EMNLP), 5783–5797. On-
line.
Nakano, R.; Hilton, J.; Balaji, S.; Wu, J.; Ouyang, L.; Kim,
C.; Hesse, C.; Jain, S.; Kosaraju, V.; Saunders, W.; et al.
2021. Webgpt: Browser-assisted question-answering with
human feedback. arXiv preprint arXiv:2112.09332.
Ni, J.; Qu, C.; Lu, J.; Dai, Z.; Hernandez Abrego, G.; Ma,
J.; Zhao, V.; Luan, Y.; Hall, K.; Chang, M.-W.; and Yang, Y.
2022. Large Dual Encoders Are Generalizable Retrievers. In
Goldberg, Y.; Kozareva, Z.; and Zhang, Y., eds., Proceedings
of the 2022 Conference on Empirical Methods in Natural
Language Processing, 9844–9855. Abu Dhabi, United Arab
Emirates.
OpenAI, R. 2023. Gpt-4 technical report. arxiv 2303.08774.
View in Article, 2: 13.
Pan, Z.; Luo, H.; Li, M.; and Liu, H. 2024. Chain-of-action:
Faithful and multimodal question answering through large
language models. arXiv preprint arXiv:2403.17359.
Petroni, F.; Lewis, P.; Piktus, A.; Rockt¨aschel, T.; Wu,
Y.; Miller, A. H.; and Riedel, S. 2020.
How context af-
fects language models’ factual predictions. arXiv preprint
arXiv:2005.04611.
Petroni, F.; Piktus, A.; Fan, A.; Lewis, P.; Yazdani, M.;
De Cao, N.; Thorne, J.; Jernite, Y.; Karpukhin, V.; Mail-
lard, J.; Plachouras, V.; Rockt¨aschel, T.; and Riedel, S.
2021. KILT: a Benchmark for Knowledge Intensive Lan-
guage Tasks. In Toutanova, K.; Rumshisky, A.; Zettlemoyer,
L.; Hakkani-Tur, D.; Beltagy, I.; Bethard, S.; Cotterell, R.;
Chakraborty, T.; and Zhou, Y., eds., Proceedings of the 2021
Conference of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Language Tech-
nologies, 2523–2544. Online.
Press, O.; Zhang, M.; Min, S.; Schmidt, L.; Smith, N.; and
Lewis, M. 2023. Measuring and Narrowing the Composi-
tionality Gap in Language Models. In Bouamor, H.; Pino,
J.; and Bali, K., eds., Findings of the Association for Compu-
tational Linguistics: EMNLP 2023, 5687–5711. Singapore.
Ram, O.; Levine, Y.; Dalmedigos, I.; Muhlgay, D.; Shashua,
A.; Leyton-Brown, K.; and Shoham, Y. 2023. In-Context
Retrieval-Augmented Language Models.
Transactions of
the Association for Computational Linguistics, 11: 1316–
1331.
Rasley, J.; Rajbhandari, S.; Ruwase, O.; and He, Y. 2020.
Deepspeed: System optimizations enable training deep
learning models with over 100 billion parameters. In Pro-
ceedings of the 26th ACM SIGKDD International Confer-
ence on Knowledge Discovery & Data Mining, 3505–3506.
Schick, T.; Dwivedi-Yu, J.; Dess`ı, R.; Raileanu, R.; Lomeli,
M.; Zettlemoyer, L.; Cancedda, N.; and Scialom, T. 2023.
Toolformer: Language models can teach themselves to use
tools. arXiv preprint arXiv:2302.04761.
Stelmakh, I.; Luan, Y.; Dhingra, B.; and Chang, M.-W. 2022.
ASQA: Factoid Questions Meet Long-Form Answers. In
Goldberg, Y.; Kozareva, Z.; and Zhang, Y., eds., Proceedings
of the 2022 Conference on Empirical Methods in Natural
Language Processing, 8273–8288. Abu Dhabi, United Arab
Emirates.
Thorne, J.; Vlachos, A.; Christodoulopoulos, C.; and Mittal,
A. 2018. FEVER: a Large-scale Dataset for Fact Extraction
and VERification. In Walker, M.; Ji, H.; and Stent, A., eds.,
Proceedings of the 2018 Conference of the North American
Chapter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long Papers),
809–819. New Orleans, Louisiana.
Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.;
Babaei, Y.; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale,
S.; et al. 2023. Llama 2: Open foundation and fine-tuned
chat models. arXiv preprint arXiv:2307.09288.
Trivedi, H.; Balasubramanian, N.; Khot, T.; and Sabharwal,
A. 2023. Interleaving Retrieval with Chain-of-Thought Rea-
soning for Knowledge-Intensive Multi-Step Questions. In
Proceedings of the 61st Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long Papers),
10014–10037.
Wang, X.; Wei, J.; Schuurmans, D.; Le, Q.; hsin Chi,
E. H.; and Zhou, D. 2022.
Self-Consistency Improves
Chain of Thought Reasoning in Language Models. ArXiv,
abs/2203.11171.
Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.;
Chi, E.; Le, Q. V.; Zhou, D.; et al. 2022.
Chain-of-
thought prompting elicits reasoning in large language mod-
els. Advances in neural information processing systems, 35:
24824–24837.
Xu, F.; et al. 2024.
RECOMP: Improving Retrieval-
Augmented LMs with Context Compression and Selective
Augmentation. In The Twelfth International Conference on
Learning Representations.
Xu, S.; Pang, L.; Shen, H.; Cheng, X.; and Chua, T.-s.
2023. Search-in-the-chain: Towards the accurate, credible
and traceable content generation for complex knowledge-
intensive tasks. arXiv preprint arXiv:2304.14732.
Yan, S.-Q.; Gu, J.-C.; Zhu, Y.; and Ling, Z.-H. 2024.
Corrective retrieval augmented generation. arXiv preprint
arXiv:2401.15884.
Yao, S.; Zhao, J.; Yu, D.; Du, N.; Shafran, I.; Narasimhan,
K.; and Cao, Y. 2023. ReAct: Synergizing Reasoning and
Acting in Language Models. In International Conference
on Learning Representations (ICLR).
Yoran, O.; Wolfson, T.; Ram, O.; and Berant, J. 2023. Mak-
ing retrieval-augmented language models robust to irrele-
vant context. arXiv preprint arXiv:2310.01558.
Yu, W.; Zhang, H.; Pan, X.; Ma, K.; Wang, H.; and
Yu, D. 2023.
Chain-of-note: Enhancing robustness in
retrieval-augmented language models.
arXiv preprint
arXiv:2311.09210.
Zhou, C.; Liu, P.; Xu, P.; Iyer, S.; Sun, J.; Mao, Y.; Ma, X.;
Efrat, A.; Yu, P.; Yu, L.; et al. 2023. Lima: Less is more for
alignment. arXiv preprint arXiv:2305.11206.
Zhou, D.; Scharli, N.; Hou, L.; Wei, J.; Scales, N.; Wang, X.;
Schuurmans, D.; Bousquet, O.; Le, Q.; and hsin Chi, E. H.
2022. Least-to-Most Prompting Enables Complex Reason-
ing in Large Language Models. ArXiv, abs/2205.10625.
Appendix
More Related Work of LMs for Reasoning
One of the most well-known methods of using LLMs for
reasoning is the Chain-of-Thought (CoT) (Wei et al. 2022),
which demonstrates the capability of LLMs to create their
thinking process for problem-solving. Zhou et al. (2022)
proposes a least-to-most prompting for solving complex
tasks. Wang et al. (2022) introduces a method to reason with
self-consistency. Press et al. (2023) proposes a method to
further improve the chain of thought by reasoning explicitly
instead of implicitly.
Recent works have extended beyond the internal reason-
ing ability of LLMs to include interactions with external
tools (e.g., search engines or retrievers) for solving com-
plex tasks. The ReAct (Yao et al. 2023) presents an itera-
tive paradigm to combine reasoning and acting with LLMs
for tackling language reasoning and decision-making tasks.
Xu et al. (2023) introduces a framework to enable informa-
tion retrieval and LLMs to interact with each other effec-
tively with chain-of-query decomposition. Pan et al. (2024)
proposes a novel framework named Chain-of-Action (CoA),
which integrates a reasoning retrieval method to decompose
complex questions into chains of configurable actions.
Different from the above works, which are mostly
based on relatively large LLMs (e.g., ChatGPT), our pro-
posed method focuses on enhancing smaller LLMs (e.g.,
LLaMA2) using only a limited number of samples to
achieve high robustness and interpretability through single-
step interaction.
Instructions
The instructions for GPT-4 to generate self-reasoning tra-
jectories are shown in Figure 5 (the short-form and long-
form QA tasks) and Figure 6 (the fact verification task). The
words in the orange font are key fields that need to be gen-
erated.
Datasets Description
We conducted an extensive experimental evaluation of two
short-form QA datasets, one long-form QA dataset, and a
fact verification dataset.
NaturalQuestion (NQ) (Kwiatkowski et al. 2019) con-
tains real user questions issued to the Google search and
answers found from Wikipedia by the annotators. NQ is cre-
ated to train and evaluate automated question answering sys-
tems.
PopQA (Mallen et al. 2023) is a large-scale open-domain
question answering dataset, consisting of entity-centric QA
pairs. Each question is made by converting a knowledge
triplet retrieved from Wikidata using a template. In this
work, we use PopQA to evaluate performance in long-tail
settings.
ASQA (Stelmakh et al. 2022) is a long-form factoid
dataset, and most questions can be answered by Wikipedia.
Each question originates from AmbigQA (Min et al. 2020)
and represents an ambiguous query that requires multiple
short answers to cover various aspects. The dataset provides
a long-form answer that contains all short answers.
FEVER (Thorne et al. 2018) is a fact verification dataset
that contains claims generated by rewriting sentences ex-
tracted from Wikipedia and subsequently verified without
knowledge of the sentence from which they were derived.
The claims are classified as Supported, Refuted, or NotE-
noughInfo.
Experiment settings
Training settings.
During gradual learning, we fine-tune
the LLaMA-2 (Touvron et al. 2023) model with our self-
reasoning framework for 3 epochs with a batch size set to
32, leveraging the DeepSpeed library (Rasley et al. 2020)
and the ZeRO optimizer, and we use parameter partitioning
ZeRO stage 3 with float16 precision. The learning rate ra
for the first stage is set to 5e-5, the learning rate rb for the
second stage is set to 3e-5, and the learning rate rc for the
final stage is set to 1e-5. Our SELF-REASONING 13B model
is trained on the NVIDIA Tesla 8 × V100 32GB GPU for 4
hours, while the 7B model is trained for 2 hours.
Inference settings.
We use the vLLM 2 framework (Kwon
et al. 2023) to accelerate the inference speed during infer-
ence. The codes follow the Apache-2.0 license agreement.
We use greedy decoding in all experiments to ensure deter-
ministic generations. We test the temperature within a range
of {0.2, 0.4, 0.6, 0.8, 1.0}, finally we set the temperature to
0.2, as we observed lower temperature results in better per-
formance in the open-domain question answering task. The
maximum generation length is set to 2048 for our model.
All baseline models are tested with zero-shot settings for
short-form QA datasets, and with one-shot settings for the
long-form QA and fact verification datasets.
Other settings.
For the document retrieval, we retrieve the
top-k relevant documents, and the k is set to 5. We use the
DPR and the Contriever in short-form QA settings. For long-
form QA, we use GTR as a retrieval and evaluate it using
one-shot to instruct the model to generate citations. For the
data generation quality control setting, the threshold for cita-
tion recall is set to 0.8, and the threshold of citation precision
is set to 0.8.
Categorical Comparisons
We differentiate our method from existing strong baseline
models by categorizing and comparing it across five dimen-
sions, as presented in Table 4. As illustrated in the table, our
method can stand out in several key aspects.
First, our SELF-REASONING method is the only end-to-
end framework among existing methods that can improve
performance without relying on external models or tools.
Second, our method eliminates the need for external mod-
ules during both the training and inference phases. In practi-
cal applications, our framework does not need to call multi-
ple tools or modules. Third, our framework requires a sig-
nificantly smaller dataset for training the LLM compared
to other methods, needing only 2,000 samples with self-
reasoning trajectories. This efficiency in training drastically
2Codes are available at https://github.com/vllm-project/vllm
Models
End-to-End
External module
Train data for LLM
(train)
(inference)
(data)
Self-Reasoning (Ours)
Y
N
N
No Need
2K
Self-RAG (Asai et al. 2024)
N
N
N
No Need
145K (Generator)/ 46K (Critic)
ReAct (Yao et al. 2023)
N
N
Y
No Need
No Need
RECOMP (Xu et al. 2024)
N
Y
Y
152K
No Need
Table 4: Categorical comparisons with strong baseline models. External module (train) and External module (data) refer to
whether the external module needs to be trained and the number of samples required, respectively. External module (inference)
indicates whether the external module is needed during the inference stage. Train data for LLM indicates the number of training
samples needed to train with LLMs.
Algorithm 1: Data Quality Control
Input: the origin self-reasoning dataset D0 generated by GPT-4
Output: filtered high quality self-reasoning dataset Dsr
1: Initialize the evaluation metrics program P
2: Initialize the citation score tools T (Gao et al. 2023b)
3: for i = 1 to N do
4:
Evaluate whether the answer of sample di is correct using
program P
5:
if True and di ∈Long-form QA dataset then
6:
Compute citation recall score sr for sample di
7:
Compute citation precision score sp for sample di
8:
if sr ≥δr and sp ≥δp then
9:
Add the training sample to Dl
10:
end if
11:
end if
12:
if True and di /∈Long-form QA dataset then
13:
Add the training sample to Ds
14:
end if
15: end for
16: Dsr = Dl ∪Ds
17: return Dsr
lowers the resources and time needed, making our method
both cost-effective and scalable for practical applications.
Case Study
In our case study, as illustrated in Figure 7, we compare the
responses generated by the raw LLM, the standard RALM
(e.g. LLaMA2 with retrievals), and our SELF-REASONING
method. The challenge involves reconciling information
from multiple retrieved documents to provide a correct an-
swer as the retrieved documents contained noisy data.
The response from the raw LLM (e.g., LLaMA2) sug-
gested that the film was made in 2000, based on its inher-
ited knowledge. However, this answer is incorrect and is a
hallucination generated by the LLM. The standard RALM
approach yielded 1989 as the production date. This answer
was based on unrelated details from the retrieved documents,
showing a lack of context-specific understanding and robust-
ness for noisy retrieved documents.
Our SELF-REASONING framework provided a compre-
hensive approach by assessing the relevance and context of
retrieved documents. First, in the relevance-aware process,
the documents were identified as relevant based on their con-
tent regarding the production dates and events surrounding
the film. Second, in the evidence-aware selective process,
the model retrieved the first documents, which highlighted
the original start date as January 2002, with filming com-
mencing in February 2002 (highlighted in green in the fig-
ure). This information was crucial in establishing the time-
line for the film’s production. The model can also understand
of the difference between the production date and the release
date in the third retrieved document (highlighted in red in the
figure). In the trajectory analysis process, the correct time-
line was deduced by piecing together self-generated trajecto-
ries, leading to the conclusion that the film Catch Me If You
Can was indeed produced in 2002. As the case illustrated
above, by leveraging relevant documents and focusing on
contextual evidence, our SELF-REASONING framework can
achieve a precise and well-supported answer, highlighting
its utility and robustness in complex information retrieval
tasks.
More Analysis on Ablation Study
Effectiveness of Gradual Learning.
Further, we validate
the effect of gradual learning. Rather than training an LLM
with a stage-by-stage approach, we initially concatenate the
reasoning trajectories from all stages and put them into the
LLM for end-to-end training. As shown in Table 2, the per-
formance decline can be observed in three datasets, suggest-
ing that gradual learning can help improve performance.
Effectiveness of Quality Control.
The effect of quality
control on data generation is also evaluated in our work. In-
stead of using the filtered high-quality training samples, we
randomly sampled 2,000 unfiltered training samples gener-
ated by GPT-4. As shown in Table 2, the substitution of un-
filtered training data leads to the degradation of the model
results.
Human Evaluation
We randomly sample 100 examples from the ASQA dataset
and annotate the outputs of selected models. Each sample is
then assigned to two people for annotation. Each annotator
is required to verify the citation recall and citation precision
acorrding to our provided scheme. The annotation scheme is
inspired by (Gao et al. 2023b) as follows:
Citation Recall.
The annotators are shown the question q,
the statement si, and all of its citations Ci, and they assess if
the set of citations fully support the statement (recall=1) or
Models
NQ
ASQA
LLaMA2
0.19
1.92
Self-RAG
2.08
2.38
SELF-REASONING
2.19
2.32
GPT-4
35.5
40.5
Table 5: The latency experiment results on NQ and ASQA
datasets. The average inference latency per question is eval-
uated using 7B-parameter models.
if they do not support all the claims (recall=0). We calculate
the overall recall score for the model by averaging the recall
scores of all statements.
Citation Precision.
The annotators are shown the ques-
tion q and a statement si and one of its citation c(k)
i
∈Ci.
We ask the annotator if the citation fully supports, partially
supports, or does not support the generated claims in si. Ci-
tation c(k)
i
has a citation precision of 1 if si has a recall of 1,
and c(k)
i
fully or partially supports si. Finally, we calculate
the overall precision score for the model by averaging the
precision scores of all statements.
The Pseudo-code of Data Quality Control
The details and pseudo-code of data quality control are il-
lustrated in Algorithm 1. In the table, D0 is the origin self-
reasoning dataset generated by GPT-4. P is the program to
evaluate the answer according to the metrics. T is the off-
the-shelf tool (Gao et al. 2023b) to calculate the citation
scores. N is the number of total samples from the origin
dataset, and di is the i-th training sample. sr and sc are cita-
tion recall and precision scores for each sample respectively.
δr and δp are thresholds of citation recall and precision re-
spectively. Dl is filtered trajectory data for long-form QA,
and Ds is filtered dataset for short-form QA and fact veri-
fication dataset. Dsr is the final high quality self-reasoning
training dataset.
Latency Analysis
We conduct extra experiments to measure inference latency
across our approach, Self-RAG, and GPT-4. The results
demonstrated that our method achieved better performance
while maintaining latency comparable to Self-RAG. The re-
sults of average inference latency per question for the NQ
and ASQA datasets are shown in Table 5.
Instructions
# Role
You are an experienced expert, skilled in answering various questions.
# Task
Please answer the question according to the provided reference evidence as
required.
# Reference Evidence
[1] Retrieved Document  {{DOCUMENT 1}}
}
[2] Retrieved Document  {{DOCUMENT 2}}
}
[3] Retrieved Document  {{DOCUMENT 3}}
}
[4] Retrieved Document  {{DOCUMENT 4}}
}
[5] Retrieved Document  {{DOCUMENT 5}}
}
# Requirements
1. First,  please judge whether the provided documents are relevant with the
question, and put it in the relevant field. If the provided content is irrelevant to
the question, explain the reason in the relevant reason field, then you can give
the answer with your internal knowledge.
2. If possible, answer the question in points and provide explanations.
3. If the content in the answer comes from different pieces of evidence, you
need to cite the sequence number of the evidence at the end of the sentence.
The citation format is shown below: [1], [1,3].
4. Place each cited piece of evidence in the cite_list field, cite content field to
store each paragraph of cited content (omitted words can be replaced by ...),
cite reason is used to store your thoughts and analysis of this content, how
this paragraph can answer the question.
5. Put the long answer content in the analysis field, and put the short
answer(no more than 10 words) in the answer field.
# Question
{{QUESTION}}
Figure 5: The instructions for the GPT-4 to generate the self-reasoning trajectories for short-form and long-form QA tasks.
Instructions
# Role
You are an experienced expert, skilled in answering various questions.
# Task
Please answer the question according to the provided reference evidence as
required.
# Reference Evidence
[1] Retrieved Document  {{DOCUMENT 1}}
[2] Retrieved Document  {{DOCUMENT 2}}
[3] Retrieved Document  {{DOCUMENT 3}}
[4] Retrieved Document  {{DOCUMENT 4}}
[5] Retrieved Document  {{DOCUMENT 5}}
# Requirements
1. First, please judge whether the provided documents are relevant with the
claim, and put it in the relevant field. If the provided content is irrelevant to the
question, explain the reason in the relevant reason field, then you can give the
answer with your internal knowledge.
2. If possible, answer the question in points and provide explanations.
3. If the content in the answer comes from different pieces of evidence, you
need to cite the sequence number of the evidence at the end of the sentence.
The citation format is shown below: [1], [1,3].
4. Place put each cited piece of evidence in the list, use cite content field to
store each paragraph of cited content (omitted words can be replaced by ...),
cite reason is used to store your thoughts and analysis of this content, how this
paragraph can answer the question.
5. Put the long answer content in the analysis field, and put the short answer
(SUPPORT/REFUTE/NOT ENOUGH INFO) in the answer field.
# Question
You should check whether the given claim is supported or refuted by above the
evidence.  If supported，output SUPPORT, if controversial, output REFUTE;  if
the evidence provided is not enough to make a decision or the provided
evidence is irrelevant, output NOT ENOUGH INFO.
{{CLAIM}}
Figure 6: The instructions for the GPT-4 to generate the self-reasoning trajectories for the fact verification task.
Question
When was catch me if you can made?
Retrieved document #1: The film due to her busy schedule. The original start date was
January 2002, but was pushed to February 7 in Los Angeles, California. Locations included
Burbank, Downey, New York City, Quebec City and Montreal. …
Retrieved document #2: eventually turned to him for help in catching other check forgers.
Development for the film started in 1980, but did not progress until 1997, when Spielberg's
DreamWorks bought the film rights to Abagnale's book. David Fincher, Gore Verbinski, Lasse
Hallström, Miloš Forman, and Cameron Crowe had all been possible candidates for director
before Spielberg decided to direct. …
Retrieved document #3: It was funded independently and had a budget of $800,000. The film
was Sommers's directorial debut and was written by him. When production finished, the studio
that had promised to distribute the movie had gone out of business, and it was eventually
released by MCA Inc. in July 1989. The movie…
(Other retrieved documents are omitted.)
Raw LLM Answer:  2000 ❌
Standard RAG Answer: 1989 ❌
Self-Reasoning Answer:
Relevant: True
Relevant Reason: The provided documents are relevant with question.
Cite content: [1] … the original start date was January 2002, but was pushed to February 7 in
Los Angeles, …
Reason to cite: This piece provides information on the commencement and location of filming
for 'Catch Me If You Can', indicating that it started in April 2002
Cite content: [3] … When production finished, the studio that had promised to distribute the
movie had gone out of business, and it was eventually released by MCA Inc. in July 1989., …
Reason to cite: This piece provides information on the release date of 'Catch Me If You Can',
indicating that it was eventually released by MCA Inc. in July.
Analysis:（long answer）
The film 'Catch Me If You Can' was made in 2002. It started filming in April 2002 in Park Avenue,
just outside the Waldorf-Astoria Hotel, and moved to Orange, New Jersey, before returning to
Brooklyn for bank and courthouse scenes [1].
Final Answer：2002 ✅ （short answer）
(Relevant Aware Process)
(Evidence Aware Selective Process)
(Trajectory Analysis Process)
Figure 7: A Case Study. We present the self-reasoning trajectories generated by our framework during inference and demonstrate
how they can logically generate the correct answer.
