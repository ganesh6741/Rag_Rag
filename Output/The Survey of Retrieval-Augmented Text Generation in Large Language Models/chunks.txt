The Survey of Retrieval-Augmented Text Generation in Large
Language Models
YIZHENG HUANG and JIMMY X. HUANG, York University, Canada
Retrieval-Augmented Generation (RAG) merges retrieval methods with deep learning advancements to address
the static limitations of large language models (LLMs) by enabling the dynamic integration of up-to-date
external information. This methodology, focusing primarily on the text domain, provides a cost-effective
solution to the generation of plausible but possibly incorrect responses by LLMs, thereby enhancing the
accuracy and reliability of their outputs through the use of real-world data. As RAG grows in complexity and
incorporates multiple concepts that can influence its performance, this paper organizes the RAG paradigm into
four categories: pre-retrieval, retrieval, post-retrieval, and generation, offering a detailed perspective from the
retrieval viewpoint. It outlines RAG’s mechanics and discusses the field’s progression through the analysis of
significant studies. Additionally, the paper introduces evaluation methods for RAG, addressing the challenges
faced and proposing future research directions. By offering an organized framework and categorization, the
study aims to consolidate existing research on RAG, clarify its technological underpinnings, and highlight its
potential to broaden the adaptability and applications of LLMs.
CCS Concepts: • Computing methodologies →Natural language generation; • Information systems
→Information retrieval.
Additional Key Words and Phrases: retrieval-augmented generation, information retrieval, large language
model
ACM Reference Format:
Yizheng Huang and Jimmy X. Huang. 2018. The Survey of Retrieval-Augmented Text Generation in Large
Language Models. In Proceedings of Make sure to enter the correct conference title from your rights confirmation
emai (Conference acronym ’XX). ACM, New York, NY, USA, 37 pages. https://doi.org/XXXXXXX.XXXXXXX
1
Introduction
The advent of ChatGPT has significantly impacted both academia and industry due to its interactive
capabilities and widespread application, establishing itself as a leading artificial intelligence tool
[55, 60, 77]. At the core of ChatGPT is the large language model (LLM) GPT-4, as detailed by
[1], which has seen numerous enhancements to its predecessors, showcasing exceptional abilities
in a variety of Natural Language Processing (NLP) tasks [78]. Despite these advancements, the
adoption of LLMs has highlighted several critical issues primarily due to their reliance on extensive
datasets. This reliance restricts their ability to incorporate new information post-training, leading
to three primary challenges. First, the focus on broad and general data to maximize accessibility and
applicability results in subpar performance in specialized areas. Second, the rapid creation of online
data, combined with the significant resources required for data annotation and model training,
Authors’ Contact Information: Yizheng Huang, hyz@yorku.ca; Jimmy X. Huang, jhuang@yorku.ca, York University, Toronto,
Ontario, Canada.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee
provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the
full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.
Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires
prior specific permission and/or a fee. Request permissions from permissions@acm.org.
Conference acronym ’XX, June 03–05, 2018, Woodstock, NY
© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-XXXX-X/18/06
https://doi.org/XXXXXXX.XXXXXXX
, Vol. 1, No. 1, Article . Publication date: August 2018.
arXiv:2404.10981v2  [cs.IR]  23 Aug 2024
2
Huang et al.
Fig. 1. An example of RAG benefits ChatGPT resolves questions that cannot be answered beyond the scope
of the training data and generates correct results.
hinders LLMs’ ability to stay updated. Third, LLMs are susceptible to generating convincing yet
inaccurate responses, known as “hallucinations”, which can mislead users.
Addressing these challenges is crucial for LLMs to be effectively utilized across various domains.
A promising solution is the integration of Retrieval-Augmented Generation (RAG) technology,
which supplements models by fetching external data in response to queries, thus ensuring more
accurate and current outputs. Figure 1 illustrates how RAG can enable ChatGPT to provide precise
answers beyond its initial training data.
Since its introduction by Lewis et al. [83] in 2020, RAG has seen rapid development, especially
with the rise of models like ChatGPT. Despite these advancements, there remains a noticeable gap
in the literature regarding a comprehensive analysis of the mechanisms underlying RAG and the
progress achieved by subsequent studies. Moreover, the field suffers from fragmented research
focuses and inconsistent terminology for similar methods, leading to confusion. This survey seeks
to bridge this gap by offering a structured overview of RAG, categorizing various approaches, and
providing an in-depth understanding of the current research landscape, with a focus on textual
applications given their prominence in recent research.
To provide clarity and structure, this paper is organized as follows: Section 2 outlines the
overall RAG workflow, dividing the methodologies into pre-retrieval, retrieval, post-retrieval, and
generation phases. Sections 3 through 6 explore the core techniques within each phase. Section
7 focuses on the evaluation methodologies for RAG. Section 8 summarizes the reviewed studies,
detailing the retrievers and generators used, while Section 9 discusses challenges and future research
directions, extending beyond text-based studies to include multimodal data applications. The paper
concludes with Section 10.
Other related surveys provide valuable insights into the evolving RAG landscape from different
angles. Gao et al. [38] identified three key stages in RAG development: pre-training enhancement,
inference, and fine-tuning. Zhao et al. [162] focused on the diverse applications of RAG, including
text, code, image, and video generation, emphasizing augmented intelligence in generative tasks.
Meanwhile, Hu et al. [48] explored Retrieval-Augmented Language Models (RALMs), examining how
interactions between retrievers, language models, and augmentations influence model architectures
and applications.
In this paper, we aim to offer a comprehensive and unified framework for understanding RAG from
an information retrieval (IR) perspective, identifying key challenges and areas for improvement. We
delve into the core technologies that drive RAG, assessing their effectiveness in addressing retrieval
and generation tasks. Additionally, this survey introduces the evaluation methods employed in RAG
research, highlights current limitations, and proposes promising avenues for future exploration.
, Vol. 1, No. 1, Article . Publication date: August 2018.
The Survey of Retrieval-Augmented Text Generation in Large Language Models
3
Fig. 2. The unified RAG core concepts with basic workflow.
2
RAG Framework
The hallucinations are largely attributed to LLMs’ inability to access up-to-date information. This
limitation stems from the models’ reliance on their training datasets. RAG proposes a solution to
this issue by supplementing the LLM’s training data with current information from external sources
through a retrieval model, thereby enabling the generation of accurate responses. RAG presents a
more cost-effective alternative to the extensive training and fine-tuning processes typically required
for LLMs. It allows for the dynamic incorporation of fresh information via traditional retrieval
methods or pre-trained LMs, without the need to directly integrate this new data into the LLM. This
feature makes RAG both flexible and scalable, facilitating its application across different LLMs for
various purposes. The information retrieved through RAG is derived from real-world data, authored
by humans, which not only simplifies the generation process but also increases the reliability of
the generated responses.
Research by Khandelwal et al. [72] demonstrates that accessing relevant information from the
training dataset itself can significantly improve LLM performance, highlighting the effectiveness of
RAG. Over time, RAG has evolved from a means of providing supplementary information to enabling
multiple interactions between the retrieval and generation components. This involves conducting
several rounds of retrieval to refine the accuracy of the information retrieved and iteratively
improve the quality of the generated output. Toolkits such as LangChain1 and LlamaIndex2 have
modularized the RAG approach, enhancing its adaptability and expanding its range of applications.
Despite these toolkits employing diverse methodologies to tackle different aspects of RAG—from
multiple search iterations to iterative generation—they maintain adherence to the fundamental
RAG workflow. This consistency is crucial for understanding their operation and pinpointing
opportunities for further development.
2.1
Basic RAG Workflow
Figure 2 represents the unified RAG core concepts with basic workflow. The workflow of RAG
begins with the creation of an index comprising external sources. This index serves as the basis for
retrieving relevant information through a retriever model based on a specific query. The final step
1https://www.langchain.com
2https://www.llamaindex.ai
, Vol. 1, No. 1, Article . Publication date: August 2018.
4
Huang et al.
involves a generator model, which combines the retrieved information with the query to produce
the desired output.
2.1.1
Indexing. Efficient retrieval begins with comprehensive indexing, where data preparation
is key. This stage involves text normalization processes such as tokenization, stemming, and
the removal of stop words to enhance the text’s suitability for indexing [98]. Text segments are
then organized into sentences or paragraphs to facilitate more focused searches, allowing for
the pinpointing of segments containing pertinent keywords. The integration of deep learning
has revolutionized indexing through the use of pretrained LMs for generating semantic vector
representations of texts. These vectors are stored, enabling rapid and precise retrieval from extensive
data collections, significantly enhancing retrieval efficiency.
2.1.2
Retrieval. While traditional retrieval methods, such as the BM25 algorithm [44], focus on
term frequency and presence for document ranking, they often overlook the semantic information
of queries. Current strategies leverage pretrained LMs like BERT [29], which capture the semantic
essence of queries more effectively. These models improve search accuracy by considering synonyms
and the structure of phrases, thereby refining document ranking through the detection of semantic
similarities. This is typically achieved by measuring vector distances between documents and
queries, combining traditional retrieval metrics with semantic understanding to yield search results
that are both relevant and aligned with user intent.
2.1.3
Generation. The generation phase is tasked with producing text that is both relevant to
the query and reflective of the information found in the retrieved documents. The usual method
involves concatenating the query with the retrieved information, which is then fed into an LLM
for text generation [85]. Although ensuring the generated text’s alignment and accuracy with
the retrieved content presents challenges, it is also essential to strike a balance between adhering
closely to the source material and infusing the output with creativity. The generated text should
accurately convey the information from the retrieved documents and align with the query’s intent,
while also offering the flexibility to introduce new insights or perspectives not explicitly contained
within the retrieved data.
2.2
RAG Paradigm
The RAG paradigm organizes research within the domain, offering a straightforward yet robust
framework to enhance LLM performance. Central to RAG is its search mechanism, crucial for
generating high-quality outcomes. Therefore, this paradigm is structured into four main phases
from a retrieval perspective: pre-retrieval, retrieval, post-retrieval, and generation. Both single-hop
and multi-hop retrieval approaches, encompassing iterative retrieve-generate cycles, follow this
four-phase structure. Figure 3 is the taxonomy tree of RAG’s core techniques.
2.2.1
Pre-Retrieval. The pre-retrieval phase of retrieval-augmented generation lays the foundation
for successful data and query preparation, ensuring efficient information retrieval. This phase
includes essential tasks to prepare for effective data access.
Indexing. The process starts with indexing, which establishes an organized system to enable fast
and accurate retrieval of information. The specificity of indexing depends on the task and data
type. For example, sentence-level indexing is beneficial for question-answering systems to precisely
locate answers, while document-level indexing is more appropriate for summarizing documents to
understand their main concepts and ideas.
Query Manipulation. After indexing, query manipulation is performed to adjust user queries for
a better match with the indexed data. This involves query reformulation [61, 155], which rewrites
, Vol. 1, No. 1, Article . Publication date: August 2018.
The Survey of Retrieval-Augmented Text Generation in Large Language Models
5
the query to align more closely with the user’s intention; query expansion [51], which extends the
query to capture more relevant results through synonyms or related terms; and query normalization,
which resolves differences in spelling or terminology for consistent query matching.
Data Modification. Data modification is also critical in enhancing retrieval efficiency. This step
includes preprocessing techniques like removing irrelevant or redundant information to improve
the quality of results and enriching the data with additional information such as metadata to boost
the relevance and diversity of the retrieved content [6].
2.2.2
Retrieval.
Search & Ranking. The retrieval stage is the combination of search and ranking. It focuses on
selecting and prioritizing documents from a dataset to enhance the quality of the generation
model’s outputs. This stage employs search algorithms to navigate through the indexed data,
finding documents that match a user’s query. After identifying relevant documents, the process of
initially ranking these documents starts to sort them according to their relevance to the query.
2.2.3
Post-Retrieval. The post-retrieval phase serves to refine the initially retrieved documents to
improve the quality of text generation. This phase consists of re-ranking and filtering, each aimed
at optimizing the document selection for the final generation task.
Re-Ranking. In the re-ranking step, the documents previously retrieved are reassessed, scored,
and reorganized. The objective is to more accurately highlight the documents most relevant to
the query and diminish the importance of the less relevant ones. This step involves incorporating
additional metrics and external knowledge sources to enhance precision. In this context, pre-trained
models with superior accuracy but lower efficiency can be effectively employed due to the limited
set of candidate documents available [54].
Filtering. Filtering aims to remove documents that fail to meet specified quality or relevance
standards [56, 74]. This can be done through several approaches, such as establishing a minimum
relevance score threshold to exclude documents below a certain relevance level. Furthermore, the
use of feedback from users or prior relevance evaluations assists in adjusting the filtering process,
guaranteeing that only the most relevant documents are retained for text generation.
2.2.4
Generation. The generation stage is a crucial component of the RAG process, responsi-
ble for leveraging retrieved information to enhance the quality of the generated response. This
stage encompasses several sub-steps aimed at producing content that is readable, engaging, and
informative.
Enhancing. At the heart of the generation phase is the enhancement step, where the objective is
to merge the retrieved information with the user’s query to create a coherent and relevant response.
This includes the process of elaboration, adding extra details to the retrieved content to enrich
it. Efforts are focused on improving the output’s quality by increasing its clarity, coherence, and
stylistic appeal through methods such as rephrasing and restructuring. Information from various
sources is combined to offer a comprehensive perspective, and verification is conducted to ensure
the accuracy and relevance of the content.
Customization. Customization is user-centric. It encompasses tailoring content in two primary
ways. First, it aligns the generated output with relevant information retrieved in earlier stages,
ensuring consistency and accuracy by incorporating key knowledge. Second, it adapts the content
to suit user-specific factors such as intended audience, situational context, and personal prefer-
ences, shaping the response to be both contextually relevant and user-centric. This dual focus on
, Vol. 1, No. 1, Article . Publication date: August 2018.
6
Huang et al.
RAG
Pre-Retrieval
Indexing
REALM [42];
kNN-LMs [72];
RAG [83];
Webgpt [100];
RETRO [9];
MEMWALKER [13];
Atlas [94];
Chameleon [63];
AiSAQ [126];
PipeRAG [64];
LRUS-CoverTree [93]
Query Ma-
nipulation
Webgpt [100];
DSP [73];
CoK [86];
IRCOT [131];
Query2doc [137];
Step-Back [163];
PROMPTAGATOR [27];
KnowledGPT [140];
Rewrite-Retrieve-
Read [94];
FLARE [65];
RQ-RAG [12];
RARG [159];
DRAGIN [124]
Data Modification
RA-DIT [89];
RECITE [125];
UPRISE [20];
GENREAD [156];
KnowledGPT [140];
Selfmem [21];
RARG [159]
Retrieval
Search & Ranking
REALM [42];
kNN-LMs [72];
RAG [83];
FiD [58];
Webgpt [100];
RETRO [9];
ITRG [34];
RA-DIT [89];
SURGE [70];
PRCA [151];
AAR [157];
ITER-RETGEN [121];
UPRISE [20];
MEMWALKER [13];
Atlas [94];
FLARE [65];
PlanRAG [81]
Post-Retrieval
Re-Ranking
Re2G [40];
DSP [73];
CoK [86];
FiD-TF [5];
ITER-RETGEN [121];
PROMPTAGATOR [27];
Selfmem [21];
DKS-RAC [53];
In-Context
RALM [112];
Fid-light [47];
GenRT [148]
Filtering
Webgpt [100];
Self-RAG [4];
FiD-TF [5];
PROMPTAGATOR [27];
RECOMP [147];
DKS-RAC [53];
CoK [86];
FILCO [141];
BlendFilter [135];
CRAG [149]
Generation
Enhancing
FiD [58];
Webgpt [100];
DSP [73];
IRCOT [131];
ITRG [34];
RA-DIT [89];
PRCA [151];
RECITE [125];
UPRISE [20];
GENREAD [156];
Selfmem [21];
MEMWALKER [13];
Atlas [94]
Customization
LAPDOG [52];
PersonaRAG [160];
ERAGent [123];
ROPG [118]
Fig. 3. Taxonomy tree of RAG’s core techniques
integrating relevant knowledge and adjusting to diverse contextual demands forms the basis of
effective customization in RAG.
3
Pre-Retrieval
3.1
Indexing
One of the most commonly used indexing structures in traditional information retrieval systems
is the inverted index. This structure associates documents with words to form a vocabulary list,
allowing users to quickly locate references where a specific word appears within a collection of
documents. The vocabulary list here refers to the set of all unique words present in the document
collection, while the reference includes the documents where the word appears, along with the
word’s position and weight within those documents. However, traditional indexing structures
struggle to retrieve documents that are semantically related to a user’s query but do not contain
the exact query terms.
To address this limitation, retrieval methods using dense vectors generated by deep learning
models have become the preferred choice. These vectors, also known as embeddings, capture
the semantic meaning of words and documents, allowing for more flexible and accurate retrieval.
Dense vector-based indexing methods can be categorized into three main types: graphs, product
quantization (PQ) [62], and locality-sensitive hashing (LSH) [28]. Since generating dense vectors
with large language models requires substantial resources, and the document collections to be
searched are typically vast, the core strategy of these indexing methods is based on approximate
nearest neighbor search (ANNS) [3]. This approach significantly speeds up the search process at
the cost of a slight reduction in search accuracy.
Graph. Using graphs to build indexes is a common practice in RAG. By indexing vectors with
a graph structure, the range of nodes where distances need to be computed during retrieval
can be limited to a local subgraph, thereby enhancing search speed. Several prominent methods
and tools have been developed using this approach. For example, k-nearest neighbor language
models kNN-LMs [72], as demonstrated by Khandelwal et al., integrate the kNN algorithm with
pre-trained language models. This method employs a datastore created from collections of texts
to dynamically retrieve contextually relevant examples, enhancing model performance without
requiring additional training. FAISS [68], a tool widely adopted for indexing in many studies [72, 73,
83], integrates enhancements like the Hierarchical Navigable Small World (HNSW) approximation
, Vol. 1, No. 1, Article . Publication date: August 2018.
The Survey of Retrieval-Augmented Text Generation in Large Language Models
7
[97] to further speed up retrieval [83]. WebGPT [100] showcases another practical application by
utilizing the Bing API3 for indexing based on actual user search histories, which illustrates the
potential of integrating real-world user data into the retrieval process. Additionally, other methods
like MEMWALKER [13] introduces innovative approaches to overcome limitations such as context
window size in large language models. It creates a memory tree from input text, segmenting the
text into smaller pieces and summarizing these segments into a hierarchical structure. Moreover,
LRUS-CoverTree method [93] designed another tree structure for k-Maximum Inner-Product
Search (k-MIPS) and achieves performance comparable with significantly lower index construction
time. These techniques facilitate efficient indexing and management of large information volumes,
demonstrating the versatility and effectiveness of graph-based approaches.
Product Quantization. PQ is one of the most representative methods for handling large-scale data.
It accelerates searches by segmenting vectors and then clustering each part for quantization. Unlike
graph-based methods, which speed up searches by reducing the number of vectors for distance
calculation, PQ achieves faster searches by reducing the time spent on calculating word distances.
Several implementations of PQ have emerged in RAG, each improving its efficiency and scalability
in different ways. PipeRAG [64] integrates PQ within a pipeline-parallelism framework to enhance
retrieval-augmented generation by optimizing retrieval intervals. Chameleon system [63] leverages
PQ in a disaggregated accelerator environment to balance memory usage and retrieval speed in RAG
tasks. AiSAQ [126] introduces an all-in-storage ANNS method that offloads PQ vectors from DRAM
to storage, drastically reducing memory usage while maintaining high recall. It demonstrates that
even with billion-scale datasets, memory usage can be minimized to around 10 MB with only minor
latency increases, making it a highly scalable solution for RAG systems.
Locality-sensitive Hashing. The core idea of LSH is to place similar vectors into the same hash
bucket with high probability. LSH uses hash functions that map similar vectors to the same or
nearby hash values, making it easier to find approximate nearest neighbours. In LSH, when a query
vector is hashed, the system quickly retrieves candidate vectors that share the same hash value.
This method reduces the dimensionality of the problem and can be implemented efficiently, but it
may introduce some inaccuracies due to the hashing process itself. While LSH is rarely used in RAG
systems compared to graph-based and PQ methods, it still offers a useful approach in scenarios
where speed is prioritized over the slight loss in accuracy.
3.2
Query Manipulation
Query manipulation is pivotal in enhancing the effectiveness and accuracy of modern IR systems. By
refining users’ original queries, it addresses challenges such as ambiguous phrasing and vocabulary
mismatches between the query and target documents. This process involves more than merely
replacing words with synonyms; it requires a deep understanding of user intent and the context
of the query, particularly in complex tasks like RAG. Effective query manipulation significantly
boosts retrieval performance, which in turn can greatly impact the quality of generated outputs.
The three primary approaches to query manipulation are query expansion, query reformulation,
and prompt-based rewriting.
Query Expansion. Query expansion involves augmenting the original query with additional
terms or phrases that are related to or synonymous with the query terms. In the context of
LLMs, query expansion can be more sophisticated, utilizing the model’s extensive knowledge to
generate contextually relevant expansions. This technique aims to improve recall by ensuring that
the retrieval process captures a broader range of relevant documents, accommodating different
3https://www.microsoft.com/en-us/bing/apis/bing-web-search-api
, Vol. 1, No. 1, Article . Publication date: August 2018.
8
Huang et al.
terminologies or expressions. Techniques such as synonym expansion, semantic similarity, or
leveraging external knowledge bases are commonly employed in query expansion. For example,
the method described in FiD [58] expands the query by retrieving a wider range of passages
using both sparse and dense retrieval techniques, enabling the model to aggregate evidence from
multiple sources and thereby improving the accuracy and robustness of generated answers. A more
advanced form of query expansion is demonstrated in Query2doc [137], where pseudo-documents
generated by LLMs enhance the original query, effectively bridging the gap between the user’s
input and the corpus information, which benefits both sparse and dense retrieval systems. Similarly,
KnowledGPT [140] broadens the scope of information accessed during retrieval by leveraging
external knowledge bases, further refining the retrieval process. The RARG [159] framework uses
an evidence-driven query expansion approach, incorporating a wide array of supporting documents
to generate informed and accurate counter-misinformation responses.
Query Reformulation. Query reformulation involves rephrasing or restructuring the original
query to enhance its effectiveness. This might include making the wording more specific, removing
vague terms, or adjusting the syntax to better align with the retrieval system’s requirements.
With LLMs, query reformulation can be dynamically driven by understanding the user’s intent
and context, allowing for more precise modifications that lead to improved retrieval results. This
reformulation process can also be informed by past queries or user interactions, adapting the query
to better fit the specific retrieval task. For instance, the RQ-RAG [12] model represents an advanced
form of query reformulation by rewriting, decomposing, and disambiguating queries, making it
particularly effective in scenarios that demand complex query handling. This approach ensures
that the refined query better matches the needed context, improving the relevance of retrieved
information. Rewrite-Retrieve-Read framework [94] adjusts the original query to optimize the
retrieval process, allowing the system to more effectively leverage retrieved data for generating
accurate responses. Additionally, FLARE [65] exemplifies query reformulation through its active
retrieval-augmented generation approach, which iteratively refines the query based on a feedback
loop between retrieval and generation, thereby enhancing the accuracy and relevance of the
retrieved information.
Prompt-based Rewriting. Prompt-based rewriting, particularly in the context of LLMs, represents
an innovative approach where the original query is embedded within a larger prompt or context
to guide the LLM’s response. This technique harnesses the model’s ability to understand and
generate language within a specific context, effectively rewriting the query to align with the desired
output. Prompt-based rewriting is especially powerful in scenarios where the retrieval process is
integrated into a generative workflow, allowing the system to adapt the query to various stages of
retrieval and generation. This approach may also involve dynamic prompts that evolve based on
interaction, further refining the retrieval process. For example, Step-Back [163] refines the query
context through carefully crafted prompts that guide the LLM’s reasoning process, ensuring that the
outputs are more aligned with the user’s intent, particularly in complex reasoning tasks. The CoK
[86] method focuses on dynamically adapting the knowledge source and using prompts to rewrite
the context in which a query is interpreted. This approach leverages prompt-based rewriting to
enable the LLM to effectively integrate and ground its responses based on various heterogeneous
knowledge sources. Additionally, Promptagator [27] discusses using prompt-based techniques to
adapt and rewrite the query to better align with the retrieval system’s expectations, particularly in
few-shot learning scenarios. These prompts guide the model in generating or refining the query to
optimize retrieval results.
, Vol. 1, No. 1, Article . Publication date: August 2018.
The Survey of Retrieval-Augmented Text Generation in Large Language Models
9
3.3
Data Modification
Document modification techniques play a critical role in enhancing retrieval performance, particu-
larly when integrated with LLMs. These techniques can be broadly categorized into Internal Data
Augmentation and External Data Enrichment. Internal Data Augmentation focuses on maximizing
the value of existing information within documents or models, while External Data Enrichment
introduces supplementary data from outside sources to fill gaps, provide additional context, or
broaden the scope of the content.
Internal Data Augmentation. Internal Data Augmentation leverages information already present
within documents or taps into the inherent knowledge embedded in LLMs. Techniques like para-
phrasing, where content is rewritten for improved readability or multiple perspectives, and summa-
rization, which condenses information while retaining core content, are commonly employed. Other
methods involve generating supplementary content or explanations that are contextually related
without introducing external data. For instance, RECITE [125] utilizes a model’s internal memory
to recite relevant information before generating responses, thus enhancing performance in tasks
like closed-book question answering without external data. KnowledGPT [140] similarly refines the
internal knowledge embedded within LLMs, optimizing its use during generation. GENREAD [156]
further demonstrates how pre-existing knowledge within LLMs can be used to generate context
that enhances task performance, bypassing the need for external sources. In another example,
the Selfmem [21] framework allows the model to iteratively use its own outputs as memory in
subsequent generation tasks. By selecting and utilizing the best internal outputs as memory, this
approach boosts model performance without depending on external memory resources.
External Data Enrichment. External Data Enrichment enhances document content by incorpo-
rating new information from external sources, enriching the overall context and accuracy. This
process can involve integrating facts, data, or contextual knowledge from external datasets or knowl-
edge bases. For example, RA-DIT [89] augments input prompts during fine-tuning by leveraging
large datasets like Wikipedia and CommonCrawl, enhancing the model’s capability in knowledge-
intensive tasks. The dual instruction tuning technique optimizes both the language model and the
retriever to more effectively incorporate retrieved information. UPRISE [20] demonstrates how
retrieving prompts from diverse task datasets improves model generalization in zero-shot scenarios
by enriching the context during inference. Additionally, RARG [159] exemplifies external data
enrichment by integrating scientific evidence from academic databases to strengthen responses
countering misinformation. This method involves a two-stage retrieval pipeline that identifies and
ranks relevant documents, which are then used to support and enhance the factual accuracy of
generated responses.
4
Retrieval
4.1
Search & Ranking
The search and ranking process within RAG is crucial for improving the relevance and accuracy
of generated outputs. Several methodologies have been developed to refine this process, each
contributing unique strategies for enhancing retrieval and ranking. For example, Atlas [59] and
AAR [157] both aim to improve the relevance of retrieved documents, but they approach this
challenge differently. Atlas focuses on optimizing the retriever’s ability to select contextually
relevant documents, especially in new domains with limited data, by employing few-shot learning
techniques such as Attention Distillation and Perplexity Distillation. AAR, on the other hand,
adapts retrieval preferences to better align with the requirements of LLMs, enhancing retrieval
generalization across tasks by training a smaller source model.
, Vol. 1, No. 1, Article . Publication date: August 2018.
10
Huang et al.
Fig. 4. An example of a typical RAG framework with interative retrieval strategy.
Additionally, IRCOT [131] and FLARE [65] introduce dynamic interactions within the retrieval
process, albeit with distinct goals. IRCOT integrates retrieval with chain-of-thought (CoT) reasoning,
interleaving these processes to ensure that each retrieval step supports the ongoing reasoning task.
FLARE, in contrast, adopts a confidence-based active retrieval mechanism, dynamically triggering
retrieval when the model generates low-confidence tokens. This approach is particularly useful in
scenarios where model confidence varies, as it allows the system to fetch additional information to
resolve uncertainties during the generation process.
When addressing domain-specific retrieval challenges, SURGE [70] and PRCA [151] offer different
solutions. SURGE uses a subgraph retriever to extract relevant subgraphs from knowledge graphs,
, Vol. 1, No. 1, Article . Publication date: August 2018.
The Survey of Retrieval-Augmented Text Generation in Large Language Models
11
integrating structured data into the retrieval process to improve the contextual understanding of
generated responses. The relational structure of knowledge graphs allows for more accurate and
informed retrieval. PRCA, in contrast, focuses on domain-specific abstractive summarization, using
a reward-driven approach to refine the retrieved content. This strategy is designed to optimize
content for the generator, particularly in scenarios where the generator functions as a black box,
thereby enhancing alignment between retrieval and generation.
MEMWALKER [13] presents a unique approach to handling long-context question answering by
incorporating an internal search and ranking mechanism within a memory tree structure. This
method navigates extensive memory stores, ensuring that the most relevant information is retrieved
and used for complex queries. Unlike other methods, MEMWALKER emphasizes efficient processing
of long texts through iterative navigation and summarization, rather than solely optimizing the
initial retrieval phase.
4.2
Retrieval Strategy
The retrieval strategies within RAG are vital for customizing the retrieval process to specific appli-
cation needs, with each strategy offering distinct advantages and addressing particular challenges.
In RAG, it is mostly the utilization of retrieval techniques rather than the exploration of retrieval
algorithms that is involved, so it is the strategy of retrieval that is usually considered in searching
and ranking. While basic RAGs are usually single-hop searches, i.e., they are retrieved only once
as generated supplementary material, today’s RAGs are mostly multi-hop searches, i.e., they are
searched several times through different search strategies until they are satisfied. In terms of
practical applications these strategies belong to the design on the engineering pipeline. Figure 4
shows a typical case of the RAG framework with iterative retrieval strategy. There are five main
retrieval strategies in RAG:
Basic Retrieval Strategy. Basic retrieval strategies typically follow a linear workflow, moving
sequentially through pre-retrieval, retrieval, post-retrieval, and generation phases. The Atlas [94]
framework exemplifies this straightforward approach, guiding the retrieval process efficiently from
start to finish without iterations or complex conditional modifications. REPLUG [122] similarly
follows this basic strategy, augmenting black-box language models with retrieval in a simple
manner, where the retrieved information is directly used to enhance the generation process.
Iterative Retrieval Strategy. For more complex scenarios, iterative retrieval strategies (Algorithm
1) are employed, where information is retrieved in multiple steps, each informed by previous results.
IRCOT [131] exemplifies this by integrating retrieval with chain-of-thought reasoning, where
the retrieval process is sequential and closely tied to reasoning steps. This method is particularly
effective in scenarios requiring multi-step problem-solving, such as research assistance or complex
queries that benefit from detailed exploration. ITER-RETGEN [121] also employs iterative retrieval,
refining the process based on generated responses, allowing for continuous improvement and
closer alignment between retrieval and generation. RQ-RAG [12] advances this approach by using
techniques like query rewriting, decomposition, and disambiguation, refining the retrieval step-by-
step to enhance the final output. PlanRAG [81] also fits within this strategy, iteratively refining
the retrieval process based on generated content and feedback, ensuring that each step is better
informed than the last.
Recursive Retrieval Strategy. Recursive retrieval (Algorithm 2) involves retrieval that can call
itself, creating a hierarchy or tree of retrievals. This method effectively handles hierarchical or
layered information by breaking down complex queries into simpler sub-queries. It is particularly
useful for hierarchical data exploration, knowledge base construction, and detailed information
, Vol. 1, No. 1, Article . Publication date: August 2018.
12
Huang et al.
Algorithm 1 Iterative Retrieval Strategy in RAG
Require: Query 𝑞, Documents 𝐷, Maximum Iterations 𝑁, Retriever 𝑅, Generator 𝐺, Pre-retrieval
Function 𝐹𝑝𝑟𝑒, Post-retrieval Function 𝐹𝑝𝑜𝑠𝑡
Ensure: Final Output 𝑦𝑓𝑖𝑛𝑎𝑙
1: Initialize 𝑖←1
// Start iteration counter
2: while 𝑖≤𝑁do
3:
Pre-retrieval Phase
4:
𝑞′ ←𝐹𝑝𝑟𝑒(𝑞)
// Indexing, Query Manipulation, Data Modification
5:
Retrieval Phase
6:
𝐷𝑖←𝑅(𝑞′, 𝐷)
// Search and initial ranking of documents
7:
Post-retrieval Phase
8:
𝐷′
𝑖←𝐹𝑝𝑜𝑠𝑡(𝑞′, 𝐷𝑖)
// Re-ranking and filtering to refine documents
9:
Generation Phase
10:
𝑦𝑖←𝐺(𝑞′, 𝐷′
𝑖)
// Generate output based on refined documents
11:
if stopping condition met based on 𝑦𝑖then
12:
BREAK
// Stop iterations if output is satisfactory
13:
end if
14:
Update 𝑞′ ←UpdateQuery(𝑞,𝑦𝑖)
// Refine query based on the generated output
15:
𝑖←𝑖+ 1
// Increment iteration counter
16: end while
17: Final Synthesis
18: 𝑦𝑓𝑖𝑛𝑎𝑙←SynthesizeResults({𝑦1,𝑦2, . . . ,𝑦𝑖})
// Merge results
19: return 𝑦𝑓𝑖𝑛𝑎𝑙
retrieval. SURGE [70] leverages this strategy through knowledge graphs, where relevant subgraphs
are extracted to enhance contextual understanding. The relational structure of knowledge graphs
facilitates navigating multiple layers of information, ensuring accurate and contextually relevant
retrieval. MEMWALKER [13] similarly adopts a recursive approach, processing long texts by
constructing a memory tree of summaries. The system navigates through this tree to retrieve
relevant information, effectively breaking down complex queries into manageable segments, which
is particularly useful for handling long-context question answering. IMRAG [150] introduces a
multi-round retrieval mechanism, where each round of retrieval is based on the model’s internal
monologues, progressively refining the search with each iteration. Selfmem [21] employs a self-
memory module, enabling the system to store and retrieve information recursively, building upon
previously retrieved knowledge in a hierarchical manner. This recursive strategy enhances the
system’s ability to manage and integrate vast amounts of information across multiple retrieval
iterations.
Conditional Retrieval Strategy. Conditional retrieval strategies (Algorithm 3) are governed by
specific conditions or rules, which may be predefined or dynamically determined during the process.
This method ensures that retrieval aligns with specific constraints or criteria, enhancing relevance
and specificity. It is particularly useful for compliance checking, rule-based recommendation
systems, and context-sensitive information retrieval. PRCA [151] is a prime example, where retrieval
strategies are adapted based on reward-driven adjustments, refining the context used by large
language models to enhance precision and relevance. RARG [159] similarly emphasizes retrieval
based on specific evidence conditions, ensuring that the retrieval process aligns with predefined
requirements, which is critical for generating factual and polite responses. CRAG [149] adds another
, Vol. 1, No. 1, Article . Publication date: August 2018.
The Survey of Retrieval-Augmented Text Generation in Large Language Models
13
Algorithm 2 Recursive Retrieval Strategy in RAG
Require: Initial Query𝑞, Documents 𝐷, Maximum Depth 𝐿, Retriever 𝑅, Generator𝐺, Pre-retrieval
Function 𝐹𝑝𝑟𝑒, Post-retrieval Function 𝐹𝑝𝑜𝑠𝑡, Sub-query Generation Function 𝐹𝑠𝑢𝑏𝑞, Hierarchical
Layer Building Function 𝐹𝑏𝑢𝑖𝑙𝑑, Hierarchical Information Operating Function 𝐹ℎ𝑖𝑒𝑟
Ensure: Final Output 𝑦𝑓𝑖𝑛𝑎𝑙
1: Build Hierarchical Layers (Pre-retrieval)
2: 𝐻𝑖𝑒𝑟𝑎𝑟𝑐ℎ𝑦←𝐹𝑏𝑢𝑖𝑙𝑑(𝑞)
// Build hierarchical layers based on the initial query
3: Initialize 𝑙←0
// Start depth counter from 0
4: Initialize 𝑆𝑢𝑏_𝑞𝑢𝑒𝑟𝑖𝑒𝑠←[𝑞]
// Initialize list of sub-queries
5: while 𝑙≤𝐿do
6:
for each 𝑞′
𝑙∈𝑆𝑢𝑏_𝑞𝑢𝑒𝑟𝑖𝑒𝑠do
7:
Pre-retrieval Phase
8:
𝑞′
𝑙←𝐹ℎ𝑖𝑒𝑟(𝐻𝑖𝑒𝑟𝑎𝑟𝑐ℎ𝑦,𝑞′
𝑙)
// Adjust query based on hierarchical layers
9:
𝑞′
𝑙←𝐹𝑝𝑟𝑒(𝑞′
𝑙)
// Query Manipulation, Indexing, and Data Modification
10:
Retrieval Phase
11:
𝐷𝑙←𝑅(𝑞′
𝑙, 𝐷)
// Retrieve documents for the current sub-query
12:
Post-retrieval Phase
13:
𝐷′
𝑙←𝐹𝑝𝑜𝑠𝑡(𝑞′
𝑙, 𝐷𝑙)
// Re-ranking and filtering to refine documents
14:
Generation Phase
15:
𝑦𝑙←𝐺(𝑞′
𝑙, 𝐷′
𝑙)
// Generate output based on refined documents
16:
Sub-query Generation (if needed)
17:
if additional refinement needed based on 𝑦𝑙then
18:
𝑆𝑢𝑏_𝑞𝑢𝑒𝑟𝑖𝑒𝑠←𝐹𝑠𝑢𝑏𝑞(𝑦𝑙)
// Generate new sub-queries based on current output
19:
end if
20:
end for
21:
𝑙←𝑙+ 1
// Increment depth counter
22: end while
23: Final Synthesis
24: 𝑦𝑓𝑖𝑛𝑎𝑙←SynthesizeResults({𝑦0,𝑦1, . . . ,𝑦𝑙})
// Merge results
25: return 𝑦𝑓𝑖𝑛𝑎𝑙
layer to this approach by incorporating a retrieval evaluator that assesses the quality of retrieved
documents and triggers different actions based on confidence thresholds, ensuring that only the
most relevant and accurate information is used in the generation process.
Adaptive Retrieval Strategy. Adaptive retrieval (Algorithm 4) dynamically adjusts the retrieval
strategy based on the context and nature of the query or the data retrieved so far. This highly
flexible method tailors retrieval approaches on-the-fly to optimize for relevance and precision,
making it ideal for personalized search engines, adaptive learning systems, and real-time decision
support. AAR [157] exemplifies adaptive retrieval by adjusting its strategy based on the prefer-
ences of LLMs, learning from a small source model and generalizing to unseen tasks. FLARE [65]
takes a similar adaptive approach but focuses on dynamically fetching additional information
when model confidence is low, thereby improving the relevance of generated responses. SelfRAG
[4] goes further by incorporating self-reflective processes, where the retrieval strategy evolves
based on critiques of the generated content. CoK [86], on the other hand, implements a dynamic
mechanism that adjusts retrieval strategies based on the evolving needs of the task. The retrieval
process in CoK is not static but adapts according to the specific scenario and the nature of the
, Vol. 1, No. 1, Article . Publication date: August 2018.
14
Huang et al.
Algorithm 3 Conditional Retrieval Strategy in RAG
Require: Query 𝑞, Documents 𝐷, Maximum Iterations 𝑁, Retriever 𝑅, Generator 𝐺, Pre-retrieval
Function 𝐹𝑝𝑟𝑒, Post-retrieval Function 𝐹𝑝𝑜𝑠𝑡, Condition Evaluation Function 𝐹𝑐𝑜𝑛𝑑
Ensure: Final Output 𝑦𝑓𝑖𝑛𝑎𝑙
1: Initialize 𝑖←1
// Start iteration counter
2: 𝑞′ ←𝑞
// Initialize query
3: while 𝑖≤𝑁do
4:
Pre-retrieval Phase
5:
𝑞′ ←𝐹𝑝𝑟𝑒(𝑞′)
// Perform query manipulation and data modification
6:
Retrieval Phase
7:
𝐷𝑖←𝑅(𝑞′, 𝐷)
// Retrieve documents based on the current query
8:
Post-retrieval Phase
9:
𝐷′
𝑖←𝐹𝑝𝑜𝑠𝑡(𝑞′, 𝐷𝑖)
// Re-rank and filter documents based on conditions
10:
Generation Phase
11:
𝑦𝑖←𝐺(𝑞′, 𝐷′
𝑖)
// Generate output using the refined documents
12:
Conditional Branching
13:
if 𝐹𝑐𝑜𝑛𝑑(𝑦𝑖, 𝐷′
𝑖) is Condition A then
14:
Apply Strategy A
// e.g., refine the query based on feedback
15:
else if 𝐹𝑐𝑜𝑛𝑑(𝑦𝑖, 𝐷′
𝑖) is Condition B then
16:
Apply Strategy B
// e.g., expand the scope or adjust parameters
17:
else if 𝐹𝑐𝑜𝑛𝑑(𝑦𝑖, 𝐷′
𝑖) is Condition C then
18:
Apply Strategy C
// e.g., modify retrieval strategy or output processing
19:
else
20:
Continue without changes
// If no conditions are met, proceed without adjustments
21:
end if
22:
Check Termination Condition
23:
if 𝐹𝑐𝑜𝑛𝑑(𝑦𝑖, 𝐷′
𝑖) meets stopping criteria then
24:
BREAK
// Exit the loop if the stopping condition is met
25:
end if
26:
𝑖←𝑖+ 1
// Increment iteration counter
27: end while
28: Final Synthesis
29: 𝑦𝑓𝑖𝑛𝑎𝑙←SynthesizeResults({𝑦1,𝑦2, . . . ,𝑦𝑖})
// Merge results
30: return 𝑦𝑓𝑖𝑛𝑎𝑙
information being accessed, making it highly effective for context-sensitive applications. DRAGIN
[124] discusses a real-time dynamic retrieval mechanism that adapts to the evolving needs of the
language model, ensuring that the retrieval strategy remains responsive and aligned with the im-
mediate task requirements, thus optimizing the relevance and precision of the retrieved information.
In summary, the choice of retrieval strategy within RAG depends on the specific requirements of
the application at hand. While basic retrieval strategies offer simplicity and efficiency, iterative
retrieval is well-suited for tasks requiring detailed exploration and refinement. Recursive retrieval
excels in managing hierarchical information, while adaptive retrieval provides flexibility in dynamic
environments. Conditional retrieval ensures strict adherence to predefined criteria, making it
indispensable in applications where compliance and specific constraints are critical. By carefully
, Vol. 1, No. 1, Article . Publication date: August 2018.
The Survey of Retrieval-Augmented Text Generation in Large Language Models
15
Algorithm 4 Adaptive Retrieval Strategy in RAG
Require: Query 𝑞, Documents 𝐷, Maximum Iterations 𝑁, Retriever 𝑅, Generator 𝐺, Pre-retrieval
Function 𝐹𝑝𝑟𝑒, Post-retrieval Function 𝐹𝑝𝑜𝑠𝑡, Adaptive Adjustment Function 𝐹𝑎𝑑𝑎𝑝𝑡, Feedback
Function 𝐹𝑓𝑒𝑒𝑑𝑏𝑎𝑐𝑘
Ensure: Final Output 𝑦𝑓𝑖𝑛𝑎𝑙
1: Initialize 𝑖←1
// Start iteration counter
2: 𝑞′,𝐶𝑜𝑛𝑡𝑒𝑥𝑡←𝑞, ∅
// Initialize query and context
3: while 𝑖≤𝑁do
4:
Pre-retrieval Phase
5:
𝑞′,𝐶𝑜𝑛𝑡𝑒𝑥𝑡←𝐹𝑝𝑟𝑒(𝑞′,𝐶𝑜𝑛𝑡𝑒𝑥𝑡)
// Query Manipulation, Indexing, Data Modification, and
Context Setup
6:
Dynamic Retrieval Phase
7:
𝐷𝑖←𝑅(𝑞′, 𝐷,𝐶𝑜𝑛𝑡𝑒𝑥𝑡)
// Retrieve documents based on the current query and context
8:
Adaptive Post-retrieval Phase
9:
𝐷′
𝑖←𝐹𝑝𝑜𝑠𝑡(𝑞′, 𝐷𝑖,𝐶𝑜𝑛𝑡𝑒𝑥𝑡)
// Re-ranking and filtering based on adaptive criteria
10:
Generation Phase
11:
𝑦𝑖←𝐺(𝑞′, 𝐷′
𝑖,𝐶𝑜𝑛𝑡𝑒𝑥𝑡)
// Generate output using the refined documents
12:
Adaptive Adjustment
13:
if 𝐹𝑓𝑒𝑒𝑑𝑏𝑎𝑐𝑘(𝑦𝑖, 𝐷′
𝑖) is negative then
14:
𝑞′,𝐶𝑜𝑛𝑡𝑒𝑥𝑡←𝐹𝑎𝑑𝑎𝑝𝑡(𝑞′,𝐶𝑜𝑛𝑡𝑒𝑥𝑡,𝑦𝑖, 𝐷′
𝑖)
// Dynamically adjust the query and context
15:
end if
16:
Feedback Integration
17:
if 𝐹𝑓𝑒𝑒𝑑𝑏𝑎𝑐𝑘(𝑦𝑖) is positive then
18:
BREAK
// Stop iterations if output is satisfactory
19:
end if
20:
𝑖←𝑖+ 1
// Increment iteration counter
21: end while
22: Final Synthesis
23: 𝑦𝑓𝑖𝑛𝑎𝑙←SynthesizeResults({𝑦1,𝑦2, . . . ,𝑦𝑖})
// Merge results
24: return 𝑦𝑓𝑖𝑛𝑎𝑙
selecting and combining these strategies, RAG systems can be tailored to effectively handle a wide
range of information retrieval scenarios, leveraging the strengths of each approach to deliver robust
and precise results.
5
Post-Retrieval
5.1
Re-Ranking
As retrieval mechanisms often return a large number of potentially relevant documents, re-ranking
methods are employed to reorder these documents, prioritizing those most likely to contribute
meaningfully to the final output. By leveraging various strategies, including unsupervised tech-
niques, supervised learning, and data augmentation, re-ranking aims to optimize the alignment
between the retrieved content and the desired response, thereby improving the overall effectiveness
of RAG systems [165].
Unsupervised Re-ranking. Unsupervised re-rankers do not rely on labeled data for training. They
use strategies such as pointwise, listwise, or pairwise methods to rank documents based on LLM
outputs without the need for supervised fine-tuning. For example, In-Context RALM [112] employs
, Vol. 1, No. 1, Article . Publication date: August 2018.
16
Huang et al.
a zero-shot approach where an off-the-shelf language model is used to re-rank the top-k documents
retrieved by a BM25 retriever. This process involves selecting the document that maximizes the
likelihood of the generated text, effectively using the LM’s semantic understanding to improve
document relevance without requiring additional supervised training. The paper also explores
training a dedicated re-ranker using self-supervised learning to further enhance the selection of
relevant documents, demonstrating that training a re-ranker with domain-specific data can be
more effective than zero-shot re-ranking.
Supervised Re-ranking. Supervised re-rankers involve fine-tuning LLMs on specific ranking
datasets. This category can be further divided into models like BERT that process query-document
pairs to compute relevance scores, models like T5 that treat ranking as a generation task and
use generated tokens to determine relevance, and models like RankLLaMA [95] that employ a
prompt-based approach, focusing on the last token’s representation for relevance calculation [165].
For instance, the re-ranker in Re2G [40] is based on a BERT model trained on labeled data (such as
MS MARCO) and fine-tuned to improve the relevance ranking of retrieved documents. FiD-Light
[47] employs a supervised approach where the model is fine-tuned on specific datasets to learn
how to re-rank passages effectively using source pointers during autoregressive text generation.
The model uses a listwise auto-regressive re-ranking mechanism, trained to identify and re-rank
relevant passages based on the output generated during the text generation process. GenRT [148]
utilizes a combination of an encoder to capture global list-level features and a sequential decoder
to reorder documents based on relevance. The model is trained to learn relevance scores through
supervised learning, guided by labeled relevance data, ensuring that the most pertinent documents
are prioritized in the final reranked list. Furthermore, ITER-RETGEN [121] proposes using a more
capable re-ranker, which has access to model generations, to distill knowledge into a dense retriever.
This knowledge distillation process optimizes the query encoder of the dense retriever, enabling it
to better capture the semantic relevance of documents relative to the task input.
Data Augmentation for Re-ranking. Data augmentation for re-rankers focuses on enhancing the
training process by generating additional training data, such as pseudo-relevance labels, using
LLMs. This data augmentation provides more varied training examples, which helps improve the
performance of re-ranking models. For example, DKS-RAC [53] introduces methods like Dense
Knowledge Similarity (DKS) and Retriever as Answer Classifier (RAC), which focus on improving
the retrieval process by incorporating rich answer encodings. These methods involve generating
additional training signals or utilizing enriched data representations to improve the retrieval and
ranking of documents. Additionally, the PROMPTAGATOR [27] framework utilizes synthetic data
generated through LLM-based query generation to enhance the training of the reranker. This data
augmentation approach allows the re-ranker to refine candidate passages more effectively, using a
cross-attention model trained on these additional examples to boost retrieval accuracy.
5.2
Filtering
Filtering and re-ranking are distinct processes in the post-retrieval stage of RAG systems. Filtering
focuses on eliminating irrelevant or low-quality documents from the retrieved set, thereby reducing
the document set size and improving efficiency and effectiveness in subsequent processing. In
contrast, re-ranking orders the remaining documents based on their relevance or utility for the task,
often prioritizing those that enhance the quality of the generated output, especially in response-
aware scenarios.
Several filtering methods have been developed to refine document sets in RAG systems, each
with unique mechanisms but sharing common goals of improving relevance and reducing compu-
tational load. Self-RAG [4] employs a self-reflection mechanism, utilizing special “reflection tokens”
, Vol. 1, No. 1, Article . Publication date: August 2018.
The Survey of Retrieval-Augmented Text Generation in Large Language Models
17
generated by the model to evaluate the relevance and quality of retrieved passages and the model’s
own generated outputs. This self-reflection ensures that only the most pertinent documents are
retained, leveraging the model’s internal capabilities without relying on external models during
inference. Similarly, BlendFilter [135] utilizes the LLM itself as the filter, assessing and removing
irrelevant or less useful documents by applying filtering separately to knowledge retrieved from
original, externally augmented, and internally augmented queries. Both Self-RAG and BlendFilter
highlight the model’s intrinsic ability to perform filtering, reducing the need for additional models
and enhancing computational efficiency.
In contrast, RECOMP [147] and CRAG [149] employ more external or structural strategies.
RECOMP focuses on selective augmentation, where summaries generated from retrieved documents
are selectively prepended to the input for the language model. If the retrieved documents are deemed
irrelevant, the compressor can generate an empty summary, effectively filtering out unnecessary
information. This method allows for a dynamic approach to filtering, where only helpful content is
retained. CRAG, on the other hand, uses a decompose-then-recompose approach, where retrieved
documents are split into finer knowledge strips. These strips are evaluated for relevance using a fine-
tuned T5 model, and only the relevant strips are recomposed to form a refined set of information
for the generation task. This granular filtering process ensures that the final document set is both
relevant and concise, tailored specifically to the generation task.
Dynamic filtering techniques are also employed in methods like FiD-TF [5] and CoK [86].
FiD-TF introduces Token Filtering during the decoding process, where less relevant tokens are
dynamically filtered out based on cross-attention scores. This approach reduces the computational
load by eliminating tokens deemed uninformative for generating the final answer, enhancing
efficiency with minimal impact on performance. CoK employs a filtering technique based on
self-consistency, identifying and processing only those questions with “uncertain” answers. This
method works by sampling various reasoning paths and answers, preserving only predictions with
high consistency. Questions that do not meet the specified consistency threshold undergo further
processing, effectively preventing the propagation of errors in the generation process.
Finally, FILCO [141] implements a comprehensive filtering approach using three distinct strate-
gies: String Inclusion (STRINC) to match exact outputs, Lexical Overlap to measure word-level
similarity, and Conditional Cross-Mutual Information (CXMI) to assess how much the context
improves output likelihood. FILCO applies these filtering strategies at the sentence level, refining
the retrieved content for better relevance. Additionally, FILCO trains a context filtering model
using these strategies, which predicts the most useful context at inference time, thereby enhancing
the accuracy and relevance of the generation model’s output.
6
Generation
6.1
Enhancing
Enhancing methods are strategies aimed at improving the quality and relevance of generated
outputs by integrating retrieved content in various ways. These methods differ in how they
combine, aggregate, or refine retrieved information, offering multiple approaches to enrich the final
output. Broadly, these techniques can be grouped into three categories: enhancing with queries,
enhancing with ensemble approaches, and enhancing with feedback loops.
Enhance with Query. This approach integrates the retrieved documents with the original query,
enabling the generator to leverage both sources in producing the final output. By combining the
query with the retrieved content, the generation process ensures that the response remains closely
aligned with the user’s intent while being enriched by relevant information. The focus here is on the
seamless fusion of the query and context, allowing the generated output to maintain both relevance
, Vol. 1, No. 1, Article . Publication date: August 2018.
18
Huang et al.
and completeness. For instance, the RETRO [9] model enhances generation by integrating retrieved
text chunks with the user’s query using a chunked cross-attention mechanism, where relevant
information from the retrieved neighbors is directly injected into the generation process. This
method involves first retrieving similar document chunks based on the query and then using a cross-
attention module to align and combine these chunks with the input sequence during generation.
In-Context RALM [112] takes a comparable approach, directly prepending the retrieved documents
to the input query. In this way, the language model can generate responses conditioned on both
the query and the retrieved content without requiring changes to the model’s architecture. Both
examples illustrate a straightforward yet effective method: concatenating the query and retrieved
documents into a single input sequence that the LLMs process together, yielding outputs that are
contextually enhanced.
Enhance with Ensemble. When multiple sources are synthesized, the generation process can
achieve a more coherent and well-rounded response. Rather than relying solely on a single source,
this approach aggregates information from various documents, allowing the generator to reconcile
conflicting details, blend diverse perspectives, and select the most reliable or comprehensive output.
The ensemble process can manifest in different ways: it may involve combining insights from
several sources into a unified narrative, or generating multiple candidate outputs and choosing
the best one based on criteria like consistency, relevance, or factual accuracy. An instance of
this strategy is seen in FiD [58], which encodes multiple retrieved passages independently before
fusing them in the decoder to create a coherent answer. By treating each passage separately during
encoding and then merging them during decoding, the model effectively combines evidence from
multiple sources. Meanwhile, in REPLUG [122], an ensemble approach is adopted where each
retrieved document is independently prepended to the query and processed separately. The outputs
are then aggregated, with relevance scores guiding the weighting of each document’s contribution.
Through this process, the model capitalizes on diverse information across several sources, leading
to improvements in answer accuracy, coverage, and scalability as more data becomes available.
Enhance with Feedback. In contrast to approaches that process retrieved information in a single
pass, this method introduces iterative refinement into the generation process by incorporating
feedback loops. Initially, the generator produces a draft response, which is then evaluated and
adjusted based on feedback mechanisms, such as self-reflection or predefined criteria focused on
factual accuracy and fluency. This iterative approach aims to incrementally improve the output
by identifying and correcting errors or fine-tuning content to better align with quality standards,
ultimately producing a polished and reliable response. PRCA [151] offers an example by positioning
itself between the retriever and generator, distilling retrieved information based on feedback from
the generator. This distilled information serves as a reward model to guide context optimization,
leveraging reinforcement learning and metrics like ROUGE-L scores to iteratively refine which
details should be emphasized or downplayed. DSP [73], on the other hand, refines both queries
and retrieved passages through a multi-hop retrieval process that incorporates programmatically
bootstrapped feedback. Here, the language model generates intermediate queries, retrieves relevant
passages, and updates the context in subsequent steps—each stage building on the last to refine the
final output. Feedback-driven enhancements are also evident in models like Selfmem [21], which
focus on generating self-memory. The model first produces an unbounded pool of outputs and
then selects the most relevant one as memory for the next generation, guided by metrics like BLEU
or ROUGE. Finally, RECITE [122] integrates feedback by generating multiple recitations from
the model’s internal knowledge and using self-consistency techniques to aggregate the outputs.
By introducing diversity in the recitations and leveraging passage hints during generation, this
approach selects the best content through majority voting. Together, these methods demonstrate
, Vol. 1, No. 1, Article . Publication date: August 2018.
The Survey of Retrieval-Augmented Text Generation in Large Language Models
19
how feedback loops and iterative refinements can lead to outputs that are not only more accurate
but also increasingly coherent and contextually grounded as they evolve.
6.2
Customization
Customization focuses on tailoring content to the user’s personality and needs. It involves adjusting
the output either to align with specific knowledge retrieved during earlier stages (content alignment)
or to adapt the generated response to meet the user’s preferences, context, or audience needs
(contextual adaptation).
In LAPDOG [52], customization is achieved primarily through content alignment by integrating
persona profiles with external stories to enrich the context used for generation. The story retriever
identifies relevant narratives based on the persona, expanding the limited profiles with additional
information. The generator then combines this enriched knowledge with the dialogue history,
ensuring that responses align closely with the persona’s traits and background. This approach
allows for a nuanced understanding of the user’s personality, making the output more engaging
and contextually appropriate.
On the other hand, PersonaRAG [160] emphasizes real-time adaptation by customizing generated
content based on dynamic user profiles, session behavior, and ongoing feedback. A multi-agent
system continuously analyzes user interactions to refine responses, ensuring alignment with
the user’s preferences and context. By integrating personalized insights at each step, the system
can adjust its output to suit specific informational needs and situational contexts. This level of
responsiveness allows the system to evolve in line with the user’s changing requirements, creating
more relevant and targeted responses.
ERAGent [123] also focuses on customization but through the use of a Personalized LLM Reader,
which adapts responses using user-specific profiles. This module integrates rewritten questions,
filtered knowledge, and user preferences to tailor responses according to both content relevance
and user needs. For instance, it takes into account preferences like environmental consciousness
or dietary restrictions, ensuring that the generated content is not only aligned with retrieved
knowledge but also personalized to the user’s particular values and requirements. This deep level
of customization ensures that the output is both relevant and personally meaningful, enhancing
user engagement.
ROPG [118] proposes a dynamic pre- and post-generation retriever selection model, enhancing
personalization by aligning the retrieval process with both the input context and the user’s pref-
erences. The pre-generation model determines which retrieval strategy—such as recency-based,
keyword matching, or semantic retrieval—is most appropriate before generation begins. By tailoring
the retrieval process in this way, the model ensures that the documents retrieved from the user
profile closely match the current input, thereby aligning the content with relevant user-specific
knowledge. Following this, the post-generation model evaluates the outputs generated by different
retrieval strategies and selects the most personalized result. This selection is guided by feedback
from the generated content, which is then used to adjust future retrievals. By combining content
alignment (through pre-generation retrieval) with contextual adaptation (through post-generation
evaluation), this approach offers a comprehensive solution for customization within RAG.
7
Evaluation in RAG
To assess how effectively language models can generate more accurate, relevant, and robust
responses by leveraging external knowledge, the evaluation of RAG systems has emerged as a
crucial research focus. Given the rising popularity of dialogue-based interactions, much recent
work has concentrated on evaluating RAG models’ performance on such downstream tasks using
established metrics like Exact Match (EM) and F1 scores. These metrics have been applied across a
, Vol. 1, No. 1, Article . Publication date: August 2018.
20
Huang et al.
Evaluation Framework
Aspects
Methods
Metrics
Datasets
RAGAS [33]
Quality of RAG Systems
Context Relevance
Extracted Sentences / Total Sentences
WikiEval7
Answer Relevance
Average Cosine Similarity
Faithfulness
Supported Statements / Total Statements
ARES [117]
Improving RAGAS
Context Relevance
Confidence Intervals
KILT [109]
SuperGLUE [134]
Answer Relevance
Answer Faithfulness
RECALL [91]
Counterfactual Robustness
Response Quality
Accuracy (QA)
BLEU, ROUGE-L (Generation)
EventKG [41]
UJ [50]
Robustness
Misleading Rate (QA)
Mistake Reappearance Rate (Generation)
RGB [14]
Impact of RAG on LLMs
Noise Robustness
Accuracy
Synthetic
Negative Rejection
Rejection Rate
Information Integration
Accuracy
Counterfactual Robustness
Error Detection Rate
Error Correction Rate
MIRAGE [144]
RAG in Medical QA
Zero-Shot Learning
Accuracy
MMLU-Med [45]
MedQA-US [66]
MedMCQA [105]
PubMedQA [67]
BioASQ-Y/N [132]
Multi-Choice Evaluation
Retrieval-Augmented Generation
Question-Only Retrieval
eRAG [119]
Retrieval Quality in RAG
Downstream Task
Accuracy, ROUGE
KILT
Set-based
Precision, Recall, Hit Rate
Ranking
MAP, MRR, NDCG
BERGEN [114]
Standardizing RAG Experiments
Surface-Based
EM, F1, Precision, Recall
QA Datasets [69, 76]
Semantic
BEM [11], LLMeval [114]
Table 1. The Comparison of Different RAG Evaluation Frameworks.
wide array of datasets, including TriviaQA [69], HotpotQA [153], FEVER [129], Natural Questions
(NQ) [76], Wizard of Wikipedia (WoW) [30], and T-REX [32], which are often used to benchmark
the effectiveness of retrieval and generation components in knowledge-intensive tasks.
While downstream task evaluations provide valuable insights, they fail to address the multifaceted
challenges that arise as RAG systems continue to evolve. To fill this gap, recent research has
proposed various frameworks and benchmarks that aim to evaluate these systems from multiple
perspectives, considering not only the quality of the generated text but also the relevance of retrieved
documents and the system’s resilience to misinformation, as shown in Table 1. These evaluations
include metrics that assess noise robustness, negative prompting, information integration, and
counterfactual robustness, all of which reflect the complex challenges RAG systems face in real-
world applications. The ongoing development of comprehensive evaluation frameworks and metrics
is essential for advancing the field, broadening the applicability of RAG systems, and ensuring that
they meet the demands of an increasingly dynamic and complex information landscape [154].
7.1
Retrieval-based Aspect
In information retrieval, standard metrics such as Mean Average Precision (MAP), Precision, Recip-
rocal Rank, and Normalized Discounted Cumulative Gain (NDCG) [103, 110, 115] have traditionally
been used to evaluate the relevance of retrieved documents to a given query. These metrics are
essential in assessing the effectiveness of traditional information retrieval systems, where the
primary goal is to measure how well the retrieved documents match the user’s query.
When applied to RAG systems, these retrieval-based metrics extend their focus to consider
how the retrieved information contributes to the quality of the generated output. In this context,
Accuracy becomes a crucial metric, assessing how precisely the retrieved documents provide
correct information for answering queries. Additionally, Rejection Rate [14], which measures the
system’s ability to decline answering when no relevant information is available, has emerged as a
, Vol. 1, No. 1, Article . Publication date: August 2018.
The Survey of Retrieval-Augmented Text Generation in Large Language Models
21
key indicator of responsible output generation. Similarly, Error Detection Rate [14] evaluates the
model’s capability to identify and filter out incorrect or misleading information, ensuring that the
generation process is based on trustworthy sources.
Another important consideration is Context Relevance, which assesses the alignment of retrieved
documents with the specific query, emphasizing the need for content directly relevant to the
generation task’s context. Faithfulness [33] is also critical in determining whether the generated
text accurately reflects the information found in the retrieved documents, thereby minimizing the
risk of generating misleading or incorrect content.
The eRAG framework [119] introduces a more refined approach to evaluating retrieval quality
in RAG systems by focusing on individual documents rather than the entire retrieval process. It
operates by feeding each document in the retrieval list into the LLM alongside the query and
evaluating the generated output against downstream task metrics such as Accuracy. The document-
level scores are then aggregated using ranking metrics like MAP to produce a single evaluation score.
This focus on document-level contributions offers a more precise assessment of retrieval quality
while being significantly more computationally efficient than traditional end-to-end evaluations.
Notably, eRAG demonstrates that its document-level evaluation correlates more strongly with
downstream RAG performance compared to conventional methods like human annotations or
provenance labels. This correlation underscores that the LLM, as the primary consumer of the
retrieved results, is the most reliable judge of retrieval performance [119]. Regardless of the retrieval
model or the number of retrieved documents, eRAG consistently outperforms other evaluation
approaches, indicating that directly evaluating how each document supports the LLM’s output is
the most effective way to measure retrieval quality in RAG systems.
7.2
Generation-based Aspect
The evaluation of text produced by large language models involves analyzing performance across a
range of downstream tasks using standard metrics that assess linguistic quality, coherence, accuracy,
and alignment with ground-truth data. Metrics like BLEU [107] and ROUGE-L [87] are often used
to measure fluency, similarity to human-produced text, and the overlap with reference summaries,
respectively, providing insights into how well the generated content captures key ideas and phrases.
In addition to these metrics, which focus on the quality of linguistic output, Accuracy and overlap
with ground-truth data are evaluated using EM and F1 scores, which respectively measure the
percentage of completely correct answers and offer a balanced view of precision and recall. This
ensures that relevant answers are retrieved while inaccuracies are minimized.
Beyond these standard evaluation techniques, more specialized criteria have been introduced
to assess RAG systems in specific contexts. For dialogue generation, for instance, metrics like
perplexity and entropy are employed to evaluate response diversity and naturalness. In scenarios
where misinformation is a concern, metrics like Misleading Rate and Mistake Reappearance Rate
[91] have been developed to measure a model’s ability to avoid generating incorrect or misleading
content. Other advanced metrics include Answer Relevance [33], which assesses the precision of
responses to queries, Kendall’s tau [117], used for evaluating the accuracy of system rankings, and
Micro-F1 [117], which fine-tunes accuracy evaluation in tasks involving multiple correct answers.
Prediction Accuracy further complements these by directly measuring how closely the generated
responses align with the expected answers, offering a clear measure of a system’s effectiveness in
producing accurate content.
, Vol. 1, No. 1, Article . Publication date: August 2018.
22
Huang et al.
Research
Year
Retrieval Source
Multi-hop
Training
Pre-Retrieval
Retrieval
Post-Retrieval
Generation
Internal
External
Indexing
Query Manipulation
Data Modification
Search & Ranking
Re-Ranking
Filtering
Enhancing
Customization
REALM [42]
2020
!
!
!
!
kNN-LMs [72]
2020
!
!
!
!
!
RAG [83]
2020
!
!
!
!
FiD [58]
2021
!
!
!
Webgpt [100]
2021
!
!
!
!
!
!
!
!
Re2G [40]
2022
!
!
!
!
RETRO [9]
2022
!
!
!
!
!
DSP [73]
2022
!
!
!
!
!
CoK [86]
2023
!
!
!
!
IRCOT [131]
2023
!
!
!
!
ITRG [34]
2023
!
!
!
!
!
PKG [92]
2023
!
RA-DIT [89]
2023
!
!
!
!
!
!
Self-RAG [4]
2023
!
!
!
SURGE [70]
2023
!
!
FiD-TF [5]
2023
!
!
!
PRCA [151]
2023
!
!
!
!
REPLUG [122]
2023
!
!
!
AAR [157]
2023
!
!
!
Query2doc [137]
2023
!
!
Step-Back [163]
2023
!
!
!
ITER-RETGEN [121]
2023
!
!
!
!
RECITE [125]
2023
!
!
!
!
!
PROMPTAGATOR [27]
2023
!
!
!
!
!
UPRISE [20]
2023
!
!
!
!
!
!
GENREAD [156]
2023
!
!
!
LAPDOG [52]
2023
!
!
!
!
!
!
!
KnowledGPT [140]
2023
!
!
!
!
Selfmem [21]
2023
!
!
!
!
!
MEMWALKER [13]
2023
!
!
!
!
RECOMP [147]
2023
!
!
!
Rewrite-Retrieve-Read [94]
2023
!
!
!
Atlas [94]
2023
!
!
!
!
!
!
DKS-RAC [53]
2023
!
!
!
!
!
In-Context RALM [112]
2023
!
!
Fid-light [47]
2023
!
!
!
FLARE [65]
2023
!
!
!
Chameleon [63]
2023
!
!
!
!
ERAGent [123]
2024
!
!
!
!
!
!
!
!
PipeRAG [64]
2024
!
!
!
!
!
GenRT [148]
2024
!
!
!
!
PersonaRAG [160]
2024
!
!
!
!
!
!
!
!
!
CRAG [149]
2024
!
!
!
!
!
!
!
IMRAG [150]
2024
!
!
!
!
!
!
AiSAQ [126]
2024
!
!
!
!
ROPG [118]
2024
!
!
!
!
!
RQ-RAG [12]
2024
!
!
!
!
!
PlanRAG [81]
2024
!
!
!
!
!
RARG [159]
2024
!
!
!
!
!
DRAGIN [124]
2024
!
!
!
!
!
LRUS-CoverTree [93]
2024
!
!
!
!
!
Table 2. The comprehensive summary of RAG studies. A !in the “Multi-hop” column signifies that the
research involves multiple search rounds. Similarly, a !in the “Training” column indicates that the study
included training phases. It is important to note that in this context, “Training” encompasses both initial
model training and fine-tuning processes.
8
Comparisons of RAG
8.1
The Comprehensive Summary of RAG
Table 2 presents a detailed analysis of the RAG studies discussed in this paper. The analysis shows
that the majority of these studies have utilized external data sources to enrich the content of LLMs.
A preference for multiple-hop over single-hop retrieval was noted, indicating that iterative search
rounds generally yield superior results. In other words, most methods employ dense retrieval to
secure higher quality candidate documents. Compared to modifying datasets in the pre-retrieval
stage, more studies focus on manipulating the query to improve retrieval performance. Additionally,
there is a significant emphasis on optimizing the retrieval phase, highlighting its crucial role in
the research. However, there seems to be a scarcity of studies concentrating on customization in
the generation stage, pointing to this as a potential area for future exploration. Overall, while the
goal of RAG is to enhance the response quality of LLMs, greater efforts have been directed towards
improving retrieval aspects.
, Vol. 1, No. 1, Article . Publication date: August 2018.
The Survey of Retrieval-Augmented Text Generation in Large Language Models
23
Research
Year
Retriever
Generator
REALM [42]
2020
BERT [29]
Transformers [133]
kNN-LMs [72]
2020
FAISS [68]
Transformers
RAG [83]
2020
DPR [71]
BART-Large [82]
FiD [58]
2021
BM25 [116], DPR
T5 [111]
Webgpt [100]
2021
Bing
GPT-3 [10]
Re2G [40]
2022
BM25, DPR
BART
RETRO [9]
2022
BERT
Transformer
DSP [73]
2022
ColBERTv2 [74]
GPT-3.5 (text-davinci-002)
CoK [86]
2023
LLaMA2-7B [130], ChatGPT (gpt-3.5-turbo-0613)
ChatGPT (gpt-3.5-turbo-0613)
IRCOT [131]
2023
BM25
GPT-3 (code-davinci-002), Flan-T5 [24]
ITRG [34]
2023
Atlas [94]
LLaMA-33B
PKG [92]
2023
LLaMA-7B
InstructGPT-3.5 (text-davinic-002) [104]
RA-DIT [89]
2023
DRAGON+ [88]
LLaMA
Self-RAG [4]
2023
Contriever [57]
LLaMA2 (7B and 13B) , GPT-4 [1]
SURGE [70]
2023
Graph Neural Networks (GNN) [43]
Transformers
FiD-TF [5]
2023
BM25, SBERT [115]
T5
PRCA [151]
2023
BM25, DPR, Contriver, SimCSE [37], SBERT
T5, Phoenix-7B [19], Vicuna-7B [22], ChatGLM [31], GPT-3.5
REPLUG [122]
2023
Contriever
GPT-3
AAR [157]
2023
ANCE [146], Contriever
Flan-T5, InstructGPT
Query2doc [137]
2023
BM25, DPR
GPT-3 (text-davinci-003)
Step-Back [163]
2023
PaLM-2L [23]
PaLM-2L, GPT-4
ITER-RETGEN [121]
2023
Contriever
InstructGPT (text-davinci-003), LLaMA2
RECITE [125]
2023
PaLM, UL2 [127], OPT [161], Codex [16]
PROMPTAGATOR [27]
2023
T5
FLAN
UPRISE [20]
2023
GPT-Neo-2.7B [8]
BLOOM-7.1B [142], OPT-66B, GPT-3-175B
GENREAD [156]
2023
InstructGPT
LAPDOG [52]
2023
Contriever
T5
KnowledGPT [140]
2023
GPT-4
Selfmem [21]
2023
BM25
XGLM [90], XLM-Rbase [25]
MEMWALKER [13]
2023
LLaMA2
LLaMA2
RECOMP [147]
2023
BM25
T5-Large
Rewrite-Retrieve-Read [94]
2023
Bing
T5-Large, ChatGPT(gpt-3.5-turbo), Vicuna-13B
Atlas [94]
2023
Contriever
T5
DKS-RAC [53]
2023
DPR
BART
In-Context RALM [112]
2023
BM25, BERT-base, Contriever, Spider [113]
GPT-2, GPT-Neo, GPT-J [35], OPT, and LLaMA
Fid-light [47]
2023
GTR-Base [101]
T5
FLARE [65]
2023
BM25, Bing
GPT-3.5 (text-davinci-003)
Chameleon [63]
2023
ChamVS [63]
ChamLM [63]
ERAGent [123]
2024
Bing
GPT-3.5, Falcon 1B [108]
PipeRAG [64]
2024
SBERT
RETRO [9]
GenRT [148]
2024
LambdaMart [2]
PersonaRAG [160]
2024
BM25
GPT-3.5
CRAG [149]
2024
Contriever
LLaMA2
IMRAG [150]
2024
DPR
Vicuna-7B
AiSAQ [126]
2024
DiskANN [106]
ROPG [118]
2024
BM25, Contriever
FlanT5-XXL
RQ-RAG [12]
2024
DuckDuckGo8
LLaMA2-7B
PlanRAG [81]
2024
GPT-4
GPT-4
RARG [159]
2024
BM25, E5 [136]
LLaMA2-7B
DRAGIN [124]
2024
BM25, SGPT [99]
LLaMA2 (7B and 13B), Vicuna-13B
LRUS-CoverTree [93]
2024
k-MIPS
Table 3. The summary of Retrievers and Generators. The retrieval models and pre-trained language models
explicitly mentioned in these studies have been recorded.
8.2
Retriever and Generator
In RAG, the retriever and generator are central components, each playing a distinct role in the
system’s overall performance. Table 3 summarizes the retrievers and generators used across the
studies discussed in this paper. The table reveals that while a wide range of advanced language
models are employed as generators, many systems still rely on traditional retrievers like BM25,
valued for their efficiency. This highlights the continued importance of optimizing retrieval methods
while balancing computational demands. Interestingly, despite the availability of powerful models
such as LLaMA2, GPT-3.5, and GPT-4, these are not widely adopted as generators. Instead, models
like T5 remain prevalent, while more foundational retrieval approaches, such as those based on
BERT, see limited use. The relative scarcity of IR-focused LLMs in retrievers suggests a promising
avenue for future research and development in this domain.
, Vol. 1, No. 1, Article . Publication date: August 2018.
24
Huang et al.
Corpus
Retriever
Mirage Benchmark Dataset
Average
MMLU-Med
MedQA-US
MedMCQA
PubMedQA*
BioASQ-Y/N
None
None
72.91 ± 1.35
65.04 ± 1.34
55.25 ± 0.77
36.00 ± 2.15
74.27 ± 1.76
60.69
PubMed
(23.9M)
BM25
72.27 ± 1.36
63.71 ± 1.35
55.49 ± 0.77
66.20 ± 2.12
88.51 ± 1.28
69.23
Contriever
71.72 ± 1.36
63.94 ± 1.35
54.29 ± 0.77
65.60 ± 2.12
85.44 ± 1.42
68.20
SPECTER
73.19 ± 1.34
65.20 ± 1.34
53.12 ± 0.77
54.80 ± 2.23
75.73 ± 1.72
64.41
MedCPT
73.09 ± 1.34
66.69 ± 1.32
54.94 ± 0.77
66.40 ± 2.11
85.76 ± 1.41
69.38
RRF-2
75.57 ± 1.30
64.34 ± 1.34
55.34 ± 0.77
69.00 ± 2.07
87.06 ± 1.35
70.26
RRF-4
73.37 ± 1.34
64.73 ± 1.34
54.75 ± 0.77
67.20 ± 2.10
88.51 ± 1.28
69.71
Wikipedia
(29.9M)
BM25
73.37 ± 1.34
63.47 ± 1.35
54.10 ± 0.77
26.40 ± 1.97
71.36 ± 1.82
57.74
Contriever
74.10 ± 1.33
65.99 ± 1.33
54.03 ± 0.77
26.40 ± 1.97
69.90 ± 1.85
58.08
SPECTER
72.18 ± 1.36
63.63 ± 1.35
52.71 ± 0.77
22.20 ± 1.86
66.83 ± 1.89
55.51
MedCPT
71.99 ± 1.36
65.12 ± 1.34
55.15 ± 0.77
29.00 ± 2.03
73.46 ± 1.78
58.95
RRF-2
74.20 ± 1.33
64.57 ± 1.34
54.72 ± 0.77
31.00 ± 2.07
76.21 ± 1.71
60.14
RRF-4
73.19 ± 1.34
64.96 ± 1.34
54.53 ± 0.77
31.00 ± 2.07
72.01 ± 1.81
59.14
Table 4. Part results of Accuracy (%) of GPT-3.5 across different corpora and retrievers on Mirage. Red and
green highlight declines and improvements compared to CoT (first row), with shading intensity reflecting
the degree of change. Data sourced from Mirage [144].
Impact of the Retriever. The results shown in Table 4 highlight the accuracy of GPT-3.5 across
different corpora and retrievers on the Mirage benchmark [144]. These findings underscore how
retriever performance closely depends on the alignment between training data and the target
corpus. For example, in the MEDRAG system, MedCPT—trained specifically on PubMed user
logs—significantly improves retrieval performance when accessing the PubMed corpus. This illus-
trates the benefits of using domain-specific retrievers tailored to specialized datasets. In contrast,
general-purpose retrievers like Contriever, which incorporate Wikipedia data during training, excel
in retrieving information from Wikipedia, especially for tasks like MMLU-Med and MedQA-US. On
the other hand, SPECTER, which focuses more on regularizing pairwise article distances than opti-
mizing query-to-article relevance, underperforms on the MedCorp corpus. The study also explores
combining multiple retrievers using Reciprocal Rank Fusion (RRF). However, results show that
adding more retrievers does not always lead to better outcomes; for instance, excluding SPECTER in
RRF-2 on Wikipedia yields better results than RRF-4, indicating that simply increasing the number
of retrievers is not beneficial unless their strengths align with the retrieval task.
Figure 5a illustrates how eRAG investigates the correlation between LLM performance and
retrieval effectiveness on the NQ dataset using three retrievers with different characteristics: BM25
(lexical sparse), RetroMAE (dense) [143], and SPLADEv3 (learned sparse) [80]. The initial retrievals
are re-ranked using a DeBERTa-v3 [79] cross-encoder. The analysis demonstrates that as retrieval
quality improves, LLM performance increases significantly across various models. Notably, re-
ranking with SPLADEv3 and DeBERTa-v3 consistently achieves the best results across datasets and
metrics. This underscores the critical role that high-quality retrieval plays in determining overall
RAG system effectiveness, suggesting that IR-focused LLMs could be a valuable asset in enhancing
generation performance.
Impact of the Generator. The BERGEN study [114] compares the performance of LLMs with
gold passages (Oracle) against closed-book settings without retrieval, as shown in Figure 5b.
Surprisingly, the experiments do not reveal a straightforward relationship between model size and
the performance gains from retrieval. For instance, smaller models like LLaMA2-7B benefit more
, Vol. 1, No. 1, Article . Publication date: August 2018.
The Survey of Retrieval-Augmented Text Generation in Large Language Models
25
0.6
0.7
0.8
0.9
1.0
Retrieval Performance (Recall@5)
0.60
0.65
0.70
0.75
0.80
0.85
RAG Performance (LLMeval)
SPLADE-v3+RR
BM25
SPLADE-v3
BM25+RR
Oracle
RetroMAE
RetroMAE+RR
(a) Impact of retrieval performance on RAG per-
formance for SOLAR-10.7B [75] on NQ with dif-
ferent ranking systems. RR means with additional
re-ranking using DeBERTa-v3.
TinyLlama-1.1B
SOLAR-10.7B
Mixtral-8x7B
Llama3-8B
Llama2-7B
Llama2-70B
LLM
0.0
0.2
0.4
0.6
0.8
1.0
RAG Performance (LLMeval)
+0.09
+0.03
+0.06
+0.15
+0.17
+0.15
Retriever
Closed Book
Oracle
(b) Performance gains w/ and w/o oracle retrieval
for LLMs with different sizes. Comparing closed
book vs oracle passages averaged over all QA
datasets in KILT.
(c) The correlation between eRAG and the downstream performance of different LLM sizes. In this experiment,
T5small (60M parameters) and T5-base (220M parameters) with FiD are used. The documents are retrieved
using BM25.
Fig. 5. Retriever and generator experiment results sourced from eRAG [119] and BERGEN [114].
from retrieval than larger models like LLaMA2-70B. In fact, LLaMA2-7B with retrieval outperforms
LLaMA2-70B in a closed-book setting, suggesting that retrieval augmentation can make smaller
models more competitive. Similarly, results from the eRAG experiments in Figure 5c indicate that
varying LLM sizes (e.g., T5-small vs. T5-base) does not significantly affect the correlation between
eRAG and downstream performance. These findings highlight that retrieval quality has a more
substantial impact on RAG performance than the choice of generator, reinforcing the notion that
investing in better retrieval strategies often yields more benefits than relying solely on larger LLMs.
9
Challenges and Future Directions
The evolving landscape of RAG systems faces significant challenges that impact the quality of
generated outputs, system efficiency, and the integration of multimodal data. As these systems
become more prevalent across a range of applications, addressing these challenges is essential for
improving their effectiveness and scalability.
, Vol. 1, No. 1, Article . Publication date: August 2018.
26
Huang et al.
9.1
Retrieval Quality
The quality of retrieval is fundamental to any effective RAG system, directly influencing the rele-
vance and accuracy of the generated content [46, 119, 138, 164]. Current retrieval methods, however,
frequently struggle with issues like noise, irrelevant documents, and fragmented information, all of
which compromise the generation process.
Noise Robustness. Irrelevant or misleading documents within the retrieved set can introduce
noise, leading to hallucinations or unreliable answers. This challenge highlights the need for more
sophisticated filtering and context-aware retrieval methods that can better differentiate relevant
from irrelevant content. However, Cuconasu et al. [26] present an interesting perspective by
showing that, under certain conditions, the inclusion of irrelevant documents can enhance overall
accuracy. This finding challenges conventional retrieval strategies and suggests the potential for
developing specialized approaches that strategically integrate noise within the retrieval process.
Negative Rejection. When retrieval fails to return relevant results, models often attempt to
generate responses regardless, increasing the risk of incorrect outputs. This issue is particularly
problematic when queries are poorly expressed or lack sufficient context, making it difficult for
retrieval models to surface relevant documents. Techniques like generating a pseudo-document
that captures the query’s essence, as demonstrated by HyDE [36], can help bridge this gap. By
allowing retrieval systems to find more relevant documents even from suboptimal queries, HyDE
improves retrieval accuracy, albeit with a trade-off in computational cost. Future research could
focus on optimizing this process to balance improved retrieval accuracy with reduced latency.
Information Integration. Complex queries often require synthesizing information from multi-
ple documents, yet fragmented or conflicting information can result in incoherent or incomplete
answers. Pre- and post-retrieval techniques play a critical role in addressing this challenge. Enhanc-
ing retrieval granularity and incorporating techniques like entity-level retrieval and re-ranking
can improve the cohesiveness of retrieved documents. However, many post-retrieval methods,
as investigated by Zhu et al. [165], rely heavily on calling LLM APIs, which incurs significant
costs. Exploring alternatives such as knowledge distillation to lightweight models could offer more
scalable solutions, making advanced retrieval strategies more practical in online settings.
Recent research highlights the development of generative models for search as a promising
direction for improving retrieval quality. Models like GERE [15] and PARADE [84] enhance docu-
ment re-ranking and fact verification by directly generating relevant document titles or evidence
sentences. Fine-tuning pre-trained models like RankT5 [166] for ranking-specific tasks has also
demonstrated potential in boosting out-of-domain performance, which is crucial for generalizing
RAG systems across diverse contexts.
9.2
System Efficiency
System efficiency remains a significant bottleneck, especially as RAG systems scale to handle large
datasets and real-time applications. The multi-step nature of RAG workflows—including query
classification, retrieval, re-ranking, and generation—adds complexity and latency, which can hinder
overall performance.
Latency in Retrieval Processes. As document collections grow, retrieval and re-ranking processes
increasingly become sources of latency. Lightweight search methods and hybrid retrieval approaches
that combine sparse and dense techniques offer potential solutions by balancing speed and accuracy.
For example, indexing, a traditionally resource-intensive process, has seen innovations through
, Vol. 1, No. 1, Article . Publication date: August 2018.
The Survey of Retrieval-Augmented Text Generation in Large Language Models
27
differentiable search indices such as DSI [128] and SEAL [7]. These methods integrate retrieval
within Transformer models, enabling direct mapping of text queries to document identifiers and
thereby improving both performance and retrieval efficiency.
Computational Costs. The introduction of deep learning-based re-ranking models like monoT5
[102] and RankLLaMA [96] brings significant computational overhead, particularly in scenarios
requiring iterative reasoning. Future research could focus on optimizing these models or developing
retrieval pruning techniques that reduce the number of documents passed to the generation phase
without sacrificing performance [145].
Modular Workflow Optimization. The complexity of RAG systems often stems from interde-
pendencies between components like chunking strategies, embedding models, and re-ranking
algorithms. Modular designs that enable independent optimization of each step while accounting
for cross-component interactions are key to enhancing system throughput [39]. Advanced chunking
methods and hybrid search strategies could offer trade-offs that maximize both retrieval precision
and speed. An example is the Hybrid with HyDE [139] approach, which integrates both sparse and
dense retrieval to capture relevant documents from both lexical and semantic perspectives.
9.3
Multimodal RAG
The expansion of RAG systems to support multimodal data—encompassing text, images, and
audio—presents new challenges. Integrating diverse modalities requires not only effective retrieval
but also seamless alignment and generation across different data types.
Cross-Modal Alignment. Aligning multimodal documents with text-based queries remains a
core challenge. The complexity of mapping diverse data types into a unified retrieval framework
necessitates improved cross-modal retrieval strategies capable of simultaneously handling text,
image, and potentially video or audio data.
Coherent Multimodal Generation. Generating responses that meaningfully integrate information
from multiple modalities is another difficult task. Advanced generation models capable of reasoning
across different modalities are required to produce outputs that are both contextually relevant and
visually coherent.
Recent advancements in multimodal RAG, such as MuRAG [17], REVEAL [49], and Re-ViLM
[152], have shown potential in incorporating multimodal retrieval and generation into real-world
applications like visual question answering [18], image captioning [120], and text-to-audio gen-
eration [158]. Moving forward, research will likely focus on refining these techniques, especially
in scaling multimodal retrieval to handle larger datasets and more complex queries. Extending
retrieval capabilities to include more diverse media types, such as video and speech, also represents
a promising direction for the continued evolution of RAG systems.
10
Conclusions
In this paper, we have presented a comprehensive framework for understanding the RAG domain,
highlighting its significance in enhancing the capabilities of LLMs. Through a structured overview
of RAG, categorizing various methods, and an in-depth analysis of its core technologies and
evaluation methods, this study illuminates the path for future research. It identifies crucial areas
for improvement and outlines potential directions for advancing RAG applications, especially in
textual contexts. This survey aims to elucidate the core concepts of the RAG field from a retrieval
perspective, and it is intended to facilitate further exploration and innovation in the accurate
retrieval and generation of information.
, Vol. 1, No. 1, Article . Publication date: August 2018.
28
Huang et al.
Acknowledgments
This research is supported by the Natural Sciences and Engineering Research Council (NSERC) of
Canada.
References
[1] OpenAI Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, and etc. 2023. GPT-4 Technical Report. arXiv
(2023).
[2] Qingyao Ai, Keping Bi, Jiafeng Guo, and W. Bruce Croft. 2018. Learning a Deep Listwise Context Model for Ranking
Refinement. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval.
ACM.
[3] Sunil Arya, David M. Mount, Nathan S. Netanyahu, Ruth Silverman, and Angela Y. Wu. 1998. An Optimal Algorithm
for Approximate Nearest Neighbor Searching Fixed Dimensions. J. ACM 45, 6 (1998), 891–923. https://doi.org/10.
1145/293347.293348
[4] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2024. Self-RAG: Learning to Retrieve,
Generate, and Critique through Self-Reflection. In The Twelfth International Conference on Learning Representations,
Vol. abs/2310.11511.
[5] Moshe Berchansky, Peter Izsak, Avi Caciularu, Ido Dagan, and Moshe Wasserblat. 2023. Optimizing Retrieval-
augmented Reader Models via Token Elimination. In Proceedings of the Conference on Empirical Methods in Natural
Language Processing. Association for Computational Linguistics, 1506–1524.
[6] Michele Bevilacqua, Giuseppe Ottaviano, Patrick S. H. Lewis, Scott Yih, Sebastian Riedel, and Fabio Petroni. 2022.
Autoregressive Search Engines: Generating Substrings as Document Identifiers. In Advances in Neural Information
Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans,
LA, USA, November 28 - December 9, 2022, Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho,
and A. Oh (Eds.). http://papers.nips.cc/paper_files/paper/2022/hash/cd88d62a2063fdaf7ce6f9068fb15dcd-Abstract-
Conference.html
[7] Michele Bevilacqua, Giuseppe Ottaviano, Patrick S. H. Lewis, Scott Yih, Sebastian Riedel, and Fabio Petroni. 2022.
Autoregressive Search Engines: Generating Substrings as Document Identifiers. In Conference on Neural Information
Processing Systems (NeurIPS).
[8] Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor
Leahy, Kyle McDonell, Jason Phang, Michael Pieler, Usvsn Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan
Tow, Ben Wang, and Samuel Weinbach. 2022. GPT-NeoX-20B: An Open-Source Autoregressive Language Model.
In Proceedings of BigScience Episode #5 – Workshop on Challenges & Perspectives in Creating Large Language Models,
Vol. abs/2204.06745. Association for Computational Linguistics.
[9] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den
Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman
Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini,
Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre. 2022.
Improving Language Models by Retrieving from Trillions of Tokens. In International Conference on Machine Learning
(ICML). 2206–2240.
[10] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan,
Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,
Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In Conference on Neural Information
Processing Systems (NeurIPS), Vol. abs/2005.14165.
[11] Jannis Bulian, Christian Buck, Wojciech Gajewski, Benjamin Börschinger, and Tal Schuster. 2022. Tomayto, Tomahto.
Beyond Token-level Answer Equivalence for Question Answering Evaluation.. In Conference on Empirical Methods in
Natural Language Processing (EMNLP). 291–305.
[12] Chi-Min Chan, Chunpu Xu, Ruibin Yuan, Hongyin Luo, Wei Xue, Yike Guo, and Jie Fu. 2024. RQ-RAG: Learning to
Refine Queries for Retrieval Augmented Generation. arXiv abs/2404.00610 (2024).
[13] Howard Chen, Ramakanth Pasunuru, Jason Weston, and Asli Celikyilmaz. 2023. Walking Down the Memory Maze:
Beyond Context Limit through Interactive Reading. arXiv abs/2310.05029 (2023).
[14] Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2024. Benchmarking Large Language Models in Retrieval-
Augmented Generation. Proceedings of the AAAI Conference on Artificial Intelligence 38, 16 (2024), 17754–17762.
[15] Jiangui Chen, Ruqing Zhang, Jiafeng Guo, Yixing Fan, and Xueqi Cheng. 2022. GERE: Generative Evidence Retrieval
for Fact Verification. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in
, Vol. 1, No. 1, Article . Publication date: August 2018.
The Survey of Retrieval-Augmented Text Generation in Large Language Models
29
Information Retrieval. ACM.
[16] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé de Oliveira Pinto, Jared Kaplan, Harrison
Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov,
Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power,
Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias
Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas
Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N.
Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira
Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech
Zaremba. 2021. Evaluating Large Language Models Trained on Code. arXiv abs/2107.03374 (2021).
[17] Wenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, and William Cohen. 2022. MuRAG: Multimodal Retrieval-Augmented
Generator for Open Question Answering over Images and Text. In Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP).
[18] Wenhu Chen, Hexiang Hu, Chitwan Saharia, and William W. Cohen. 2023. Re-Imagen: Retrieval-Augmented Text-to-
Image Generator. In International Conference on Learning Representations (ICLR).
[19] Zhihong Chen, Feng Jiang, Junying Chen, Tiannan Wang, Fei Yu, Guiming Chen, Hongbo Zhang, Juhao Liang, Chen
Zhang, Zhiyi Zhang, Jianquan Li, Xiang Wan, Benyou Wang, and Haizhou Li. 2023. Phoenix: Democratizing ChatGPT
across Languages. arXiv abs/2304.10453 (2023).
[20] Daixuan Cheng, Shaohan Huang, Junyu Bi, Yuefeng Zhan, Jianfeng Liu, Yujing Wang, Hao Sun, Furu Wei, Weiwei
Deng, and Qi Zhang. 2023. UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation. In Proceedings
of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics,
12318–12337.
[21] Xin Cheng, Di Luo, Xiuying Chen, Lemao Liu, Dongyan Zhao, and Rui Yan. 2023. Lift Yourself Up: Retrieval-augmented
Text Generation with Self-Memory.. In Conference on Neural Information Processing Systems (NeurIPS).
[22] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao
Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An Open-Source Chatbot Impressing GPT-4
with 90%* ChatGPT Quality. https://lmsys.org/blog/2023-03-30-vicuna/
[23] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham,
Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua
Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben
Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke,
Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson,
Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan
Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai,
Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou,
Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas
Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2023. PaLM: Scaling Language Modeling with Pathways. Journal of
Machine Learning Research (JMLR) 24 (2023), 240:1–240:113.
[24] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang,
Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen,
Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra,
Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin,
Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2022. Scaling Instruction-Finetuned Language Models. arXiv
abs/2210.11416 (2022).
[25] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán,
Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised Cross-lingual Representation
Learning at Scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association
for Computational Linguistics, 8440–8451.
[26] Florin Cuconasu, Giovanni Trappolini, Federico Siciliano, Simone Filice, Cesare Campagnano, Yoelle Maarek, Nicola
Tonellotto, and Fabrizio Silvestri. 2024. The Power of Noise: Redefining Retrieval for RAG Systems. In Annual
International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), Vol. abs/2401.14887.
[27] Zhuyun Dai, Vincent Y. Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith B. Hall, and
Ming-Wei Chang. 2023. Promptagator: Few-shot Dense Retrieval From 8 Examples. In International Conference on
Learning Representations (ICLR).
[28] Mayur Datar, Nicole Immorlica, Piotr Indyk, and Vahab S. Mirrokni. 2004. Locality-sensitive hashing scheme based
on p-stable distributions.. In International Symposium on Computational Geometry (SoCG). 253–262.
, Vol. 1, No. 1, Article . Publication date: August 2018.
30
Huang et al.
[29] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional
Transformers for Language Understanding. In Proceedings of the Conference of the North. Association for Computational
Linguistics, 4171–4186.
[30] Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. 2019. Wizard of Wikipedia:
Knowledge-Powered Conversational Agents. In International Conference on Learning Representations (ICLR).
[31] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. GLM: General
Language Model Pretraining with Autoregressive Blank Infilling. In Proceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics.
[32] Hady ElSahar, Pavlos Vougiouklis, Arslen Remaci, Christophe Gravier, Jonathon S. Hare, Frédérique Laforest, and Elena
Simperl. 2018. T-REx: A Large Scale Alignment of Natural Language with Knowledge Base Triples. In International
Conference on Language Resources and Evaluation (LREC).
[33] Shahul ES, Jithin James, Luis Espinosa Anke, and Steven Schockaert. 2023. RAGAs: Automated Evaluation of
Retrieval Augmented Generation. Conference of the European Chapter of the Association for Computational Linguistics
abs/2309.15217 (2023).
[34] Zhangyin Feng, Xiaocheng Feng, Dezhi Zhao, Maojin Yang, and Bing Qin. 2024. Retrieval-Generation Synergy
Augmented Large Language Models. In IEEE International Conference on Acoustics, Speech, and Signal Processing,
Vol. abs/2310.05149. IEEE.
[35] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish
Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2021. The Pile: An 800GB Dataset of Diverse Text for
Language Modeling. arXiv abs/2101.00027 (2021).
[36] Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. 2023. Precise Zero-Shot Dense Retrieval without Relevance
Labels. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers). Association for Computational Linguistics, 1762–1777.
[37] Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple Contrastive Learning of Sentence Embeddings.
In Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational
Linguistics, 6894–6910.
[38] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen
Wang. 2023. Retrieval-Augmented Generation for Large Language Models: A Survey. arXiv abs/2312.10997 (2023).
[39] Yunfan Gao, Yun Xiong, Meng Wang, and Haofen Wang. 2024. Modular RAG: Transforming RAG Systems into
LEGO-like Reconfigurable Frameworks. arXiv (2024).
[40] Michael Glass, Gaetano Rossiello, Md Faisal Mahbub Chowdhury, Ankita Naik, Pengshan Cai, and Alfio Gliozzo. 2022.
Re2G: Retrieve, Rerank, Generate. In Proceedings of the Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics, 2701–2715.
[41] Simon Gottschalk and Elena Demidova. 2018. EventKG: A Multilingual Event-Centric Temporal Knowledge Graph.
Springer International Publishing. 272–287 pages.
[42] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020. Retrieval Augmented Language
Model Pre-Training. In International Conference on Machine Learning (ICML). 3929–3938.
[43] William L. Hamilton. 2020. Graph representation learning. Springer International Publishing.
[44] Micheline Hancock-Beaulieu, Mike Gatford, Xiangji Huang, Stephen E. Robertson, Steve Walker, and P. W. Williams.
1996. Okapi at TREC-5. In Proceedings of The Fifth Text REtrieval Conference, TREC 1996, Gaithersburg, Maryland,
USA, November 20-22, 1996 (NIST Special Publication, Vol. 500-238), Ellen M. Voorhees and Donna K. Harman (Eds.).
National Institute of Standards and Technology (NIST). http://trec.nist.gov/pubs/trec5/papers/city.procpaper.ps.gz
[45] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021.
Measuring Massive Multitask Language Understanding.. In International Conference on Learning Representations
(ICLR).
[46] Enrique HerreraViedma, Gabriella Pasi, Antonio G. LopezHerrera, and Carlos Porcel. 2006. Evaluating the information
quality of Web sites: A methodology based on fuzzy computing with words. Journal of the American Society for
Information Science and Technology 57, 4 (2006), 538–549.
[47] Sebastian Hofstätter, Jiecao Chen, Karthik Raman, and Hamed Zamani. 2023. Fid-light: Efficient and effective retrieval-
augmented text generation. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development
in Information Retrieval. 1437–1447.
[48] Yucheng Hu and Yuxing Lu. 2024. RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural
Language Processing. arXiv abs/2404.19543 (2024).
[49] Ziniu Hu, Ahmet Iscen, Chen Sun, Zirui Wang, Kai-Wei Chang, Yizhou Sun, Cordelia Schmid, David A. Ross, and
Alireza Fathi. 2023. Reveal: Retrieval-Augmented Visual-Language Pre-Training with Multi-Source Multimodal
Knowledge Memory. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 23369–
23379.
, Vol. 1, No. 1, Article . Publication date: August 2018.
The Survey of Retrieval-Augmented Text Generation in Large Language Models
31
[50] Jie Huang, Hanyin Shao, Kevin Chen-Chuan Chang, Jinjun Xiong, and Wen-mei Hwu. 2022. Understanding Jargon:
Combining Extraction and Generation for Definition Modeling. In Proceedings of the Conference on Empirical Methods
in Natural Language Processing. Association for Computational Linguistics.
[51] Jimmy Xiangji Huang, Jun Miao, and Ben He. 2013. High performance query expansion using adaptive co-training.
Inf. Process. Manag. 49, 2 (2013), 441–453. https://doi.org/10.1016/J.IPM.2012.08.002
[52] Qiushi Huang, Shuai Fu, Xubo Liu, Wenwu Wang, Tom Ko, Yu Zhang, and Lilian H. Y. Tang. 2023. Learning Retrieval
Augmentation for Personalized Dialogue Generation.. In Conference on Empirical Methods in Natural Language
Processing (EMNLP). 2523–2540.
[53] Wenyu Huang, Mirella Lapata, Pavlos Vougiouklis, Nikos Papasarantopoulos, and Jeff Z Pan. 2023.
Retrieval
Augmented Generation with Rich Answer Encoding. Proc. of IJCNLP-AACL 2023 (2023).
[54] Xiangji Huang and Qinmin Hu. 2009. A bayesian learning approach to promoting diversity in ranking for biomedical
information retrieval. In Proceedings of the 32nd Annual International ACM SIGIR Conference on Research and Devel-
opment in Information Retrieval, SIGIR 2009, Boston, MA, USA, July 19-23, 2009, James Allan, Javed A. Aslam, Mark
Sanderson, ChengXiang Zhai, and Justin Zobel (Eds.). ACM, 307–314. https://doi.org/10.1145/1571941.1571995
[55] Yizheng Huang and Jimmy Huang. 2024. Exploring ChatGPT for Next-generation Information Retrieval: Opportunities
and Challenges. CoRR abs/2402.11203 (2024). https://doi.org/10.48550/ARXIV.2402.11203 arXiv:2402.11203
[56] Yizheng Huang and Jimmy X. Huang. 2023. Diversified Prior Knowledge Enhanced General Language Model for
Biomedical Information Retrieval. In ECAI 2023 - 26th European Conference on Artificial Intelligence, September 30 -
October 4, 2023, Kraków, Poland - Including 12th Conference on Prestigious Applications of Intelligent Systems (PAIS 2023)
(Frontiers in Artificial Intelligence and Applications, Vol. 372), Kobi Gal, Ann Nowé, Grzegorz J. Nalepa, Roy Fairstein,
and Roxana Radulescu (Eds.). IOS Press, 1109–1115. https://doi.org/10.3233/FAIA230385
[57] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard
Grave. 2022. Unsupervised Dense Information Retrieval with Contrastive Learning. Transactions on Machine Learning
Research (TMLR) 2022 (2022).
[58] Gautier Izacard and Edouard Grave. 2021. Leveraging Passage Retrieval with Generative Models for Open Domain
Question Answering. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational
Linguistics: Main Volume. Association for Computational Linguistics, 874–880.
[59] Gautier Izacard, Patrick S. H. Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu,
Armand Joulin, Sebastian Riedel, and Edouard Grave. 2023. Atlas: Few-shot Learning with Retrieval Augmented
Language Models. Journal of Machine Learning Research (JMLR) 24 (2023), 251:1–251:43.
[60] Israt Jahan, Md. Tahmid Rahman Laskar, Chun Peng, and Jimmy Xiangji Huang. 2023. Evaluation of ChatGPT on
Biomedical Tasks: A Zero-Shot Comparison with Fine-Tuned Generative Transformers. CoRR abs/2306.04504 (2023).
https://doi.org/10.48550/ARXIV.2306.04504 arXiv:2306.04504
[61] Bernard J. Jansen, Danielle L. Booth, and Amanda Spink. 2009. Patterns of query reformulation during Web searching.
J. Assoc. Inf. Sci. Technol. 60, 7 (2009), 1358–1371. https://doi.org/10.1002/ASI.21071
[62] H Jégou, M Douze, and C Schmid. 2011. Product Quantization for Nearest Neighbor Search. IEEE Transactions on
Pattern Analysis and Machine Intelligence 33, 1 (2011), 117–128.
[63] Wenqi Jiang, Marco Zeller, Roger Waleffe, Torsten Hoefler, and Gustavo Alonso. 2023. Chameleon: a Heterogeneous
and Disaggregated Accelerator System for Retrieval-Augmented Language Models. arXiv abs/2310.09949 (2023).
[64] Wenqi Jiang, Shuai Zhang, Boran Han, Jie Wang, Bernie Wang, and Tim Kraska. 2024. PipeRAG: Fast Retrieval-
Augmented Generation via Algorithm-System Co-design. arXiv abs/2403.05676 (2024).
[65] Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and
Graham Neubig. 2023. Active Retrieval Augmented Generation. In Conference on Empirical Methods in Natural
Language Processing (EMNLP). 7969–7992.
[66] Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. 2021. What Disease Does
This Patient Have? A Large-Scale Open Domain Question Answering Dataset from Medical Exams. Applied Sciences
11, 14 (2021), 6421.
[67] Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William W. Cohen, and Xinghua Lu. 2019. PubMedQA: A Dataset for
Biomedical Research Question Answering.. In Conference on Empirical Methods in Natural Language Processing
(EMNLP). 2567–2577.
[68] Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2021. Billion-Scale Similarity Search with GPUs. IEEE Transactions on
Big Data 7, 3 (2021), 535–547. https://doi.org/10.1109/TBDATA.2019.2921572
[69] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. TriviaQA: A Large Scale Distantly Supervised
Challenge Dataset for Reading Comprehension. In Proceedings of the 55th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, 1601–1611.
[70] Minki Kang, Jin Myung Kwak, Jinheon Baek, and Sung Ju Hwang. 2023. Knowledge Graph-Augmented Language
Models for Knowledge-Grounded Dialogue Generation. arXiv abs/2305.18846 (2023).
, Vol. 1, No. 1, Article . Publication date: August 2018.
32
Huang et al.
[71] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and
Wen-tau Yih. 2020. Dense Passage Retrieval for Open-Domain Question Answering.. In Conference on Empirical
Methods in Natural Language Processing (EMNLP). 6769–6781.
[72] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2020. Generalization through
Memorization: Nearest Neighbor Language Models. In International Conference on Learning Representations (ICLR).
[73] Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, and Matei Zaharia.
2022. Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP. arXiv
abs/2212.14024 (2022).
[74] Omar Khattab and Matei Zaharia. 2020. ColBERT - Efficient and Effective Passage Search via Contextualized Late
Interaction over BERT. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in
Information Retrieval. ACM, 39–48.
[75] Sanghoon Kim, Dahyun Kim, Chanjun Park, Wonsung Lee, Wonho Song, Yunsu Kim, Hyeonwoo Kim, Yungi Kim,
Hyeonju Lee, Jihoo Kim, Changbae Ahn, Seonghoon Yang, Sukyung Lee, Hyunbyung Park, Gyoungjin Gim, Miky-
oung Cha, Hwalsuk Lee, and Sunghun Kim. 2024. SOLAR 10.7B: Scaling Large Language Models with Simple yet
Effective Depth Up-Scaling. In Proceedings of the Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies (Volume 6: Industry Track), Vol. abs/2312.15166. Association
for Computational Linguistics.
[76] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle
Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei
Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural Questions: A Benchmark for
Question Answering Research. Transactions of the Association for Computational Linguistics 7 (2019), 453–466.
[77] Md. Tahmid Rahman Laskar, M. Saiful Bari, Mizanur Rahman, Md Amran Hossen Bhuiyan, Shafiq Joty, and Jimmy Xi-
angji Huang. 2023. A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets. CoRR
abs/2305.18486 (2023). https://doi.org/10.48550/ARXIV.2305.18486 arXiv:2305.18486
[78] Md. Tahmid Rahman Laskar, Enamul Hoque, and Jimmy X. Huang. 2020. Query Focused Abstractive Summarization
via Incorporating Query Relevance and Transfer Learning with Transformer Models. In Advances in Artificial
Intelligence - 33rd Canadian Conference on Artificial Intelligence, Canadian AI 2020, Ottawa, ON, Canada, May 13-15,
2020, Proceedings (Lecture Notes in Computer Science, Vol. 12109), Cyril Goutte and Xiaodan Zhu (Eds.). Springer,
342–348. https://doi.org/10.1007/978-3-030-47358-7_35
[79] Carlos Lassance and Stéphane Clinchant. 2022. Naver Labs Europe (SPLADE) @ TREC NeuCLIR 2022.. In Text
Retrieval Conference (TREC).
[80] Carlos Lassance, Hervé Déjean, Thibault Formal, and Stéphane Clinchant. 2024. SPLADE-v3: New baselines for
SPLADE. arXiv abs/2403.06789 (2024).
[81] Myeonghwa Lee, Seonho An, and Min-Soo Kim. 2024. PlanRAG: A Plan-then-Retrieval Augmented Generation for
Generative Large Language Models as Decision Makers. In Proceedings of the Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers). Association
for Computational Linguistics.
[82] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov,
and Luke Zettlemoyer. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation,
Translation, and Comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational
Linguistics. Association for Computational Linguistics, 7871–7880.
[83] Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich
Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-Augmented
Generation for Knowledge-Intensive NLP Tasks.. In Conference on Neural Information Processing Systems (NeurIPS).
[84] Canjia Li, Andrew Yates, Sean MacAvaney, Ben He, and Yingfei Sun. 2024. PARADE: Passage Representation
Aggregation forDocument Reranking. ACM Transactions on Information Systems 42, 2 (2024), 1–26.
[85] Huayang Li, Yixuan Su, Deng Cai, Yan Wang, and Lemao Liu. 2022. A Survey on Retrieval-Augmented Text Generation.
arXiv abs/2202.01110 (2022).
[86] Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Shafiq R. Joty, Soujanya Poria, and Lidong Bing. 2024.
Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous
Sources. In The Twelfth International Conference on Learning Representations.
[87] Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. In Text Summarization Branches
Out. Association for Computational Linguistics, Barcelona, Spain, 74–81. https://aclanthology.org/W04-1013
[88] Sheng-Chieh Lin, Akari Asai, Minghan Li, Barlas Oguz, Jimmy Lin, Yashar Mehdad, Wen-tau Yih, and Xilun Chen.
2023. How to Train Your Dragon: Diverse Augmentation Towards Generalizable Dense Retrieval. In Findings of the
Association for Computational Linguistics: EMNLP 2023. Association for Computational Linguistics, 6385–6400.
, Vol. 1, No. 1, Article . Publication date: August 2018.
The Survey of Retrieval-Augmented Text Generation in Large Language Models
33
[89] Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Richard James, Pedro Rodriguez, Jacob Kahn,
Gergely Szilvasy, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2024. RA-DIT: Retrieval-Augmented Dual
Instruction Tuning. In The Twelfth International Conference on Learning Representations, Vol. abs/2310.01352.
[90] Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal,
Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O’Horo,
Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, and Xian Li. 2022. Few-shot Learning
with Multilingual Generative Language Models. In Proceedings of the Conference on Empirical Methods in Natural
Language Processing. Association for Computational Linguistics.
[91] Yi Liu, Lianzhe Huang, Shicheng Li, Sishuo Chen, Hao Zhou, Fandong Meng, Jie Zhou, and Xu Sun. 2023. RECALL: A
Benchmark for LLMs Robustness against External Counterfactual Knowledge. arXiv abs/2311.08147 (2023).
[92] Ziyang Luo, Can Xu, Pu Zhao, Xiubo Geng, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023. Augmented
Large Language Models with Parametric Knowledge Guiding. arXiv abs/2305.04757 (2023).
[93] Hengzhao Ma, Jianzhong Li, and Yong Zhang. 2024. Reconsidering Tree based Methods for k-Maximum Inner-Product
Search: The LRUS-CoverTree. In 2024 IEEE 40th International Conference on Data Engineering (ICDE). IEEE.
[94] Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. 2023. Query Rewriting in Retrieval-Augmented
Large Language Models. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.
Association for Computational Linguistics, 5303–5315.
[95] Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. 2024. Fine-Tuning LLaMA for Multi-Stage Text
Retrieval. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information
Retrieval, Vol. 1. ACM, 2421–2425.
[96] Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. 2024. Fine-Tuning LLaMA for Multi-Stage Text
Retrieval. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information
Retrieval, SIGIR 2024, Washington DC, USA, July 14-18, 2024, Grace Hui Yang, Hongning Wang, Sam Han, Claudia
Hauff, Guido Zuccon, and Yi Zhang (Eds.). ACM, 2421–2425. https://doi.org/10.1145/3626772.3657951
[97] Yu A. Malkov and D. A. Yashunin. 2020. Efficient and Robust Approximate Nearest Neighbor Search Using Hierarchical
Navigable Small World Graphs. IEEE Transactions on Pattern Analysis and Machine Intelligence 42, 4 (2020), 824–836.
https://doi.org/10.1109/TPAMI.2018.2889473
[98] Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze. 2008. Introduction to information retrieval.
Cambridge University Press. https://doi.org/10.1017/CBO9780511809071
[99] Niklas Muennighoff. 2022. SGPT: GPT Sentence Embeddings for Semantic Search. arXiv abs/2202.08904 (2022).
[100] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu
Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button,
Matthew Knight, Benjamin Chess, and John Schulman. 2021. WebGPT: Browser-assisted question-answering with
human feedback. arXiv abs/2112.09332 (2021).
[101] Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez Abrego, Ji Ma, Vincent Zhao, Yi Luan, Keith Hall, Ming-
Wei Chang, and Yinfei Yang. 2022. Large Dual Encoders Are Generalizable Retrievers. In Proceedings of the Conference
on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 9844–9855.
[102] Rodrigo Nogueira, Zhiying Jiang, Ronak Pradeep, and Jimmy Lin. 2020. Document Ranking with a Pretrained
Sequence-to-Sequence Model. In Findings of the Association for Computational Linguistics: EMNLP 2020. Association
for Computational Linguistics.
[103] Rodrigo Nogueira, Wei Yang, Kyunghyun Cho, and Jimmy Lin. 2019. Multi-stage document ranking with BERT. CoRR
abs/1910.14424 (2019).
[104] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini
Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow
instructions with human feedback. In Conference on Neural Information Processing Systems (NeurIPS).
[105] Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. 2022. MedMCQA: A Large-scale Multi-Subject
Multi-Choice Dataset for Medical domain Question Answering.. In Conference on Health, Inference, and Learning
(CHIL). 248–260.
[106] Yu Pan, Jianxin Sun, and Hongfeng Yu. 2023. LM-DiskANN: Low Memory Footprint in Disk-Native Dynamic
Graph-Based ANN Indexing. In 2023 IEEE International Conference on Big Data (BigData). IEEE.
[107] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a method for automatic evaluation
of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics
(Philadelphia, Pennsylvania) (ACL ’02). Association for Computational Linguistics, USA, 311–318. https://doi.org/10.
3115/1073083.1073135
[108] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Hamza Alobeidli, Alessandro Cappelli, Bap-
tiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023. The RefinedWeb Dataset for Falcon LLM: Outperforming
, Vol. 1, No. 1, Article . Publication date: August 2018.
34
Huang et al.
Curated Corpora with Web Data Only.. In Conference on Neural Information Processing Systems (NeurIPS).
[109] Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine
Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rocktäschel, and Sebastian Riedel. 2021. KILT: a
Benchmark for Knowledge Intensive Language Tasks. In Proceedings of the Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language Technologies. Association for Computational
Linguistics, 2523–2544.
[110] Filip Radlinski and Nick Craswell. 2010. Comparing the sensitivity of information retrieval metrics.. In Annual
International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR). 667–674.
[111] Colin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li,
and Peter J. Liu. 2020. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Journal of
Machine Learning Research (JMLR) 21 (2020), 140:1–140:67.
[112] Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023.
In-Context Retrieval-Augmented Language Models. Transactions of the Association for Computational Linguistics 11
(2023), 1316–1331.
[113] Ori Ram, Gal Shachaf, Omer Levy, Jonathan Berant, and Amir Globerson. 2022. Learning to Retrieve Passages without
Supervision. In Proceedings of the Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies. Association for Computational Linguistics.
[114] David Rau, Hervé Déjean, Nadezhda Chirkova, Thibault Formal, Shuai Wang, Vassilina Nikoulina, and Stéphane
Clinchant. 2024. BERGEN: A Benchmarking Library for Retrieval-Augmented Generation. arXiv abs/2407.01102
(2024).
[115] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. In
Proceedings of the Conference on Empirical Methods in Natural Language Processing and the 9th International Joint
Conference on Natural Language Processing (EMNLP-IJCNLP). Association for Computational Linguistics, 3980–3990.
[116] Stephen Robertson and Hugo Zaragoza. 2009. The Probabilistic Relevance Framework: BM25 and Beyond. Foundations
and Trends® in Information Retrieval 3, 4 (2009), 333–389.
[117] Jon Saad-Falcon, O. Khattab, Christopher Potts, and Matei Zaharia. 2023. ARES: An Automated Evaluation Framework
for Retrieval-Augmented Generation Systems. In North American Chapter of the Association for Computational
Linguistics, Vol. abs/2311.09476.
[118] Alireza Salemi, Surya Kallumadi, and Hamed Zamani. 2024. Optimization Methods for Personalizing Large Language
Models through Retrieval Augmentation. In Proceedings of the 47th International ACM SIGIR Conference on Research
and Development in Information Retrieval. ACM, 752–762.
[119] Alireza Salemi and Hamed Zamani. 2024. Evaluating Retrieval Quality in Retrieval-Augmented Generation. In
Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval,
Vol. 21. ACM, 2395–2400.
[120] Sara Sarto, Marcella Cornia, Lorenzo Baraldi, and Rita Cucchiara. 2022. Retrieval-Augmented Transformer for Image
Captioning. In International Conference on Content-based Multimedia Indexing. ACM.
[121] Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. 2023. Enhancing Retrieval-
Augmented Large Language Models with Iterative Retrieval-Generation Synergy. In Findings of the Association for
Computational Linguistics: EMNLP 2023. Association for Computational Linguistics, 9248–9274.
[122] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, M. Lewis, Luke Zettlemoyer, and Wen-tau Yih.
2023. REPLUG: Retrieval-Augmented Black-Box Language Models. In North American Chapter of the Association for
Computational Linguistics, Vol. abs/2301.12652.
[123] Yunxiao Shi, Xing Zi, Zijing Shi, Haimin Zhang, Qiang Wu, and Min Xu. 2024. ERAGent: Enhancing Retrieval-
Augmented Language Models with Improved Accuracy, Efficiency, and Personalization. arXiv abs/2405.06683 (2024).
[124] Weihang Su, Yichen Tang, Qingyao Ai, Zhijing Wu, and Yiqun Liu. 2024. DRAGIN: Dynamic Retrieval Augmented
Generation based on the Real-time Information Needs of Large Language Models. arXiv abs/2403.10081 (2024).
[125] Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and Denny Zhou. 2023. Recitation-Augmented Language Models.
In International Conference on Learning Representations (ICLR).
[126] Kento Tatsuno, Daisuke Miyashita, Taiga Ikeda, Kiyoshi Ishiyama, Kazunari Sumiyoshi, and Jun Deguchi. 2024.
AiSAQ: All-in-Storage ANNS with Product Quantization for DRAM-free Information Retrieval. arXiv abs/2404.06004
(2024).
[127] Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Dara Bahri,
Tal Schuster, Huaixiu Steven Zheng, Denny Zhou, Neil Houlsby, and Donald Metzler. 2023. UL2: Unifying Language
Learning Paradigms. In International Conference on Learning Representations (ICLR).
[128] Yi Tay, Vinh Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe Zhao, Jai Prakash
Gupta, Tal Schuster, William W. Cohen, and Donald Metzler. 2022. Transformer Memory as a Differentiable Search
Index. In Conference on Neural Information Processing Systems (NeurIPS).
, Vol. 1, No. 1, Article . Publication date: August 2018.
The Survey of Retrieval-Augmented Text Generation in Large Language Models
35
[129] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. FEVER: a Large-scale Dataset
for Fact Extraction and VERification. In Proceedings of the Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers). Association for Computational
Linguistics, 809–819.
[130] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,
Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen,
Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami,
Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian
Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie,
Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith,
Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert
Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open Foundation and Fine-Tuned Chat Models. arXiv
abs/2307.09288 (2023).
[131] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2023. Interleaving Retrieval with
Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions. In Proceedings of the 61st Annual Meeting
of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics,
10014–10037.
[132] George Tsatsaronis, Georgios Balikas, Prodromos Malakasiotis, Ioannis Partalas, Matthias Zschunke, Michael R
Alvers, Dirk Weissenborn, Anastasia Krithara, Sergios Petridis, Dimitris Polychronopoulos, Yannis Almirantis, John
Pavlopoulos, Nicolas Baskiotis, Patrick Gallinari, Thierry Artiéres, Axel-Cyrille Ngonga Ngomo, Norman Heino, Eric
Gaussier, Liliana Barrio-Alvers, Michael Schroeder, Ion Androutsopoulos, and Georgios Paliouras. 2015. An overview
of the BIOASQ large-scale biomedical semantic indexing and question answering competition. BMC Bioinformatics
16, 1 (2015).
[133] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and
Illia Polosukhin. 2017. Attention is All you Need. In Neural Information Processing Systems. 5998–6008.
[134] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and
Samuel R. Bowman. 2019. SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems.
In Conference on Neural Information Processing Systems (NeurIPS). 3261–3275.
[135] Haoyu Wang, Tuo Zhao, and Jing Gao. 2024. BlendFilter: Advancing Retrieval-Augmented Large Language Models
via Query Generation Blending and Knowledge Filtering. arXiv abs/2402.11129 (2024).
[136] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei.
2022. Text Embeddings by Weakly-Supervised Contrastive Pre-training. arXiv abs/2212.03533 (2022).
[137] Liang Wang, Nan Yang, and Furu Wei. 2023. Query2doc: Query Expansion with Large Language Models. In Proceedings
of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics,
9414–9423.
[138] Qifan Wang, Yi Fang, Anirudh Ravula, Fuli Feng, Xiaojun Quan, and Dongfang Liu. 2022. WebFormer: The Web-page
Transformer for Structure Information Extraction. In Proceedings of the ACM Web Conference 2022. ACM.
[139] Xiaohua Wang, Zhenghua Wang, Xuan Gao, Feiran Zhang, Yixin Wu, Zhibo Xu, Tianyuan Shi, Zhengyuan Wang,
Shizheng Li, Qi Qian, Ruicheng Yin, Changze Lv, Xiaoqing Zheng, and Xuanjing Huang. 2024. Searching for Best
Practices in Retrieval-Augmented Generation. arXiv abs/2407.01219 (2024).
[140] Xintao Wang, Qianwen Yang, Yongting Qiu, Jiaqing Liang, Qianyu He, Zhouhong Gu, Yanghua Xiao, and Wei Wang.
2023. KnowledGPT: Enhancing Large Language Models with Retrieval and Storage Access on Knowledge Bases.
arXiv abs/2308.11761 (2023).
[141] Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md. Rizwan Parvez, and Graham Neubig. 2023. Learning to Filter Context
for Retrieval-Augmented Generation. arXiv abs/2311.08377 (2023).
[142] BigScience Workshop, :, Teven Le Scao, Angela Fan, Christopher Akiki, and etc. 2022. BLOOM: A 176B-Parameter
Open-Access Multilingual Language Model. arXiv abs/2211.05100 (2022).
[143] Shitao Xiao, Zheng Liu, Yingxia Shao, and Zhao Cao. 2022. RetroMAE: Pre-Training Retrieval-oriented Language
Models Via Masked Auto-Encoder.. In Conference on Empirical Methods in Natural Language Processing (EMNLP).
538–548.
[144] Guangzhi Xiong, Qiao Jin, Zhiyong Lu, and Aidong Zhang. 2024. Benchmarking Retrieval-Augmented Generation
for Medicine. arXiv abs/2402.13178 (2024).
[145] Jie Xiong, Li Yu, Xi Niu, and Youfang Leng. 2023. XRR: Extreme multi-label text classification with candidate retrieving
and deep ranking. Information Sciences 622 (2023), 115–132.
, Vol. 1, No. 1, Article . Publication date: August 2018.
36
Huang et al.
[146] Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul N. Bennett, Junaid Ahmed, and Arnold Overwijk.
2021. Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval. In International
Conference on Learning Representations (ICLR).
[147] Fangyuan Xu, Weijia Shi, and Eunsol Choi. 2024. RECOMP: Improving Retrieval-Augmented LMs with Context
Compression and Selective Augmentation. In The Twelfth International Conference on Learning Representations,
Vol. abs/2310.04408.
[148] Shicheng Xu, Liang Pang, Jun Xu, Huawei Shen, and Xueqi Cheng. 2024. List-aware Reranking-Truncation Joint
Model for Search and Retrieval-augmented Generation. In Proceedings of the ACM on Web Conference 2024, Vol. 21.
ACM, 1330–1340.
[149] Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling. 2024. Corrective Retrieval Augmented Generation. arXiv
abs/2401.15884 (2024).
[150] Diji Yang, Jinmeng Rao, Kezhen Chen, Xiaoyuan Guo, Yawen Zhang, Jie Yang, and Yi Zhang. 2024. IM-RAG: Multi-
Round Retrieval-Augmented Generation Through Learning Inner Monologues. In Proceedings of the 47th International
ACM SIGIR Conference on Research and Development in Information Retrieval, Vol. 33. ACM, 730–740.
[151] Haoyan Yang, Zhitao Li, Yong Zhang, Jianzong Wang, Ning Cheng, Ming Li, and Jing Xiao. 2023. PRCA: Fitting
Black-Box Large Language Models for Retrieval Question Answering via Pluggable Reward-Driven Contextual
Adapter. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for
Computational Linguistics, 5364–5375.
[152] Zhuolin Yang, Wei Ping, Zihan Liu, Vijay Korthikanti, Weili Nie, De-An Huang, Linxi Fan, Zhiding Yu, Shiyi Lan, Bo
Li, Mohammad Shoeybi, Ming-Yu Liu, Yuke Zhu, Bryan Catanzaro, Chaowei Xiao, and Anima Anandkumar. 2023.
Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot Image Captioning. In Findings of the
Association for Computational Linguistics: EMNLP 2023. Association for Computational Linguistics, 11844–11857.
[153] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D.
Manning. 2018. HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering. In Proceedings of
the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics,
2369–2380.
[154] Hao Yu, Aoran Gan, Kai Zhang, Shiwei Tong, Qi Liu, and Zhaofeng Liu. 2024. Evaluation of Retrieval-Augmented
Generation: A Survey. arXiv abs/2405.07437 (2024).
[155] Shi Yu, Jiahua Liu, Jingqin Yang, Chenyan Xiong, Paul N. Bennett, Jianfeng Gao, and Zhiyuan Liu. 2020. Few-
Shot Generative Conversational Query Rewriting. In Proceedings of the 43rd International ACM SIGIR conference
on research and development in Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020, Jimmy X.
Huang, Yi Chang, Xueqi Cheng, Jaap Kamps, Vanessa Murdock, Ji-Rong Wen, and Yiqun Liu (Eds.). ACM, 1933–1936.
https://doi.org/10.1145/3397271.3401323
[156] Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng,
and Meng Jiang. 2023. Generate rather than Retrieve: Large Language Models are Strong Context Generators. In
International Conference on Learning Representations (ICLR).
[157] Zichun Yu, Chenyan Xiong, Shi Yu, and Zhiyuan Liu. 2023. Augmentation-Adapted Retriever Improves Generalization
of Language Models as Generic Plug-In. In Proceedings of the 61st Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, 2421–2436.
[158] Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D. Plumbley, and Wenwu Wang. 2024. Retrieval-Augmented
Text-to-Audio Generation. In ICASSP - IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP), Vol. abs/2309.08051. IEEE.
[159] Zhenrui Yue, Huimin Zeng, Yimeng Lu, Lanyu Shang, Yang Zhang, and Dong Wang. 2024. Evidence-Driven Retrieval
Augmented Response Generation for Online Misinformation. In North American Chapter of the Association for
Computational Linguistics, Vol. abs/2403.14952.
[160] Saber Zerhoudi and Michael Granitzer. 2024. PersonaRAG: Enhancing Retrieval-Augmented Generation Systems
with User-Centric Agents. arXiv (2024).
[161] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona T.
Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura,
Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. OPT: Open Pre-trained Transformer Language Models.
arXiv abs/2205.01068 (2022).
[162] Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang, Jie
Jiang, and Bin Cui. 2024. Retrieval-Augmented Generation for AI-Generated Content: A Survey. arXiv abs/2402.19473
(2024).
[163] Huaixiu Steven Zheng, Swaroop Mishra, Xinyun Chen, Heng-Tze Cheng, Ed H. Chi, Quoc V Le, and Denny Zhou.
2024. Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models. In The Twelfth International
Conference on Learning Representations, Vol. abs/2310.06117.
, Vol. 1, No. 1, Article . Publication date: August 2018.
The Survey of Retrieval-Augmented Text Generation in Large Language Models
37
[164] Ning Zhong, Yuefeng Li, and Sheng-Tang Wu. 2012. Effective Pattern Discovery for Text Mining. IEEE Transactions
on Knowledge and Data Engineering 24, 1 (2012), 30–44.
[165] Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Zhicheng Dou, and Ji-Rong
Wen. 2023. Large Language Models for Information Retrieval: A Survey. arXiv abs/2308.07107 (2023).
[166] Honglei Zhuang, Zhen Qin, Rolf Jagerman, Kai Hui, Ji Ma, Jing Lu, Jianmo Ni, Xuanhui Wang, and Michael Bendersky.
2023. RankT5: Fine-Tuning T5 for Text Ranking with Ranking Losses. In Proceedings of the 46th International ACM
SIGIR Conference on Research and Development in Information Retrieval. ACM.
Received 20 February 2007; revised 12 March 2009; accepted 5 June 2009
, Vol. 1, No. 1, Article . Publication date: August 2018.
