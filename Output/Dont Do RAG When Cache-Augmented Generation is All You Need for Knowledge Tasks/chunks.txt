arXiv:2412.15605v2  [cs.CL]  23 Feb 2025
Don’t Do RAG:
When Cache-Augmented Generation is All You Need for
Knowledge Tasks
Brian J Chan∗
Chao-Ting Chen∗
Jui-Hung Cheng∗
Department of Computer Science
National Chengchi University
Taipei, Taiwan
{110703065,110703038,110703007}@nccu.edu.tw
Hen-Hsen Huang
Insititue of Information Science
Academia Sinica
Taipei, Taiwan
hhhuang@iis.sinica.edu.tw
Abstract
Retrieval-augmented generation (RAG) has gained traction as a
powerful approach for enhancing language models by integrating
external knowledge sources. However, RAG introduces challenges
such as retrieval latency, potential errors in document selection,
and increased system complexity. With the advent of large lan-
guage models (LLMs) featuring signiﬁcantly extended context win-
dows, this paper proposesan alternative paradigm, cache-augmented
generation (CAG) that bypasses real-time retrieval. Our method in-
volves preloading all relevant resources, especially when the docu-
ments or knowledge for retrieval are of a limited and manageable
size, into the LLM’s extended context and caching its runtime pa-
rameters. During inference, the model utilizes these preloaded pa-
rameters to answer queries without additional retrieval steps. Com-
parative analyses reveal that CAG eliminates retrieval latency and
minimizes retrieval errors while maintaining context relevance. Per-
formance evaluations across multiple benchmarks highlight sce-
narios where long-context LLMs either outperformor complement
traditional RAG pipelines. These ﬁndings suggest that, for certain
applications, particularly those with a constrained knowledge base,
CAG provide a streamlined and eﬃcient alternative to RAG, achiev-
ing comparable or superior results with reduced complexity.
CCS Concepts
• Computing methodologies→Discourse, dialogue and prag-
matics; Natural language generation; • Information systems
→Specialized information retrieval.
Keywords
Cache Augmented Generation, Retrieval Augmented Generation,
Retrieval-Free Question Answering, Large Language Models
∗Three authors contributed equally to this research.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
the author(s) must be honored. Abstracting with credit is permitted. To copy other-
wise, or republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee. Request permissions from permissions@acm.org.
WWW Companion ’25, April 28-May 2, 2025, Sydney, NSW, Australia
© 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-1331-6/2025/04
https://doi.org/10.1145/3701716.3715490
1
Introduction
The advent of retrieval-augmented generation (RAG) [2, 5] has
signiﬁcantly enhanced the capabilities of large language models
(LLMs) by dynamically integrating external knowledge sources. RAG
systems have proven eﬀective in handling open-domain questions
and specialized tasks, leveraging retrieval pipelines to provide con-
textually relevant answers. However, RAG is not without its draw-
backs. The need for real-time retrieval introduces latency, while
errors in selecting or ranking relevant documents can degrade the
quality of the generated responses. Additionally, integrating re-
trieval and generation components increases system complexity,
necessitating careful tuning and adding to the maintenance over-
head.
This paper proposes an alternative paradigm, cache-augmented
generation (CAG), leveraging the capabilities of long-context LLMs
to address these challenges. Instead of relying on a retrieval pipeline,
as shown in Figure 1, our approach involves preloading the LLM
with all relevant documents in advance and precomputing the key-
value (KV) cache [9], which encapsulates the inference state of
the LLM. The preloaded context enables the model to provide rich,
contextually accurate answers without the need for additional re-
trieval during runtime. This approach eliminates retrieval latency,
mitigates retrieval errors, and simpliﬁes system architecture, all
while maintaining high-quality responses by ensuring the model
processes all relevant context holistically.
Recent advances in long-context LLMs have extended their abil-
ity to process and reason over substantial textual inputs. For exam-
ple, Llama 3.1 [1] was trained with a 128K context length, and its
eﬀective context length is 32K in Llama 3.1 8B and 64K in Llama
3.1 70B [3]. This 32K to 64K context window is suﬃcient for stor-
ing knowledge sources such as internal company documentation,
FAQs, customer support logs, and domain-speciﬁc databases, mak-
ing it practical for many real-world applications. By accommodat-
ing larger context windows, these models can assimilate extensive
information in a single inference step, making them well-suited
for tasks like document comprehension, multi-turn dialogue, and
summarization of lengthy texts. This capability eliminates the de-
pendency on real-time retrieval, as all necessary information can
WWW Companion ’25, April 28-May 2, 2025, Sydney, NSW, Australia
Brian J Chan, Chao-Ting Chen, Jui-Hung Cheng, and Hen-Hsen Huang
IR Model
Query
Knowledge
Source
LLM
Context
푞
푟
Retrieval-Augmented Generation
Knowledge
Source
Query
LLM
Context
Cache
Oﬄine Preloading
푞
푟
Cache-Augmented Generation
Active during inference
Figure 1: Comparison of Retrieval-Augmented Generation
(RAG) and our Cache-Augmented Generation (CAG) Work-
ﬂows: The pink-shaded components represent the processes
active during real-time inference. In RAG (top section), the
IR model retrieves relevant information from the knowl-
edge source, and both the retrieved knowledge and query
are processed by the LLM during inference, introducing re-
trieval latency. In contrast, CAG (bottom section) preloads
and caches knowledge oﬀline, allowing the LLM to process
only the query during inference, eliminating retrieval over-
head and ensuring a more eﬃcient generation process.
be preloaded into the model. These developments create opportu-
nities to streamline workﬂows for knowledge-intensive tasks, po-
tentially reducing or even eliminating the need for traditional RAG
systems.
Recent studies [4, 7] have investigated the performance of long-
context models in RAG tasks, revealing that state-of-the-art mod-
els like GPT-o1, GPT-4, and Claude 3.5 can eﬀectively process large
amounts of retrieved data, outperforming traditional systems in
many scenarios. Findings suggest that as long as all documents
ﬁt within the extended context length, traditional RAG systems
can be replaced by these long-context models. Similarly, Lu et al.
[8] has demonstrated the beneﬁts of precomputed KV caching to
improve eﬃciency, albeit with the need for position ID rearrange-
ment to enable proper functioning. Nonetheless, these methods re-
main vulnerable to retrieval failures inherent to RAG systems.
Through a series of experiments comparing traditional RAG work-
ﬂows with our proposed approach, we identify scenarios where
long-context LLMs outperform RAG in both eﬃciency and accu-
racy. By addressing the technical and practical implications, this
paper aims to provide insights into when and why CAG may serve
as a streamlined, eﬀective alternative to RAG, particularly for cases
where the documents or knowledge for retrieval are of limited,
manageable size. Our ﬁndings challenge the default reliance on
RAG for knowledge integration tasks, oﬀering a simpliﬁed, robust
solution to harness the growing capabilities of long-context LLMs.
Our contributions are threefold as follows:
• Eﬃcient Alternative to RAG: We introduced a novel ap-
proach leveraging long-context LLMs with preloaded docu-
ments and precomputed KV caches, mitigating retrieval la-
tency, errors, and system complexity.
• QuantitativeAnalysis: We conducted extensive experiments
showing scenarios where long-context LLMs outperformtra-
ditional RAG systems, especially with manageable knowl-
edge bases.
• Practical Insights: This work provided actionable insights
into optimizing knowledge-intensive workﬂows, demonstrat-
ing the viability of retrieval-free methods for speciﬁc appli-
cations. Our CAG framework is released publicly.1
2
Methodology
Our CAG framework leverages the extended context capabilities of
long-context LLMs to enable retrieval-free knowledge integration.
By preloading external knowledge sources, such as a collection of
documents D = {푑1,푑2, . . . }, and precomputing the KV cache CKV,
we address the computational challenges and ineﬃciencies inher-
ent to real-time retrieval in traditional RAG systems. The operation
of our framework is divided into three phases:
(1) External Knowledge Preloading
In this phase, a curated collection of documents D relevant
to the target application is preprocessed and formatted to
ﬁt within the model’s extended context window. The LLM
M, with parameters 휃, processes D, transforming it into a
precomputed KV cache:
CKV = KV-Encode(D)
(1)
This KV cache, which encapsulates the inference state of
the LLM, is stored on disk or in memory for future use. The
computational cost of processing D is incurred only once,
regardless of the number of subsequent queries.
(2) Inference
During inference, the precomputed KV cache CKV is loaded
alongside the user’s query 푞. The LLM utilizes this cached
context to generate responses:
푟= M(D ⊕푞) = M(푞| CKV)
(2)
By preloading the external knowledge, this phase eliminates
retrieval latency and reduces risks of errors or omissions
that arise from dynamic retrieval. The combined prompt
D ⊕푞ensures a uniﬁed understanding of both the external
knowledge and the user query.
(3) Cache Reset
To maintain system performance across multiple inference
sessions, the KV cache, stored in memory, can be reset eﬃ-
ciently. As the KV cache grows in an append-only manner
with new tokens 푞= (푡1,푡2, . . . ,푡푘) sequentially appended,
resetting involves truncating these new tokens. This allows
for rapid reinitialization without reloading the entire cache
from disk, ensuring sustained speed and responsiveness.
The proposedmethodology oﬀers several signiﬁcant advantages
over traditional RAG systems:
1https://github.com/hhhuang/CAG
Don’t Do RAG:
When Cache-Augmented Generation is All You Need for Knowledge Tasks
WWW Companion ’25, April 28-May 2, 2025, Sydney, NSW, Australia
• ReducedInference Time: By eliminating the need for real-
time retrieval, the inference process becomes faster and more
eﬃcient, enabling quicker responses to user queries.
• Uniﬁed Context: Preloading the entire knowledge collec-
tion into the LLM provides a holistic and coherent under-
standing of the documents, resulting in improved response
quality and consistency across a wide range of tasks.
• Simpliﬁed Architecture: By removing the need to inte-
grate retrievers and generators, the system becomes more
streamlined, reducing complexity, improving maintainabil-
ity, and lowering development overhead.
Looking forward, our approach is poised to become even more
powerful with the anticipated advancements in LLMs. As future
models continue to expand their context length, they will be able
to process increasingly larger knowledge collections in a single in-
ference step. Additionally, the improved ability of these models to
extract and utilize relevant information from long contexts will
further enhance their performance. These two trends will signiﬁ-
cantly extend the usability of our approach, enabling it to handle
more complex and diverse applications. Consequently, our method-
ology is well-positioned to become a robust and versatile solution
for knowledge-intensive tasks, leveraging the growing capabilities
of next-generation LLMs.
3
Experiments
3.1
Experimental Setup
To evaluate the eﬀectiveness of our proposedmethod, we conducted
experiments using two widely recognized question-answering bench-
marks: the Stanford Question Answering Dataset (SQuAD) 1.0 [10]
and the HotPotQA dataset [11]. These datasets provide comple-
mentary challenges, with SQuAD focusing on precise, context-aware
answers within single passages and HotPotQA emphasizing multi-
hop reasoning across multiple documents. Each of both datasets
consists of documents D = {푑1,푑2, . . . } paired with questions Q =
{푞1,푞2, . . . } and golden responses R = {푟1,푟2, . . . }. These datasets
provide a robust platform for assessing both single-context com-
prehension and complex multi-hop reasoning.
To investigate how diﬀerent levels of reference text length im-
pact retrieval diﬃculty, we created three test sets for each dataset,
varying the size of the reference text. For example, in the HotPotQA-
small conﬁguration, we sampled 16 documents D푠⊂D from the
HotPotQA document set to form a long reference text. QA pairs as-
sociated with D푠were selected as test instances. The same method-
ology was applied to create test sets for SQuAD.
The dataset statistics are summarized in Table 1. As the number
of documents (and hence the length of the reference text) increases,
the task becomes more challenging, particularly for RAG systems.
Longer reference texts increase the diﬃculty of accurately retriev-
ing the correct information, which is crucial for LLMs to generate
high-quality responses.
The primary task involves generating accurate and contextually
relevant answers ˆR = {ˆ푟1, ˆ푟2, . . . } for the SQuAD and HotPotQA
questions, based on the respective preloaded passages. By leverag-
ing the precomputed key-value cache CKV = KV-Encode(D), our
system generates responses ˆ푟푖= M(푞푖| CKV) without relying on
Table 1: The SQuAD and HotPotQA test sets with varying ref-
erence text lengths, highlighting the number of documents,
questions, and associated responses for each conﬁguration.
Source
Size
# Docs
# Tokens
# QA Pairs
HotPotQA
Small
16
21k
1,392
Medium
32
43k
1,056
Large
64
85k
1,344
SQuAD
Small
3
21k
500
Medium
4
32k
500
Large
7
50k
500
retrieval mechanisms during inference. This uniﬁed approach al-
lows for direct performance comparisons against traditional RAG
systems, highlighting the strengths and limitations of our method
across diverse QA challenges.
The experiments were executed on Tesla V100 32G × 8 GPUs.
For all experiments, we used Llama 3.1 8B [1] as the underlying
LLM across all systems, including both the RAG baselines and our
proposed method. This model supports input sizes of up to 128k
tokens, enabling the processing of extensive contexts. For our pro-
posed method, the context of each dataset was preloaded into the
model via a precomputed KV cache. For SQuAD, the documents
DS were encoded into a KV cache CS
KV = KV-Encode(DS). For
HotPotQA, similarly, the documents DH were encoded into CH
KV =
KV-Encode(DH). These caches were stored oﬄine and loaded dur-
ing inference to eliminate the need for real-time retrieval, ensuring
comprehensive access to all relevant information for each dataset.
Our experiments were conducted on both the SQuAD and Hot-
PotQA datasets to evaluate the performance of diﬀerent systems
in terms of similarity to ground-truth answers, measured using
BERTScore [12]. Each dataset—SQuAD and HotPotQA—was evalu-
ated separately, with retrieval systems conﬁgured to fetch passages
exclusively from the respective dataset to ensure focused and fair
evaluation.
3.2
Baseline Systems
The baseline RAG systems were implemented using the LlamaIn-
dex framework,2 employing two retrieval strategies: BM25 for sparse
retrieval and OpenAI Indexes for dense retrieval. The details of
each baseline system are as follows:
(1) Sparse Retrieval System (BM25): The ﬁrst baseline sys-
tem employed BM25 indexes for retrieval. BM25, a sparse re-
trieval algorithm, ranks documents based on term frequency-
inverse document frequency (TF-IDF) and document length
normalization. Given a query 푞푖, BM25 retrieves the top-푘
passages P푘= {푝1, 푝2, . . . , 푝푘} from the indexed collection
D. These passages were then passed to the generator, M,
to synthesize answers:
ˆ푟푖= M(푞푖| P푘)
(3)
BM25 provides a robust and interpretable retrieval mecha-
nism, suited for tasks involving keyword matching.
2https://www.llamaindex.ai/framework
WWW Companion ’25, April 28-May 2, 2025, Sydney, NSW, Australia
Brian J Chan, Chao-Ting Chen, Jui-Hung Cheng, and Hen-Hsen Huang
Table 2: Experimental Results
HotPotQA
SQuAD
Size
System
Top-푘
BERT-Score
BERT-Score
Small
Sparse RAG
1
0.6788
0.7214
3
0.7626
0.7616
5
0.7676
0.7608
10
0.7521
0.7584
Dense RAG
1
0.7164
0.6216
3
0.7582
0.7106
5
0.7481
0.7334
10
0.7576
0.7586
CAG (Ours)
0.7951
0.7695
Medium
Sparse RAG
1
0.6592
0.6902
3
0.7546
0.7301
5
0.7633
0.7298
10
0.7458
0.7262
Dense RAG
1
0.6973
0.5871
3
0.7432
0.6702
5
0.7322
0.6890
10
0.7308
0.7310
CAG (Ours)
0.7821
0.7383
Large
Sparse RAG
1
0.6616
0.7254
3
0.7463
0.7634
5
0.7535
0.7658
10
0.7345
0.7613
Dense RAG
1
0.7020
0.6070
3
0.7409
0.7018
5
0.7234
0.7286
10
0.7374
0.7590
CAG (Ours)
0.7407
0.7734
(2) Dense Retrieval System (OpenAI Indexes): The second
baseline utilized OpenAI indexes,3 which employ dense em-
beddings to represent both documents and queries in a shared
semantic space. For a query 푞푖, dense retrieval selects the
top-푘passages P푘that semantically align with the query,
oﬀering improved contextual understanding compared to
sparse methods. These passages were similarly passed to
the generator for answer synthesis as Equation 3. This sys-
tem is particularly eﬀective for questions requiring nuanced
contextual matching beyond exact term overlap.
For the RAG baselines, the top-1, top-3, top-5, and top-10 re-
trieved passages were used for inference. In contrast, our CAG uti-
lized the preloaded context speciﬁc to each dataset to generate an-
swers without retrieval constraints.
3.3
Results
As shown in Table 2, the experimental results highlight key dis-
tinctions between our proposed CAG approach and RAG systems.
CAG consistently achieved the highest BERTScore in most cases,
outperforming both sparse and dense RAG methods. By preloading
the entire reference text from the test set, our method is immune to
3https://cookbook.openai.com/examples/evaluation/evaluate_rag_with_llamaindex
Table 3: Response Time (Seconds) Comparison on HotPotQA
Size
System
Retrieval
Generation
Small
Sparse RAG, Top-3
0.0008
0.7406
Sparse RAG, Top-10
0.0012
1.5595
Dense RAG, Top-3
0.4849
1.0093
Dense RAG, Top-10
0.3803
2.6608
CAG
-
0.8512
In-Context Learning
-
9.3197
Medium
Sparse RAG, Top-3
0.0008
0.7148
Sparse RAG, Top-10
0.0012
1.5306
Dense RAG, Top-3
0.4140
0.9566
Dense RAG, Top-10
0.4171
2.6361
CAG
-
1.4078
In-Context Learning
-
26.3717
Large
Sparse RAG, Top-3
0.0008
0.6667
Sparse RAG, Top-10
0.0012
1.5175
Dense RAG, Top-3
0.4123
0.9331
Dense RAG, Top-10
0.4100
2.6447
CAG
-
2.2631
In-Context Learning
-
92.0824
retrieval errors, ensuring holistic reasoning over all relevant infor-
mation. This advantage is particularly evident in scenarios where
RAG systems struggle with retrieving incomplete or irrelevant pas-
sages, leading to suboptimal answer generation.
However, as the data size increases, the performance gap be-
tween CAG and RAG narrows slightly, aligning with prior ﬁndings
that long-context LLMs may experience degradation when han-
dling very long contexts [6]. Additionally, the fact that sparse RAG
outperforms dense RAG suggests that the datasets may not be suf-
ﬁciently challenging, allowing traditional sparse retrieval to eﬀec-
tively capture most relevant information without requiring deeper
semantic retrieval. Despite these factors, the results underscore
the robustness and eﬃciency of CAG, particularly for tasks that
require a uniﬁed understanding of the source material. By fully
leveraging the long-context capabilities of Llama 3.1, our approach
bypasses retrieval challenges and maintains superior performance
in retrieval-free knowledge integration.
Table 3 and Figure 2 show the retrieval and generation time across
diﬀerent HotPotQA knowledge sizes for various RAG methods and
CAG. CAG eliminates retrieval time entirely, whereas sparse and
dense retrieval-based RAG systems require additional retrieval steps,
with dense retrieval incurring higher latency. Sparse RAG exhibits
insigniﬁcant retrieval latency in the experiments, but still requires
retrieval before generation. As the knowledge size increases, gener-
ation time grows across all methods, including CAG, highlighting
the computational cost of handling longer contexts. However, CAG
remains more eﬃcient than dense RAG, as it avoids retrieval over-
head while maintaining comparable or superior response times.
Table 3 also compares our CAG approach with standard in-context
learning, where the reference text is provided dynamically dur-
ing inference, requiring real-time KV-cache computation. The re-
sults demonstrate that CAG dramatically reduces generation time,
Don’t Do RAG:
When Cache-Augmented Generation is All You Need for Knowledge Tasks
WWW Companion ’25, April 28-May 2, 2025, Sydney, NSW, Australia
0
1
2
3
0
1
2
3
0
1
2
3
HotPotQA (Small)
Sparse RAG, Top-3
Sparse RAG, Top-10
Dense RAG, Top-3
Dense RAG, Top-10
CAG
HotPotQA (Medium)
Sparse RAG, Top-3
Sparse RAG, Top-10
Dense RAG, Top-3
Dense RAG, Top-10
CAG
HotPotQA (Large)
Sparse RAG, Top-3
Sparse RAG, Top-10
Dense RAG, Top-3
Dense RAG, Top-10
CAG
Retrieval time
Generation time
Figure 2: Response Time Comparison on HotPotQA (Sec-
onds). The x-axis represents response time in seconds across
diﬀerent knowledge sizes. CAG eliminates retrieval over-
head, while dense RAG incurs longer retrieval and genera-
tion times due to retrieving and feeding longer text chunks
into the LLM. Sparse RAG retrieves shorter text spans, re-
sulting in faster generation. As the knowledge size increases,
generation time grows for all methods, but CAG remains
competitive while bypassing retrieval completely.
particularly as the reference text length increases. This eﬃciency
stems from preloading the KV-cache, which eliminates the need to
process the reference text on the ﬂy.
4
Conclusion
As long-context LLMs evolve, we present a compelling case for
rethinking traditional RAG workﬂows. While our work empha-
sizes eliminating retrieval latency, there is potential for hybrid ap-
proaches that combine preloading with selective retrieval. For ex-
ample, a system could preload a foundation context and use re-
trieval only to augment edge cases or highly speciﬁc queries. This
would balance the eﬃciency of preloading with the ﬂexibility of
retrieval, making it suitable for scenarios where context complete-
ness and adaptability are equally important.
Limitations
Our method requires loading all relevant documents into the mod-
els context, making it well-suited for use cases such as internal
knowledge bases of small companies, FAQs, and call centers, where
the knowledge source is of a manageable size. However, this ap-
proach becomes impractical for signiﬁcantly larger datasets. Fortu-
nately, as LLMs continue to expand their context lengths and hard-
ware capabilities advance, this limitation is expected to diminish,
enabling broader applicability in the future.
Acknowledgments
This work was partially supported by National Science and Tech-
nology Council (NSTC), Taiwan, under the grant 112-2221-E-001-
016-MY3, by Academia Sinica, under the grant 236d-1120205, and
by National Center for High-performance Computing (NCHC), Na-
tional Applied Research Laboratories (NARLabs), and NSTC under
the project “Trustworthy AI Dialog Engine, TAIDE.” We thank Dis-
cover AI4 and the many individuals who have introduced, shared,
and discussed our work, contributing to its broader visibility.
References
[1] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad
Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan,
et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783 (2024).
[2] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai,
Jiawei Sun, and Haofen Wang. 2023. Retrieval-augmented generation for large
language models: A survey. arXiv preprint arXiv:2312.10997 (2023).
[3] Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima
Rekesh, Fei Jia, and Boris Ginsburg. 2024. RULER: What’s the Real Context
Size of Your Long-Context Language Models?. In First Conference on Language
Modeling. https://openreview.net/forum?id=kIoBbc76Sy
[4] Quinn Leng, Jacob Portes, Sam Havens, Matei Zaharia, and Michael Carbin. 2024.
Long Context RAG Performance of Large Language Models.
arXiv preprint
arXiv:2411.03538 (2024).
[5] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir
Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-
täschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp
tasks. Advances in Neural Information Processing Systems 33 (2020), 9459–9474.
[6] Tianle Li, Ge Zhang, Quy Duc Do, Xiang Yue, and Wenhu Chen. 2024. Long-
context LLMs Struggle with Long In-context Learning. arXiv:2404.02060 [cs.CL]
https://arxiv.org/abs/2404.02060
[7] Zhuowan Li, Cheng Li, Mingyang Zhang, Qiaozhu Mei, and Michael Ben-
dersky. 2024.
Retrieval Augmented Generation or Long-Context LLMs?
A Comprehensive Study and Hybrid Approach. In Proceedings of the 2024
Conference on Empirical Methods in Natural Language Processing: Industry
Track. Association for Computational Linguistics, Miami, Florida, US, 881–893.
https://doi.org/10.18653/v1/2024.emnlp-industry.66
[8] Songshuo Lu, Hua Wang, Yutian Rong, Zhi Chen, and Yaohua Tang.
2024.
TurboRAG: Accelerating Retrieval-Augmented Generation with Pre-
computed KV Caches for Chunked Text.
arXiv:2410.07590 [cs.CV]
https://arxiv.org/abs/2410.07590
[9] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Brad-
bury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and JeﬀDean. 2023. Ef-
ﬁciently scaling transformer inference. Proceedings of Machine Learning and
Systems 5 (2023), 606–624.
[10] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016.
SQuAD: 100,000+ Questions for Machine Comprehension of Text. In Proceed-
ings of the 2016 Conference on Empirical Methods in Natural Language Processing,
Jian Su, Kevin Duh, and Xavier Carreras (Eds.). Association for Computational
Linguistics, Austin, Texas, 2383–2392. https://doi.org/10.18653/v1/D16-1264
[11] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Rus-
lan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A Dataset for
Diverse, Explainable Multi-hop Question Answering. In Conference on Empirical
Methods in Natural Language Processing (EMNLP).
[12] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi.
[n. d.]. BERTScore: Evaluating Text Generation with BERT. In International Con-
ference on Learning Representations.
4https://www.youtube.com/watch?v=NaEf_uiFX6o
This figure "acm-jdslogo.png" is available in "png"
format from:
http://arxiv.org/ps/2412.15605v2
This figure "sample-franklin.png" is available in "png"
format from:
http://arxiv.org/ps/2412.15605v2
