arXiv:2404.19543v2  [cs.CL]  29 Jun 2025
RAG and RAU: A Survey on Retrieval-Augmented Language Model in
Natural Language Processing
Yucheng Hu*1 and Yuxing Lu†2
1East China University of Science and Technology , huyc@mail.ecust.edu.cn
2Peking University , yxlu0613@gmail.com
Abstract
Large Language Models (LLMs) have cat-
alyzed significant advancements in Natural
Language Processing (NLP), yet they en-
counter challenges such as hallucination and
the need for domain-specific knowledge. To
mitigate these, recent methodologies have in-
tegrated information retrieved from external
resources with LLMs, substantially enhanc-
ing their performance across NLP tasks. This
survey paper addresses the absence of a com-
prehensive overview on Retrieval-Augmented
Language Models (RALMs), both Retrieval-
Augmented Generation (RAG) and Retrieval-
Augmented Understanding (RAU), providing
an in-depth examination of their paradigm, evo-
lution, taxonomy, and applications. The paper
discusses the essential components of RALMs,
including Retrievers, Language Models, and
Augmentations, and how their interactions lead
to diverse model structures and applications.
RALMs demonstrate utility in a spectrum of
tasks, from translation and dialogue systems to
knowledge-intensive applications. The survey
includes several evaluation methods of RALMs,
emphasizing the importance of robustness, ac-
curacy, and relevance in their assessment. It
also acknowledges the limitations of RALMs,
particularly in retrieval quality and computa-
tional efficiency, offering directions for future
research. In conclusion, this survey aims to
offer a structured insight into RALMs, their po-
tential, and the avenues for their future develop-
ment in NLP. The paper is supplemented with
a Github Repository containing the surveyed
works and resources for further study: https:
//github.com/2471023025/RALM_Survey.
1
Introduction
Natural Language Processing (NLP) is a signifi-
cant focus within the realms of computer science
and artificial intelligence, dedicated to the study of
*Equal contribution.
†Equal contribution. Corresponding author.
theoretical and methodological frameworks that en-
able effective communication between humans and
computers using natural language. As a multidisci-
plinary field, NLP integrates linguistics, computer
science, and mathematics with the aim of realiz-
ing the mutual transformation between human lan-
guage and computer data. Its ultimate objective is
to empower computers with the capability to pro-
cess and "understand" natural language, thereby
facilitating tasks such as automatic translation, text
categorization, and sentiment analysis. The com-
plexity of NLP is evident in the numerous steps it
encompasses, including word segmentation, part-
of-speech tagging, parsing, stemming, named en-
tity recognition, and more, all of which contribute
to the challenge of replicating human language un-
derstanding in artificial intelligence systems.
Traditional natural language processing tasks
typically employ statistic-based algorithms (Hogen-
boom et al., 2010) (Serra et al., 2013) (Aussenac-
Gilles and Sörgel, 2005) and deep learning algo-
rithms such as convolutional neural network (CNN)
(Yin et al., 2017), recurrent neural network (RNN)
(Banerjee et al., 2019), long short-term memory
network (LSTM) (Yao and Guan, 2018), and oth-
ers. Recently, with the advent of the transformer
architecture (Vaswani et al., 2017) as a leading
representative of natural language processing, its
popularity has grown significantly. The transformer
architecture, as a prominent large language model
(Lewis et al., 2019) (Raffel et al., 2020) in the natu-
ral language processing domain, has consistently
demonstrated enhanced performance, attracting the
attention of an increasing number of researchers
who are engaged in studying its capabilities.
The most prevalent LMs nowadays are the GPT
families (Radford et al., 2019) (Brown et al., 2020)
(Achiam et al., 2023) and Bert families (Liu et al.,
2019) (Devlin et al., 2018) (Sanh et al., 2019),
which have been demonstrated to excel in a multi-
tude of natural language processing tasks. Among
1
Figure 1: A general overview of this survey‘s work
these, the AutoEncoder language model is particu-
larly adept at natural language understanding tasks,
while the AutoRegressive language model is more
suited to natural language generation tasks. While
increasing parameters (Touvron et al., 2023b) and
model tuning (Han et al., 2023) can enhance the per-
formance of LLMs, the phenomenon of "hallucina-
tion" (Ji et al., 2023) persists. Furthermore, the lim-
itations of LMs in effectively handling knowledge-
intensive work (Feng et al., 2023) and their inability
to promptly update their knowledge (Mousavi et al.,
2024) are consistently apparent. Consequently, nu-
merous researchers (Lewis et al., 2020) (Izacard
and Grave, 2020b) (Khandelwal et al., 2019) have
employed the technique of retrieval to obtain ex-
ternal knowledge, which can assist the language
model in attaining enhanced performance in a mul-
titude of tasks.
Currently, there is a paucity of surveys on the
use of retrieval augmentation to enhance the per-
formance of LLMs. Zhao et al. (2023) provide
a comprehensive overview of work on RAG for
multimodality. Zhao et al. (2024a) concentrate on
the utilisation of retrieval augmentation generation
techniques for the Artificial Intelligence Generated
Content (AIGC) domain. This article provides a
comprehensive overview of recent RAG work, but
it does not cover all relevant domains. Addition-
ally, the article lacks sufficient detail to provide a
comprehensive timeline of the overall development.
Gao et al. (2023) investigate the enhancement of
RAG for large models. This article summarizes
some of the recent RAG work, but it introduces
the retrievers and generators independently, which
is not conducive to the upgrading and interactions
with the components of subsequent work. Li et al.
2
(2022b) focus on text generation only. The arti-
cle has fewer figures and tables, and the content
is more abstract, which is not conducive to the
reader’s understanding.
Also, surveys on RAG only tells half of the
story in retrieval-augmented methods in NLP. Not
only do tasks associated with NLG require retrieval
enhancement techniques, but NLU tasks also ne-
cessitate external information. To date, there is a
scarcity of comprehensive surveys that thoroughly
review the application of augmented retrieval tech-
niques across the spectrum of NLP. In order to
improve the current situation, this paper presents
the following contributions:
(1) The article does not merely focus on the work
related to RAG; it also places significant emphasis
on RALM and aligns with the concept of NLP. The
work related to generation aligns with NLG, while
the rest of the work aligns with NLU.
(2) The two components of RALM, the Retriever
and the Language Model, are described in detail,
and the different interaction modes of these two
components are precisely defined for the first time.
(3) A comprehensive overview of the RALM
work schedule is provided, along with a summary
of the common and novel applications of current
RALM, accompanied by an analysis of the associ-
ated limitations. Potential solutions to these limi-
tations are proposed, along with recommendations
for future research directions.
Figure 1 provides a general overview of the
framework of RALM methods. The following is a
summary of the paper: Section 2 defines RALM.
Section 3 provides a detailed classification and sum-
mary of the work of retrievers in RALM. Section
4 provides a detailed classification and summary
of the work of LMs in RALM. Section 5 provides
a classification and summary of specific enhance-
ments to RALM. Section 6 of RALM is a classifica-
tion and summary of the sources of retrieved data.
Section 7 is a summary of RALM applications.
Section 8 is a summary of RALM evaluations and
benchmarks. Finally, Section 9 is a discussion of
the limitations of existing RALM and directions
for future work.
2
Definition
Retrieval-Augmented Language Model (RALM) is
the process of refining the output of the LM with
retrieved information to obtain a satisfactory re-
sult for the user. This section provides a detailed
definition of the different modes of RALM by cate-
gorising the ways in which the retriever interacts
with the language model. The specific categoriza-
tion of interactions can be seen in Figure 2. In
addition, the development history of each interac-
tion method can be seen in Figure 3. Assuming
that z is the retrieved message, x is the input, y is
the output, and F() is a function, either a language
model or a data processing function, with x and z
as independent variables, the basic architecture of
RALM is defined as follows:
y = F(x, z)
(1)
2.1
Sequential Single Interaction
The sequential single interaction process involves
finding the Top-K relevant documents z to input x
through a retriever Pη(z|x), where η is a parameter
of the retriever. Subsequently, the language model
Pθ(yi|x, z, yr) receives input x along with relevant
documents z and outputs the i-th token yi. Param-
eter θ is used, along with relevant output tokens
yr. The number of relevant output tokens is related
to the location and type of language model. The
RALM for sequential single interaction is defined
as follows:
yi = LM(z, x, yr)
(2)
When RALM was first proposed, many researchers
used this method because it aligned with their orig-
inal ideas, particularly those of Lewis et al. (2020),
Guu et al. (2020), and Izacard and Grave (2020b).
2.2
Sequential Multiple Interactions
As RALM technology develops, researchers have
discovered that a single interaction is insufficient
for long dialogue generation and solving multi-
hop problems. Therefore, a method with multiple
interactions between a retriever and a language
model has been proposed. In this method, the re-
searcher includes step s and typically has the lan-
guage model generate the output first. When a
retrieval technique is necessary, the outputted con-
tent is used for retrieval and the relevant formulas
are expressed as follows:
ys = LM(z, x|y<s)
(3)
where ys represents the generated tokens at the
current step s. Among the researchers who have
employed this method, the most renowned are Jiang
et al. (2023c), He et al. (2022), and Khattab et al.
(2022).
3
Figure 2: Three different ways the Retriever interacts with the LM
2.3
Parallel Interaction
In all of the previously mentioned approaches, the
flow of information has a clear sequential structure,
whether from the retriever to the language model or
from the language model to the retriever. However,
this sequential structure may not be optimal in all
domains and may be less extensible, it is important
to consider alternative approaches. Researchers
have proposed a novel parallel structure in which
the retriever and the language model work indepen-
dently for the user input x. The output y is then
determined by weighted interpolation. I() is the
interpolation function. The relevant equations are
expressed as follows:
y = I(LM(x, yr), z)
(4)
the specific interpolation function is:
p(y|x) = λpR(y|x) + (1 −λ)pLM(y|x)
(5)
where the retrieved output tokens are denoted by
pR(y|x), the language model generated output to-
kens are denoted by pLM(y|x), and the weights
are denoted by λ. Among the researchers who
have employed this method, the most renowned are
Khandelwal et al. (2019), Alon et al. (2022), and
He et al. (2021).
3
Retriever
Retrievers play a crucial role in the RALM architec-
ture. The information obtained through retrievers
can significantly improve the accuracy of the LM.
This section provides a summary of the retrieval
methods commonly used in the RALM architecture.
The retrieval methods are classified into four cate-
gories based on their methods and sources: Sparse
Retrieval, Dense Retrieval, Internet Retrieval, and
Hybrid Retrieval. Table 1 lists information about
specific applications of retrievers in the RALM.
3.1
Sparse Retriever
For a period of time following the proposal of the
retrieval technique, sparse retrieval proves to be a
straightforward and effective tool in solving prob-
lems, particularly those based on knowledge. One
of the main advantages of sparse retrieval is its sim-
plicity, which can be easily integrated into existing
indexing systems due to the fewer dimensions in-
volved. (Hambarde and Proenca, 2023)This is con-
sistent with human cognitive processes. Addition-
ally, sparse retrieval is easier to generalise and more
efficient. Sparse retrieval used in RALM can be
classified into two categories: Word Frequency and
4
Figure 3: A roadmap of the three types of interactions. The purple areas represent work on Sequential Interaction
RALM models, the red boxes signify work on Sequential Multiple Interactions RALMs models, and the yellow
areas indicate work on Parallel Interaction RALM models.
Sparse Vector Representation. The choice between
the two depends on whether machine learning is
used.
3.1.1
Word Frequency
In the initial stage, individuals often use methods
for retrieval that involve matching of relevant con-
tent, such as the TF-IDF (Ramos et al., 2003) and
BM25 (Robertson et al., 1995) algorithms, which
are considered classic and effective.
The TF-IDF algorithm utilises term frequency
(TF) and inverse document frequency (IDF) to rep-
resent relevance, which has the advantages of sim-
plicity and speed, and even if the corpus is un-
changed, the TF-IDF value for each word can be
computed in advance. In RALM, Lazaridou et al.
(2022) utilise the TF-IDF algorithm to match in-
formation obtained from user queries and calls to
the Google search API. Hua and Wang (2018) also
employ the algorithm to score the generated results.
The BM25 represents an enhancement over the TF-
IDF. It considers the user’s query and calculates
the relevance score as the weighted sum of the rel-
evance of each query word to the document. The
IDF algorithm is used to derive the weight of each
word, but it is improved by two moderating factors
to prevent the strength of the influence of a certain
factor from being infinite. This is consistent with
5
Table 1: Summary of Retrievers in RALM works.
Category
Technique
Year
Reference
2023
(Jiang et al., 2023c)
2023
(Ram et al., 2023)
2022
(Zhong et al., 2022)
2024
(Cheng et al., 2024)
2020
(Izacard and Grave, 2020b)
2022
(Mallen et al., 2022)
2024
(Schick et al., 2024)
2023
(Xu et al., 2023)
2022
(He et al., 2022)
Word Frequency
BM25(Robertson et al., 1995)
2023
(Yu et al., 2023)
2022
(Alon et al., 2022)
2022
(Borgeaud et al., 2022)
KNN search
2019
(Khandelwal et al., 2019)
GUD-IR(Madaan et al., 2022)
2022
(Madaan et al., 2022)
GAR(Mao et al., 2020)
2020
(Mao et al., 2020)
Sparse Retrieval
Sparse Vector Representation
Spider(Ram et al., 2021)
2023
(Ram et al., 2023)
2022
(Izacard et al., 2022)
2021
(Thulke et al., 2021)
COLBERTV2(Santhanam et al., 2021)
2023
(Hofstätter et al., 2023)
2023
(Siriwardhana et al., 2023)
2021
(Sachan et al., 2021)
Contriever(Izacard et al., 2021)
2020
(Guu et al., 2020)
DBE(Izacard and Grave, 2020a)
2023
(Ram et al., 2023)
DKR(Thulke et al., 2021)
2023
(Wang et al., 2023c)
2020
(Lewis et al., 2020)
2020
(Izacard and Grave, 2020b)
2020
(Karpukhin et al., 2020)
2022
(Mallen et al., 2022)
DPR(Karpukhin et al., 2020)
2020
(Izacard and Grave, 2020a)
GTR(Ni et al., 2021)
2019
(Lee et al., 2019)
E2E-NR(Sachan et al., 2021)
2021
(He et al., 2021)
REALM(Guu et al., 2020)
2023
(Yoran et al., 2023)
ORQA(Lee et al., 2019)
2022
(Khattab et al., 2022)
Word Embedding
EMDR2(Singh et al., 2021)
2021
(Santhanam et al., 2021)
MuRAG(Chen et al., 2022b)
2022
(Chen et al., 2022b)
RA-CM3(Yasunaga et al., 2022)
2022
(Yasunaga et al., 2022)
RE-IMAGEN(Chen et al., 2022c)
2022
(Chen et al., 2022c)
MDTIG(Li et al., 2022a)
2022
(Li et al., 2022a)
Multimodal Retrieval
RDM(Blattmann et al., 2022)
2022
(Blattmann et al., 2022)
DRAGON(Lin et al., 2023a)
2023
(Lin et al., 2023b)
Dense Retrieval
Knowledge Distillation
REPULG LSR(Shi et al., 2023b)
2023
(Shi et al., 2023b)
FLARE(Jiang et al., 2023c)
2023
(Jiang et al., 2023c)
RAG-Robust(Yoran et al., 2023)
2023
(Yoran et al., 2023)
IADG(Komeili et al., 2021)
2021
(Komeili et al., 2021)
Internet Retrieval
Webgpt(Nakano et al., 2021)
2021
(Nakano et al., 2021)
DuckDuckGo+BM25(Luo et al., 2023a)
2023
(Luo et al., 2023a)
Internet+TF-IDF(Lazaridou et al., 2022)
2022
(Lazaridou et al., 2022)
REVEAL (Hu et al., 2023)
2023
(Hu et al., 2023)
NAG-ERE(Hua and Wang, 2018)
2018
(Hua and Wang, 2018)
Internet+BM25(Adolphs et al., 2021)
2021
(Adolphs et al., 2021)
Hybrid Retrieval
kNN+BM25+translation model(Boytsov et al., 2016)
2016
(Boytsov et al., 2016)
common sense. Due to its excellent generalisation
capabilities, many Retrieval-Augmented Language
Model (RaLM) architectures, particularly those ori-
ented towards open domains, employ BM25 as a
retrieval method, such as Jiang et al. (2023c), Ram
et al. (2023) and Zhong et al. (2022).
3.1.2
Sparse Vector Representation
It has become evident that simple term matching is
no longer sufficient to meet the demand. Manual
labelling can solve problems such as the synonym
issue, but it is a resource-intensive method. With
the rise of machine learning, sparse vectors are
now used to represent words and retrieve them by
calculating the distance between them. (Hambarde
and Proenca, 2023) Sparse vector representation
techniques differ from term matching methods in
that they construct sparse vectors for queries and
documents. The purpose of these representations is
to capture the semantic essence of each input text,
which places queries and documents in a latent
space.
Ram et al. (2021) utilised the fact that when
given two paragraphs with the same repeat span,
one was used to construct a query and the other
as the retrieval target. The remaining paragraph
in the document, which did not contain a repeat
span, was used as a negative example and Ram
6
et al. (2023) applied this retriever for the first time
to the RALM architecture. On the other hand, both
Mao et al. (2020) and Madaan et al. (2022) pro-
posed using language models (LM) to enhance re-
trieval accuracy of sparse vector representation by
generating new queries from a given question and
using them to retrieve relevant documents. How-
ever, Mao’s approach emphasizes query expansion,
while Maddan’s approach emphasizes understand-
ing the user’s input.
3.2
Dense Retriever
The emergence of deep learning techniques has sig-
nificantly transformed the field of retrieval. There
is a growing interest in using deep learning tech-
niques to enhance retrieval accuracy, even if it
means sacrificing some level of comprehensibil-
ity. The dual encoder architecture is a common
design for dense retrieval models. (Hambarde and
Proenca, 2023) The system comprises of two dis-
tinct networks that receive separate inputs, namely
queries and documents, and independently gener-
ate dense embeddings for each input. Due to its
high accuracy and dual-encoder structure, which is
more suitable for RALM, most articles choose to
use the dense indexing method to build their retriev-
ers. This section classifies dense retrieval into three
types: Word Embedding, Multimodal Retrieval,
and Data Distillation, based on the characteristics
of each retrieval method.
3.2.1
Word Embedding
Word embeddings are a common approach in nat-
ural language processing. Similar to sparse vector
representations, they use deep learning techniques
to project words into a higher-dimensional vector
space. Several articles in the RALM architecture
utilize this technique, and we have selected repre-
sentative ones to describe.
Karpukhin et al. (2020) proposed the DPR re-
trieval model, which indexes all passages in a low-
dimensional and continuous space. This allows
the reader to efficiently retrieve the first k passages
associated with the input problem at runtime. A
dense encoder is used to map any text passage to
a d-dimensional real-valued vector, creating an in-
dex for all M passages used for retrieval. Due to
its excellent performance as a retriever in RALM
architectures, DPR has been widely adopted by re-
searchers such as Lewis et al. (2020), Izacard and
Grave (2020b), and Karpukhin et al. (2020). Izac-
ard and Grave (2020a) takes a similar tactic, unlike
DPR, in that he uses the same encoding function
for questions and paragraphs through shared param-
eters. In order to further minimise the intervention
and reduce the cost of manual annotation, Izacard
et al. (2021) proposed another retriever called Con-
triever, which was trained using unsupervised data.
It is based on successive dense embeddings and has
a dual-encoder architecture. Average pooling was
applied on the output of the previous layer to obtain
one vector representation for each query or docu-
ment. The similarity score between the query and
each document was obtained by computing the dot
product between their corresponding embeddings.
Researchers (Siriwardhana et al., 2023), (Sachan
et al., 2021), and (Guu et al., 2020) have used it
as a retriever in the RALM architecture due to its
ability to utilize unsupervised data.
3.2.2
Multimodal Retrieval
Retrieval techniques for multimodal tasks are more
complex than those for text-only tasks (Zhao et al.,
2023).
This is because they involve the inter-
transformation of information about different states.
For example, in the image-text domain, multimodal
techniques for dense text retrieval have attracted
interest as a means of bridging the gap between
different modalities (Zhao et al., 2024b,c). Re-
searchers have developed methods to encode tex-
tual and visual information into a shared latent
space for retrieval tasks.
Li et al. (2022a) designed four matching al-
gorithms for handling multimodal tasks, namely
sentence-to-sentence, sentence-to-image, word-to-
word and word-to-image. They used reweighting
for more accurate correlation calculations and co-
sine similarity scores as a criterion to explore the
effectiveness of each algorithm. Unlike the former,
Yasunaga et al. (2022) utilised a straightforward
extension of CLIP (Ramesh et al., 2022) to divide a
multimodal document into text and image compo-
nents, which were encoded with frozen encoders.
The L2 norm was scaled to 1 using an average pool-
ing technique, resulting in a vector representation
of the document. Similarly to Li et al. (2022a), they
also employed a Maximum Inner Product Search
(MIPS) as relevance scores, ultimately selecting K
documents.
3.2.3
Knowledge Distillation
Knowledge distillation is a technique for gradually
filtering and streamlining a large database to make
it more suitable for a user’s query. (Gou et al.,
7
2021) involves transferring data from a pre-trained,
larger model to a smaller one, often using methods
such as embedded matching. Research has even
been conducted on data distillation using LMs as
the technology has evolved.
Shi et al. (2023b) utilises knowledge distilla-
tion to divide the retrieval process into four dis-
tinct steps. Firstly, documents are retrieved and
retrieval likelihoods are computed. Secondly, the
retrieved documents are scored using a language
model. Thirdly, the parameters of the retrieval
model are updated by minimising the KL discrep-
ancy between the retrieval likelihoods and the dis-
tribution of the language model scores. Finally,
asynchronously updating of the indexes of the data
is performed. Based on this technique, Lin et al.
(2023a) further improve the accuracy of knowledge
distillation. They present a data distillation scheme
that combines sentence truncation and query en-
hancement with incremental relevance label en-
hancement using multiple enhancers.
3.3
Internet Retrieval
With the advancement of Internet search and sort-
ing technology, some researchers have focused
their search efforts on Internet retrieval, which is
a plug-and-play approach. This approach allows
non-specialists to benefit from RALM and is bet-
ter suited to the open domain and generalisation.
Another advantage of this retrieval model is that it
does not require real-time updating of the database,
but relies on updates from commercial search en-
gines. However, despite the advantages of simplic-
ity and convenience, there is a significant amount
of irrelevant and even harmful information on the
Internet that can hinder the work of RALM. If an
effective screening mechanism is not implemented,
the effectiveness of RALM will be significantly
reduced.
Unlike most studies (Yoran et al., 2023) (Nakano
et al., 2021) that directly utilise commercial search
engine APIs. Komeili et al. (2021) propose an al-
ternative approach to using multiple commercial
search engine APIs. They suggest using the Bing
Search API to generate a list of URLs for each
query. These URLs are then used as keys to look
up their page content in a lookup table constructed
from public crawl snapshots, which populates a
set of pages for that query. In addition, the evalu-
ation takes into account whether the URL is from
the English Wikipedia. If so, the page title is ex-
tracted from the URL and the corresponding page
is searched for in the Wikipedia dump.
3.4
Hybrid Retrieval
As researchers gain a better understanding of the
strengths and weaknesses of various retrieval tech-
niques, they are increasingly opting to combine
them, as described above. This is done in the hope
of further exploiting the advantages of these tech-
niques to improve the effectiveness and robustness
of the RALM architecture.
To tackle the issue of inaccurate Internet retrieval
results, Lazaridou et al. (2022) proposed using
the TF-IDF algorithm to score the retrieval results.
They used each question q verbatim as a query
and issued a call to Google Search via the Google
Search API. For each question, they retrieved the
top 20 URLs and parsed their HTML content to
extract clean text, generating a set of documents
D for each question q. To prevent irrelevant infor-
mation from hindering the resolution of a user’s
query, Hu et al. (2023) designed a gating circuit.
This circuit utilised a dual-encoder dot product to
calculate similarity and a gating circuit based on
term weights. Additionally, Boytsov et al. (2016)
presented an approach that replaced term-based
retrieval with k-Nearest Neighbors(kNN) search
while combining a translation model and BM25
to improve retrieval performance. This approach
enabled the model to take into account the semantic
relationships between terms and traditional statisti-
cal weighting schemes, resulting in a more efficient
retrieval system.
4
Language Models
Although humans used to rely solely on search-
ing for information, the development of language
models has revolutionised the field of natural lan-
guage processing, making it more vibrant and cre-
ative. In contrast to LM, which employs solely
the parameters derived from training to complete
the task, RALM integrates the nonparametric mem-
ory acquired by the retriever with the parametric
memory of LM itself to create a semiparametric
memory, thereby enhancing the performance of the
language model. In the RALM architecture, many
researchers utilise off-the-shelf language models
for evaluation. This section introduces the language
models commonly used in RALM architectures and
classifies them into three categories: AutoEncoder-
language model, AutoRegressive language model
and Encoder-Decoder model. Table 2 lists infor-
8
Table 2: Summary of LMs in RALM methods.
Category
Technique
Year
Reference
AutoEncoder Language Model
RoBERTa(Devlin et al., 2018)
2021
(Thulke et al., 2021)
2024
(Cheng et al., 2024)
BERT(Liu et al., 2019)
2021
(Sachan et al., 2021)
AutoRegressive Language Model
GPT Family
GPT-3.5
2023
(Jiang et al., 2023c)
2022
(Khattab et al., 2022)
GPT-2(Radford et al., 2019)
2023
(Ram et al., 2023)
GPT-Neo(Black et al., 2022)
2022
(Mallen et al., 2022)
chatGPT
2023
(Peng et al., 2023)
GPT-4(Achiam et al., 2023)
2023
(Asai et al., 2023)
2023
(Luo et al., 2023a)
GPT-3(Brown et al., 2020)
2022
(Madaan et al., 2022)
2022
(He et al., 2022)
GPT(Radford et al., 2018)
2023
(Shi et al., 2023b)
GPT-J
2024
(Schick et al., 2024)
2024
(Hu et al., 2024)
Llama Family
Llama2(Touvron et al., 2023b)
2024
(Yan et al., 2024)
2023
(Asai et al., 2023)
2023
(Wang et al., 2023c)
2023
(Yoran et al., 2023)
Llama(Touvron et al., 2023a)
2023
(Lin et al., 2023b)
2023
(Luo et al., 2023a)
Others
Alpaca(Dubois et al., 2024)
2024
(Yan et al., 2024)
OPT(Zhang et al., 2022)
2023
(Ram et al., 2023)
2023
(Lin et al., 2023b)
XGLM(Lin et al., 2022)
2024
(Cheng et al., 2024)
BLOOM(Workshop et al., 2022)
2023
(Shi et al., 2023b)
Mistral(Jiang et al., 2023a)
2024
(Hu et al., 2024)
Encoder-Decoder Language Model
T5(Raffel et al., 2020)
2022
(Izacard et al., 2022)
2023
(Hofstätter et al., 2023)
2021
(Sachan et al., 2021)
2023
(Wang et al., 2023c)
2022
(Chen et al., 2022b)
2021
(Singh et al., 2021)
2020
(Izacard and Grave, 2020a)
2022
(Lazaridou et al., 2022)
2023
(Hu et al., 2023)
BART(Lewis et al., 2019)
2021
(Thulke et al., 2021)
2023
(Siriwardhana et al., 2023)
2019
(Lewis et al., 2019)
2020
(Lewis et al., 2020)
2023
(Yoran et al., 2023)
2020
(Izacard and Grave, 2020a)
2022
(Lazaridou et al., 2022)
mation about specific applications of LM in the
RALM.
4.1
AutoEncoder Language Model
The logical process of an AutoEncoder is that the
original input (set to x) is weighted and mapped
to y, which is then inversely weighted and mapped
back to z. If through iterative training the loss
function L(H) is minimised, i.e. z is as close to
x as possible, i.e.
x is perfectly reconstructed,
then it can be said that forward weighting is a suc-
cessful way of learning the key features of the in-
put. AutoEncoder language models take their name
from the Denoising AutoEncoder (DAE) (Vincent
et al., 2008), which is used to predict tokens that
are [masked] by contextual words (these [masked]
words are actually noise added at the input, typi-
cal of thinking). DAE is a technique that involves
adding random noise to the input layer of data. This
helps to learn more robust features when using an
unsupervised approach to pre-train the weights of
a deep network in a hierarchical manner.
Most of AutoEncoder language models are
highly generalisable, unsupervised, and do not re-
quire data annotation. They can naturally incor-
porate contextual semantic information. However,
the independence assumption introduced in the Pre-
Training stage means that the correlation between
9
predicted [MASK] is not considered. Additionally,
the introduction of [Mask] as a special marker in
the input to replace the original Token creates in-
consistency between the data in the Pre-Training
stage and the Fine-Tuning stage, where [Mask] is
not present. Self-encoding language models are
commonly used in RALM architectures for natural
language understanding (NLU) tasks.
As AutoEncoder language models excel at Nat-
ural Language Understanding (NLU) tasks, many
RALM architectures(Thulke et al., 2021) (Cheng
et al., 2024) (Sachan et al., 2021)utilise them for
specific tasks, such as judgement. One of the most
commonly used models is BERT and its improved
versions.Devlin et al. (2018) proposed the BERT
model, which was inspired by closed tasks(Taylor,
1953). RoBERTa(Liu et al., 2019) is trained us-
ing dynamic masking, full sentences without NSP
loss, large mini-batches, and a larger byte-level
BPE to address the lack of training of Bert’s model.
According to Jiang et al. (2020), BERT heavily
relies on global self-attention blocks, resulting in
a large memory footprint and computational cost.
Although all attention heads query the entire in-
put sequence, some only need to learn local de-
pendencies, leading to computational redundancy.
To address this issue, They proposed a new span-
based dynamic convolution to replace these self-
attention heads and directly model local dependen-
cies. The new convolutional head, along with other
self-attentive heads, forms a hybrid attention block.
Furthermore, Sanh et al. (2019) was able to de-
crease the size of the BERT model by 40% while
maintaining 97% of its language comprehension
abilities and achieving a 60% increase in speed
by implementing knowledge distillation during the
pre-training phase.
4.2
AutoRegressive Language Model
The primary purpose of an AutoRegressive lan-
guage model is to predict the next word based on
the preceding words. This is commonly known as
left-to-right language modelling, where the token
at the current time t is predicted based on the first
t-1 tokens.
This model has the advantage of being left-to-
right, which is beneficial for generative natural
language processing tasks like dialog generation
and machine translation. AutoRegressive language
models are well-suited to this process, making this
model a popular choice for NLG tasks in the field
of RALM. However, The information in question
can be utilized only from the preceding or follow-
ing text, and not in combination with both. OpenAI
has made a notable impact on the field of research
pertaining to autoregressive language models. Re-
cently, Google has also made advancements in re-
search on the model.
The GPT family is one of the most common
examples of AutoRegressive language models. It
was first proposed by Radford et al. (2018), who
identified a basic architecture of unsupervised pre-
training followed by fine-tuning. Radford et al.
(2019) later proposed zero-shot learning based on
GPT. Later, Brown et al. (2020) proposed GPT-3
using an approach similar to Radford et al. (2019),
which involved scaling up and abandoning fine-
tuning. They also utilized alternately dense and
locally banded sparse attentional patterns in the
transformer layer, similar to the sparse transformer
(Child et al., 2019). There are also several related
studies, such as GPT-NEO(Black et al., 2022) and
ChatGPT, which use Reinforcement Learning from
Human Feedback (RLHF). RLHF has significantly
enhanced the accuracy of GPT models. Although
ChatGPT is not open source, many researchers
(Jiang et al., 2023c) (Khattab et al., 2022) still use
its API for generative tasks in RALM. Recently, re-
port on GPT-4(Achiam et al., 2023) have appeared,
with the main focus on building a predictable and
scalable deep learning stack dedicated to improving
GPT-4’s safety and alignment. Many researchers
(Asai et al., 2023) (Luo et al., 2023a) have recently
used GPT-4 to generate prompts for RALM.
The Llama family is a well-known class of Au-
toRegressive language models. Llama(Touvron
et al., 2023a) was first proposed as a language
model that uses only publicly available data. To im-
prove training stability, they normalise the input of
each transformer sub-layer instead of normalising
the output. They use the RMSNorm normalisa-
tion function introduced by Zhang and Sennrich
(2019) and replace the ReLU non-linearity with the
SwiGLU activation function introduced by Shazeer
(2020) to improve performance. Furthermore, the
authors replaced the absolute position embedding
with the rotational position embedding (RoPE) in-
troduced by Su et al. (2024) at each layer of the
network. Llama2(Touvron et al., 2023b) used su-
pervised fine-tuning, initial and iterative reward
modelling and RLHF in their experiments. they
also invented a new technique, Ghost Attention
(GAtt), which helps to control the flow of dialogue
in multiple turns. Qwen(Bai et al., 2023), based
10
on Llama, the following adjustments were made:
1. The method of loose embedding was chosen
instead of bundling the weights of input embed-
ding and output projection to save memory cost. 2.
The accuracy of the inverse frequency matrix was
improved. 3. For most of the layers, bias was elim-
inated as per Chowdhery et al. (2023). However,
bias was added in the QKV layer to enhance the
model’s extrapolation capability. The traditional
layer normalization technique described in Ba et al.
(2016) was replaced with RMSNorm. They chose
SwiGLU as their activation function, which is a
combination of Swish Ramachandran et al. (2017)
and gated linear units (Dauphin et al., 2017). The
dimension of the feed-forward network (FFN) is
also reduced. Furthermore, Mistral 7b (Jiang et al.,
2023a) utilises Grouped Query Attention (GQA) to
enhance inference speed and combines it with Slid-
ing Window Attention (SWA) to efficiently process
sequences of any length with reduced inference
cost. These techniques demonstrate superior per-
formance over Llama2.The Llama model is open
source and uses publicly available data, providing
researchers (Yan et al., 2024) (Asai et al., 2023)
(Wang et al., 2023c) with more opportunities to ex-
pand. As a result, many researchers use the Llama
family as language models in the RALM architec-
ture.
4.3
Encoder-Decoder Language Model
Transformer(Vaswani et al., 2017) is an "encoder-
decoder" architecture, which consists of encoders
and decoders superimposed on multi-head self-
attention modules.
Among them, the input se-
quence is divided into two parts, the source se-
quence and the destination sequence. The former
is input to the encoder and the latter is input to
the decoder, and both sequences need to embed
representation and add position information. The
Transformer architecture enables parallel compu-
tation and the processing of entire text sequences
simultaneously, resulting in a significant increase
in model training and inference speed.
Raffel et al. (2020) introduces a unified frame-
work for converting all text-based language prob-
lems into text-to-text format. The aim is to explore
the potential of transfer learning techniques for
natural language processing. In contrast to the orig-
inal transformer, a simplified version of layer nor-
malization is used, where activations are rescaled
without additional biases. After applying layer nor-
malization, a residual skip connection (He et al.,
2016) adds the input of each subcomponent to its
output. Srivastava et al. (2014) is applied to the
feed-forward network, the skip connections, the at-
tentional weights, and the inputs and outputs of the
entire stack. The T5 model has been widely used
as a language model by many researchers, such
as Hofstätter et al. (2023), Sachan et al. (2021),
and Singh et al. (2021). Additionally, Chung et al.
(2022) proposed instruction tuning as an approach
to improve model performance. The study focused
on three aspects: the number of scaling tasks, the
size of the scaled model, and the fine-tuning of
chain of thought data. The results showed that
larger model sizes and more fine-tuning tasks sig-
nificantly improved model performance. Addition-
ally, the study found that chain of thought(CoT)
significantly improves inference level. Wang et al.
(2023c) used this approach to tune T5 and apply it
to the RALM architecture.
BART(Lewis et al., 2019) is an Encoder-Decoder
model that allows for arbitrary noise transforma-
tions, as the input to the encoder does not need to
align with the output of the decoder. In this case,
the document is corrupted by replacing the text
span with mask symbols. For pre-training, the re-
searchers proposed five models: Token Masking,
Token Deletion, Text Infilling, Sentence Permu-
tation, and Document Rotation. For fine-tuning,
the encoder and decoder are fed an uncorrupted
document, and the representation of the final hid-
den state from the decoder is used.
Many re-
searchers(Thulke et al., 2021) (Siriwardhana et al.,
2023) (Lewis et al., 2019) have adopted BART
as the language model in the RALM architecture
due to its comprehensive and novel pre-training
approach, which greatly enhances the model’s ro-
bustness.
5
RALM Enhancement
This section describes how researchers in the
RALM architecture improved the output quality
by enhancing its components.
We divided the
improvement method into three parts: Retriever
Enhancement, LM Enhancement, and Overall En-
hancement. Figure 4 illustrates the categorization
of enhancement methods.
5.1
Retriever Enhancement
This section presents the researchers’ efforts on
the retriever side, which include Retrieval Quality
Control and Retrieval Timing Optimization.
11
Figure 4: Classification of RALM enhancement methods.
5.1.1
Retrieval Quality Control
Shi et al. (2023a) argue that retrieval can produce
documents that not only fail to provide helpful in-
formation but can also compromise the quality of
the language model output. As a result, many schol-
ars in the field of RALM focus on improving the
relevance between the retrieved content and the
user’s input to enhance the final output’s quality.
Lin et al. (2023b) propose an approach for
instruction-tuning. They update the query encoder
using a generalised LM supervised retrieval (LSR)
(Shi et al., 2023b)training target that completes
the computation through a combination of super-
vised tasks and unsupervised text. This enables
the retriever to produce more contextually rele-
vant results that are consistent with LLM pref-
erences. Inspired by this instruction-tuning ap-
proach, Asai et al. (2023) proposed a more sophis-
ticated model trained on an instruction-tracking
dataset: SELF-RAG. As a result of their refine-
ments, SELF-RAG can retrieve and select the best
possible model outputs on demand through fine-
grained self-reflection, making it broadly applica-
ble, more robust, and controllable. In contrast to
approaches that aim to enhance the quality of re-
trieved documents through the use of external mod-
els, such as natural language inference (Yoran et al.,
2023) and summarization(Xu et al., 2023) models,
SELF-RAG proposes entirely novel ideas. The
model divides the retrieved document into paral-
lel segments and compares their relevance. It then
combines the most similar parts of the document.
Yan et al. (2024) improves on SELF-RAG by de-
signing a correction strategy to address inaccurate
retriever results. They classify the information into
three categories: CORRECT, INCORRECT, and
AMBIGUOUS. If the information is CORRECT,
the document is refined and filtered. If it is IN-
CORRECT, the document is discarded and the web
is searched for retrieval. The term AMBIGUOUS
indicates a lack of confidence in the accuracy of
a judgement. In this case, a combination of the
two methods mentioned above will be used. Addi-
tionally, Wang et al. (2023c) proposed FILCO, a
method for retrieving document content with sen-
tence precision through three filters: STRINC, lex-
ical overlap, and conditional cross-mutual informa-
tion (CXMI).
5.1.2
Retrieval Timing Optimization
Researchers typically consider the timing of re-
trieval in two situations: when working on tasks
that require multiple retrievals, such as long dia-
logue generation and multi-hop problems, or when
it is impossible to find a suitable and relevant doc-
ument. Using irrelevant documents can harm the
accuracy of the output.
A simple way to determine the timing of a re-
trieval is to adjust the retrieval steps. Ram et al.
(2023) utilised an approach that involved a prefix
encoding to adjust the runtime cost. The prefix
encoding of the generated content was constantly
recalculated. The choice of retrieval stride is a
trade-off between runtime and performance. Ac-
cording to Toolformer (Schick et al., 2024), the
search command can be used directly to retrieve
useful information when the model needs to re-
trieve documentation help in the process of gen-
erating content. Inspired by this idea, Jiang et al.
(2023c) propose two methods for determining the
timing of retrieval. The first method involves in-
terrupting the generation of the LM when it en-
counters a place where a retrieval needs to be per-
formed and then performing the retrieval opera-
tion. The second method involves generating a
temporary sentence in its entirety. If there is a low
12
confidence marker in the sentence, the marker is
masked and the rest of the sentence is used for
retrieval. Yu et al. (2023) also employed LM to
determine the timing of retrieval. However, in-
stead of generating low-confidence markers using
LM, they had LM score the output before and after
retrieval.Mallen et al. (2022)’s approach differed
from the traditional method of having LMs gen-
erate low-confidence markers. Instead, they used
Wikipedia page views as a measure of prevalence
and converted knowledge triples of wiki data with
varying levels of prevalence into natural language
questions anchored to the original entity and rela-
tion types. This approach is more objective and
avoids subjective evaluations. For tasks that re-
quired reasoning, both He et al. (2022) and Khattab
et al. (2022) used chain of thought(CoT) to deter-
mine when to perform a retrieval.
5.2
LM Enhancement
This section presents the researchers’ efforts in
language modelling, including Pre-Generation Re-
trieval Processing, Structural Model Optimization,
and Post-Generation Output Enhancement.
5.2.1
Pre-Generation Retrieval Processing
The RALM architecture initially used a single doc-
ument for retrieval augmentation. However, it was
discovered that RALM’s performance significantly
improved when the number of retrieved paragraphs
was increased. (Izacard and Grave, 2020b) There-
fore, they proposed a new method called Fusion-in-
Decoder (FiD) which involves keeping the retriever
unchanged, using the encoder in LM to encode the
related documents one by one, and then connect-
ing the related documents and giving them to the
decoder for output. Then Hofstätter et al. (2023)
improved on the FiD. They constrained the informa-
tion flow from encoder to decoder. FiD-Light with
reranking was also tuned via text source pointers
to improve the topmost source accuracy. Izacard
and Grave (2020a) applied knowledge distillation
to the FiD model, also known as FiD-KD, using
cross-attention scores from a sequence-to-sequence
reader to obtain synthetic targets for the retriever.
Singh et al. (2021) proposed an enhancement ap-
proach that differs from knowledge distillation in
that it uses an end-to-end training approach requir-
ing fewer documents, training cycles, and no super-
vised initialization compared to FiD-KD.
5.2.2
Structural Model Optimization
As language models continue to evolve at an accel-
erated pace, an increasing number of large models
with high parameter counts and exceptional per-
formance are emerging. Tuning the parameters
and internal structure of these models has become
increasingly difficult and inefficient, making in-
struction tuning more important than ever.
FLAN(Chung et al., 2022) is one of the most sys-
tematic and comprehensive approaches among the
many studies on instruction tuning. This approach
fine-tunes the language model on the instruction-
optimised dataset, scales the number of tasks and
model size, and incorporates chain-of-thought data
in the fine-tuning. Although the authors did not
consider a specific approach to tuning instructions
in RALM architecture, their work provides a valu-
able reference for future research. In the instruction
fine-tuning of RALM, Lin et al. (2023b) integrated
in-context retrieval augmentation. This greatly re-
duces the likelihood of the language model being
misled by irrelevant retrieval content. SAIL(Luo
et al., 2023a) builds language generation and in-
struction tracking capabilities on complex search
results generated by internal and external search
engines. Using a corpus of instruction tuning, they
collect search results for each training case from
different search APIs and domains, and construct
a search-based training set containing a triplet of
(instruction, grounding information, response). In
contrast to training on instruction-tuned datasets,
Madaan et al. (2022) and Lazaridou et al. (2022)
propose to prompt large models directly from re-
trieved knowledge. Madaan et al. (2022) used GPT-
3 to clarify memory pairings of recorded cases
where the model misinterpreted the user’s inten-
tion, as well as user feedback. This ensures that
their system can generate enhanced prompts for
each new query based on user feedback. In con-
trast, Lazaridou et al. (2022) uses few-shot prompts
and answer reordering to improve inference com-
putation.
5.2.3
Post-Generation Output Enhancement
As defined in Section 2 on Parallel Interaction,
this interaction is inspired by the K-Nearest Neigh-
bor (KNN) LM (Khandelwal et al., 2019). It is a
paradigmatic instance of RALM, wherein the LM
is employed solely to enhance the outcomes. Since
the proposal of KNN-LM, many researchers have
worked to optimize the model. In this section, we
will describe the landmark work in detail.
13
The KNN-LM approach involves linearly inter-
polating the extended neural language model with
the K-Nearest Neighbours in the pre-trained LM
embedding space. Zhong et al. (2022) proposed
different processing for three types of memories
(local, long-term and external) and added training
for in-batch tokens to KNN-LM. The proposed
changes aim to improve the performance of the
model. Unlike KNN-LM, which only uses mem-
ory units during training, TRIME (Zhong et al.,
2022) uses memory units during both testing and
training. He et al. (2021) suggested that not all
generated tokens need to be retrieved. Instead,
a lightweight neural network can be trained to
aid the KNN-LM in adaptive retrieval. Addition-
ally, efficiency can be improved through database
streamlining and dimension reduction. Alon et al.
(2022) proposed RETOMATON, an unsupervised,
weighted finite automaton built on top of the data
store. RETOMATON is based on saving pointers
between successive data store entries and cluster-
ing techniques. RETOMATON is more effective
than ADAPTRET(He et al., 2021) in improving
accuracy by utilizing remaining pointers during
KNN retrieval. Even without KNN retrieval, in-
terpolation operations can still be performed us-
ing the stored previous information in the pointers,
unlike ADAPTRET which solely relies on the lan-
guage model. Furthermore, RETOMATON is unsu-
pervised, requiring no additional data for training,
making it more data-efficient. Grave et al. (2016)
proposed using continuous cache to improve the
performance of KNN-LM. This involves storing
past hidden activations and accessing them at the
appropriate time by dot product with present hid-
den activation. Yogatama et al. (2021) utilise an
extended short-term context by caching local hid-
den states and global long-term memory by retriev-
ing a set of nearest-neighbour tokens at each time
step. They also design a gating function to adap-
tively combine multiple sources of information for
prediction. Compared to KNN-LM, this method
uses dynamic weights and can handle cases where
interpolation is not feasible, such as when the mem-
ory output is an image, video, or sound. Drozdov
et al. (2022) proposed a method for adjusting the
interpolation weights. The weights are dynami-
cally adjusted based on the size of the region of
overlap between the retrieved stored data and the
assessment set, which reflects the quality of the
retrieval.
5.3
Overall Enhancement
This section presents the researchers’ efforts on the
RALM architecture as a whole, including End-to-
End Training and Build intermediate modules.
5.3.1
End-to-End Training
Researchers have begun working on a method
called end-to-end training, which aims to minimise
manual intervention and focus solely on data. This
method utilises deep learning and is becoming in-
creasingly popular due to the growing amount of
available data. During research on RALM archi-
tectures, many researchers tend to use end-to-end
training methods to achieve better results.
Lewis et al. (2020) and Guu et al. (2020) were
among the first researchers to apply end-to-end
training to the field of RALM. However, they dif-
fered in their approach. REALM (Guu et al., 2020)
used masked language training in the pre-training
phase and included a retriever that can be trained
end-to-end. In the fine-tuning phase, only the QA
task was targeted while keeping the retriever frozen.
On the other hand, RAG (Lewis et al., 2020) used
an already trained retriever, DPR, and only em-
ployed BART for partial end-to-end training. Sim-
ilar to REALM, Sachan et al. (2021) present an
unsupervised pre-training method that involves an
inverse cloze task and masked salient spans. This is
followed by supervised fine-tuning using question-
context pairs. In addition, they find that the use
of end-to-end trained retrievers resulted in a sig-
nificant improvement in performance across tasks.
Singh et al. (2021) apply end-to-end training to
multi-document processing, in their proposed ap-
proach, the value of the latent variable, which rep-
resents the set of relevant documents for a given
question, is estimated iteratively. This estimate is
then used to update the parameters of the retriever
and reader. Siriwardhana et al. (2023) describe
the end-to-end optimization of RAG from previous
studies and introduces an auxiliary training signal
to incorporate more domain-specific knowledge.
This signal forces RAG-end2end to reconstruct a
given sentence by accessing relevant information
in an external knowledge base. This approach has
greatly improved domain adaptability.
5.3.2
Intermediate Modules
Recently, some researchers have constructed an
intermediate module to coordinate the activities
of both the retriever and the language model due
to space or black-box LLM constraints, without
14
DataSource
Structureddata
KnowledgeGraph
GraphQA
G-Retriever
WikiKG90Mv2
ReSKGC
OpendialKG
SURGE
Structuredtext
NaturalQuestions（NQ）
FILCO,Atlas,ICRALM
HotpotQA(HQA)
FILCO,DSP,RA-DIT
Unstructureddata
Video
VideoQA
MA-DRNN
WebVid-10M
Animate-A-Story
MSVD
R-ConvED
Audio
Clotho
RECAP
AudioSet
Make-an-audio
AudioCaps
Re-AudioLDM
Image
LAION
RA-CM3
VQA-2
REVEAL
MultimodalQA
MuRAG
Text
FEVER
FILCO,Atlas,RAG
SQuAD
DSP,Instructretro,Prca
Figure 5: Classification of RALM data sources.
improving either.
Cheng et al. (2024) present Selfmem, a model
designed to tackle the issue of low corpus quality.
Selfmem utilises a retrieval-enhanced generator to
create an infinite pool of memory, which is then
used by a memory selector to choose an output for
subsequent generations. This approach enables the
model to use its own output to enhance generation.
Peng et al. (2023) propose an AI agent that formu-
lates human system dialogue as a Markov Decision
Process (MDP) described by a quintuple. The quin-
tuple includes an infinitely large set of dialogue
states, a collection of historical behaviours, a prob-
ability of state transfer, external rewards obtained,
and a variable parameter.
6
Data Sources
This section will introduce some of the data sources
commonly used in RALM and categorise them
into structured and unstructured data. Figure 5
illustrates the categorization of data sources.
6.1
Structured Data
Structured data includes various structures, such
as tables and knowledge graphs. The benefit of
this type of data is its clear structure, typically in
tabular form, with each field precisely defined. It
is appropriate for storing numbers, dates, text, and
other data types. Structured data can be easily
queried, analysed, and processed using a structured
query language like SQL.
Natural Questions(NQ) (Kwiatkowski et al.,
2019) is a very well-known dataset in the NLU
field.The given text describes a structured ques-
tion and a corresponding Wikipedia page. The
page is annotated with a long answer, typically a
paragraph, and a short answer consisting of one or
more entities. If there is no long or short answer,
it is labelled as empty. Due to the reliability of
the Google search engine and its vast amount of
data, many scholars have used this dataset to train
RALM, such as Wang et al. (2023c), Izacard et al.
(2022) and Ram et al. (2023). HotpotQA(HQA)
(Yang et al., 2018) stores information about multi-
hop questions and provides sentence-level support-
ing facts needed for inference. The structure in-
cludes the paragraph, question, answer, and sen-
tence number that supports the answer. This dataset
has been used by many researchers, such as Wang
et al. (2023c), Khattab et al. (2022), and Feng et al.
(2024), to train RALM for multi-hop question an-
swering. Another significant form of structured
data is the Knowledge Graph. It is a data structure
that primarily consists of triples of (entities, rela-
tionships, attributes). Some of the most frequently
used datasets include Wikidata5M, WikiKG90Mv2,
15
OpendialKG, and KOMODIS. All of these models
(Kang et al., 2023) (Yu and Yang, 2023) (He et al.,
2024) rely on knowledge graphs as a data source.
6.2
Unstructured Data
Unstructured data, in contrast, does not have a
clearly defined data structure and exists in various
forms, including text, images, and audio. Due to its
large and diverse nature, it is challenging to store
and manage in traditional tabular form. Although
it contains valuable information, it requires natural
language processing, image recognition, and other
technologies to parse and comprehend.
Several RALM researchers, including Khattab
et al. (2022), Wang et al. (2023a), and Yang et al.
(2023), have used this dataset as a source of data.
The FEVER (Thorne et al., 2018) dataset is mainly
used for fact extraction and validation. Several
RALM researchers, including Lewis et al. (2020),
Wang et al. (2023c), and Izacard et al. (2022), have
used the factual text in this dataset as a source of
data. In addition to unstructured text, there is also
a significant amount of inherently less structured
data, such as images, videos, and audio. Several
common image datasets are available for use in re-
search, including MNIST, CIFAR-10, Pascal VOC,
and COCO. Many studies (Chen et al., 2022b) (Hu
et al., 2023) (Yasunaga et al., 2022) in the field
of RALM have utilized these datasets. Common
audio datasets used in speech research include LJ
Speech, JSUT, and RUSLAN. Many studies (Yuan
et al., 2024) (Huang et al., 2023) (Ghosh et al.,
2024) in the field also rely on audio data as a
primary source. Common video datasets used in
research include HMDB, UCF101, and ASLAN.
Many studies (Chen et al., 2023) (He et al., 2023)
(Yin et al., 2019) in the field of RALM utilize audio
data as a source of information.
7
Applications
This section provides a summary of the down-
stream tasks that the RALM architecture primarily
focuses on. The relevant application directions
are categorized according to the requirements for
model generation or comprehension. RALM on
NLG indicates that the accomplishment of the task
primarily depends on the generative capabilities.
Conversely, RALM on NLU indicates that the ac-
complishment of the task primarily depends on
the comprehension capabilities. Finally, RALM
on both NLU and NLG indicates that the task is
generally handled in two ways, one that relies pri-
marily on comprehension capabilities and one that
relies primarily on generative capabilities. Figure
6 illustrates the categorization of applications.
7.1
RALM on NLG Tasks
7.1.1
Machine Translation
Machine translation, also known as automatic trans-
lation, is the process of converting one natural lan-
guage (source language) into another natural lan-
guage (target language) using a computer. It is a
branch of computational linguistics, one of the ulti-
mate goals of artificial intelligence, and has impor-
tant scientific research value. Machine translation
systems can be divided into two categories: rule-
based and corpus-based. The former comprises a
dictionary and a rule base, which collectively con-
stitute the knowledge source. In contrast, the latter
comprises a corpus that has been divided and la-
beled, and which does not require a dictionary or
rules. Instead, it is based on statistical laws and
most RALMs accomplish this task based on rules.
The Selfmem (Cheng et al., 2024) system em-
ploys two distinct language models for the machine
translation task. The first is a trainable mini-model,
which has been trained using a joint and bipartite
approach, respectively. The second is a few-shot
prompted LLM. Ultimately, Selfmem has demon-
strated a notable enhancement in its performance
across all four translation directions and for both
training architectures. This outcome suggests that
enhanced memory capabilities often result in su-
perior generation outcomes. In order to achieve
the best results, TRIME (Zhong et al., 2022) used
the IWSLT’14 De-En baseline. Given that the task
is sentence-level, the researchers did not use lo-
cal memory and long-term memory, as there are
few repetitive tokens in them. Instead, they used
only external memory, which enabled them to beat
the KNN-MT (Khandelwal et al., 2020) in perfor-
mance.
7.1.2
Math Teaching
As the RALM architecture continues to evolve,
an increasing number of potential application di-
rections are being identified. Levonian (Levonian
et al., 2023) were inspired by RALM to apply this
architecture to the domain of mathematics teaching
and learning. To address the fact that the knowl-
edge stored in the LLM may not match what is
taught in schools, they used one of three prompted
instructional conditions to generate responses to
16
Applications
RALMonBothNLUandNLG
CodeGenerationand
summarization
RACE
API-R&C
HGNN
REDCODER
QuestionAnswering
ICRALM
RR
FILCO
RAG
HyKGE
PKG
TextSummarization
RAMKG
RA-DIT
Self-Mem
LLM-R
RALMonNLU
CommonsenseReasoning
KG-BART
ITER-RETGEN
LLM-R
FLARE
KnowledgeGraphCompletion
ReSKGC
Factchecking
FILCO
Atlas
RAG
ImageGeneration
KNN-Diﬀusion
RDM
RE-IMAGEN
MDTIG
SlotFilling
KGI
RALMonNLG
DialogGeneration
SURGE
Selfmem
REPLUG
FILCO
MathTeaching
RAG-IMQA
MachineTranslation
TRIME
Selfmem
Figure 6: Classification of RALM applications.
math student queries using a retrieval enhancement
generation system. Survey respondents ranked the
responses according to preference and evaluated
basic math textbooks as a retrieval corpus.
7.1.3
Dialog Generation
Dialog generation, in particular lengthy dialogue,
is a challenging task. This is due to the necessity
of not only ensuring that the language model pos-
sesses natural language processing capabilities, but
also that the model is able to utilise context in order
to satisfy the requirements of the dialogue.
FILCO (Wang et al., 2023c) employs the
Wikipedia dataset from the KILT benchmark, re-
ferred to as the "Wizard of Wikipedia" (WoW),
to generate subsequent dialogue. This process in-
volves basing the output on a Wikipedia article,
with the input comprising the history of multiple
rounds of dialogue. RA-DIT (Lin et al., 2023b) also
employs the WoW dataset in the fine-tuning phase.
As a result of the command tuning operation, the
model outperforms Llama (Touvron et al., 2023a)
and Llama-REPLUG (Shi et al., 2023b) with the
same parameters for dialogue generation in the
zero-shot condition. The incorporation of Selfmem
(Cheng et al., 2024) into the retrieval-augmented
generator markedly enhances the generation of dia-
logue, as a consequence of its remarkable flexibil-
ity. This is achieved by the direct optimisation of
17
memory for the desired properties of diverse and
information-rich dialogues. In contrast, SURGE
(Kang et al., 2023) employs the Knowledge Graph
as a data source for the dialogue generation task,
wherein each dialogue round comprises facts from
a large-scale KG. In contrast to other related work
(Rony et al., 2022), they retrieve only contextu-
ally relevant subgraphs, thus avoiding the compu-
tational overheads and misleading models that can
result from retrieving irrelevant data.
7.2
RALM on NLU Tasks
7.2.1
Slot Filling
Slot filling is a technique employed in natural
language processing for the purpose of recogniz-
ing and extracting specific information from user-
supplied text or speech. In slot filling, the system
defines a set of slots in advance, with each slot
representing a specific information requirement.
These requirements may include, but are not lim-
ited to, date, time, location, and so forth. Upon
receipt of a user input in the form of text or speech,
the system performs an analysis of the content, at-
tempting to identify information that matches the
predefined slots or classification labels (Lu et al.,
2023b,a). This information is then populated into
the corresponding slots for subsequent processing
and response.
KGI (Glass et al., 2021) enhances dense channel
retrieval through the utilization of hard negatives
in dense indexing and implements a robust train-
ing process for retrieval enhancement generation.
The retrieval enhancement is employed to enhance
the effectiveness of the slot-filling task, thereby
facilitating the generation of high-quality knowl-
edge graphs by AI. The results demonstrate that the
method achieves excellent performance in TREx
and zsRE datasets and exhibits remarkable robust-
ness in TACRED dataset.
7.2.2
Image Generation
The process of text-to-image generation is a chal-
lenging one, requiring a model to demonstrate a
high degree of natural language understanding and
to convey this understanding through an image, in
contrast to the typical format of textual data.
In a pioneering study, Li et al. (2022a) proposed
the use of retrieval techniques to enhance the qual-
ity of text-to-image generation. They conducted
a comparative analysis of the quality and quantity
of the generated images with mainstream models
on the CUB and COCO datasets. Their findings
demonstrated that all models outperformed their
contemporaries. In contrast, RE-IMAGEN (Chen
et al., 2022c) focused on assisting the model in
generating images of uncommon objects through
retrieval.
This approach ultimately led to the
achievement of exceptionally high FID scores on
the COCO and WikiImage datasets. Even more
groundbreaking results were obtained on the au-
thors’ own proposed EntityDrawBench benchmark,
which encompasses a range of common and rare ob-
jects across multiple categories. RDM (Blattmann
et al., 2022), although trained in a similar man-
ner to RE-IMAGEN, employs image features as
the foundation for retrieval and is supplanted by
user examples during the inference process. Con-
sequently, RDM is capable of efficiently transfer-
ring the described artistic style to the generated
images. Furthermore, in contrast to RE-IMAGEN,
which employs image-text pairs for retrieval, KNN-
Diffusion (Sheynin et al., 2022) solely utilizes im-
ages for retrieval, resulting in a lower quality of
results on the COCO dataset.
7.2.3
Fact checking
Fact checking involves verifying a statement based
on evidence. This task at hand involves a retrieval
problem and a challenging implicit reasoning task.
Furthermore, This task typically involves taking
the statement as input and producing relevant docu-
ment passages that prove or disprove the statement.
Many RALM models get excellent performance
because they come with their own retrievers. It is
an important aspect of natural language understand-
ing.
RAG (Lewis et al., 2020) uses the FEVER
dataset to map labels (Supported, Refuted, NotE-
noughInfo) to individual output tokens. It is trained
directly using the declaration class, which is not
supervised over the retrieved evidence, unlike other
works. Atlas (Izacard et al., 2022) employs few-
shot learning to achieve performance comparable
to previous studies in just 64-shot conditions. Fur-
thermore, after training with the full dataset, it out-
performed the best model (Hoffmann et al., 2022)
available at the time. FILCO (Wang et al., 2023c)
approached the task of improving the quality of
retrieved documents by using the FEVER dataset
from the KILT base aggregation, which only in-
cluded the supports and refutes tags. Accuracy was
used as a metric.
18
7.2.4
Knowledge Graph Completion
A multitude of previous tasks have employed struc-
tured data in the form of knowledge graphs, with
knowledge graph completion representing a perva-
sive application. The conventional methodology
(Chen et al., 2022a) (Saxena et al., 2022) for com-
pletion entails defining the task as a sequence-to-
sequence process, wherein incomplete triples and
entities are transformed into text sequences. How-
ever, this approach is constrained by its reliance on
implicit reasoning, which significantly constrains
the utility of the knowledge graph itself. ReSKGC
(Yu and Yang, 2023) proposes the integration of
retrieval augmentation techniques with knowledge
graph completion. This integration entails the se-
lection of semantically relevant triples from the
knowledge graph and their utilization as evidence
to inform the generation of output through explicit
reasoning. This model employs data from the Wiki-
data5M and WikiKG90Mv2 datasets, demonstrat-
ing superior performance compared to other exist-
ing work in a range of conditions.
7.2.5
Commonsense Reasoning
Commonsense Reasoning is a challenging task for
language models. In addition to exhibiting human-
like thinking and reasoning patterns, these models
must also be able to store a substantial amount of
commonsense knowledge. However, the advent of
RALM has made the second requirement less de-
manding, as retrieval techniques provide language
models with access to nonparametric memories.
FLARE (Jiang et al., 2023c) uses StrategyQA,
which contains a significant number of yes/no ques-
tions from a diverse range of sources. In addition,
the authors request that the model provide the ex-
act reasoning process and the final answer that
determines the yes/no answer, ensuring that the
answer matches the gold answer exactly. The in-
corporation of in-context samples into the retrieved
content, along with training using data from the
COPA, HellaSwag, and PIQA datasets, has resulted
in the development of LLM-R (Wang et al., 2023b)
model that exhibits excellent performance. The
fundamental concept of the ITER-RETGEN (Gao
et al., 2022) model employs an iterative method-
ology to integrate retrievers and language mod-
els. The model was trained for the Commonsense
Reasoning task using the StrategyQA dataset and
achieved its optimal performance at seven itera-
tions. In contrast, KG-BART (Shao et al., 2023) is
designed to prioritize the Commonsense Reasoning
task and employs knowledge graphs to enhance its
performance in this area. This approach has proven
effective in significantly improving the model’s
ability to complete the Commonsense Reasoning
task, with performance approaching that of human
beings under certain evaluation metrics.
7.3
RALM on both NLU and NLG tasks
7.3.1
Text Summarization
Text summarization represents a crucial application
of language modelling. In essence, text summariza-
tion is the process of generating concise and fluent
summaries while maintaining the content and over-
all meaning of key information. Currently, two
distinct types of this task exist: extractive summa-
rization and abstractive summarization.
RA-DIT (Lin et al., 2023b) employs the
CNN/DailyMail dataset to refine the language
model component of the model, which demon-
strates remarkable efficacy in the text summariza-
tion task due to the operation of command fine-
tuning. In contrast, Self-Mem(Cheng et al., 2024)
was trained using the XSum and BigPatent datasets.
The authors observed that memory enhancement
had a significantly larger effect on BigPatent than
XSum. They hypothesize that this discrepancy is
due to the inclusion of official patent documents in
the BigPatent dataset, which exhibit considerable
similarity. The LLM-R (Wang et al., 2023b) model
employs an in-context learning approach, integrat-
ing RALM, and utilizes the AESLC, AG News, and
Gigaword datasets for text summarization training.
The results demonstrate that LLM-R significantly
outperforms both traditional and dense retrievers
in the summarization task. RAMKG (Gao et al.,
2022) extended the iterative training of the RALM
architecture to the multilingual domain and em-
ployed two multilingual datasets, EcommerceMKP
and AcademicMKP, for the training of summariza-
tion work, achieving the best results at that time.
7.3.2
Question Answering
Question answering also includes generative and
extractive forms. It is a common task in NLP
that relies on domain-specific knowledge. RALMs
can achieve better results than traditional language
models by utilizing externally stored knowledge.
Common question answering tasks include domain-
specific QA, open-domain QA(ODQA), and multi-
hop QA. In the medical field, large language mod-
els are commonly used, PKG(Luo et al., 2023b)
uses the relevant data from the MedMC-QA dataset,
19
using the questions in the training set as input and
the medical explanations as background knowl-
edge, and the accuracy of the background knowl-
edge generated by the model as an evaluation met-
ric. HyKGE(Jiang et al., 2023b) also targets ques-
tion answering in the medical field, but uses a
knowledge graph-enhanced approach. When tar-
geting ODQA tasks, RAG (Lewis et al., 2020) con-
siders questions and answers as input-output text
pairs and trains by minimizing the negative log-
likelihood of the answers. In contrast, ICRALM
(Ram et al., 2023) exclusively performs the ODQA
task using frozen LMs, which have not been en-
hanced by pre-training, fine-tuning, or retrieval, as
well as the associated knowledge documents. Other
models (Wang et al., 2023c) (Lin et al., 2023b) (Shi
et al., 2023b) were also trained for the ODQA task.
In relation to the multi-hop QA task, FILCO (Wang
et al., 2023c) used their proposed filtered retrieval
method to filter multiple documents. They vali-
dated their approach using the HotpotQA dataset.
RR (He et al., 2022), on the other hand, used the
Chain-of-Thought (CoT) approach to address the
multi-hop problem. In addition, many other models
(Jiang et al., 2023c) (Khattab et al., 2022) deal with
multi-hop problems.
7.3.3
Code Generation and Summarization
Code generation (Romera-Paredes et al., 2024; Ye
et al., 2024) and summarization (Nam et al., 2024)
differ from ordinary text generation and summariza-
tion in terms of the target audience and processing.
Code generation and summarization involves com-
puter program code, which may require domain-
specific syntactic and semantic understanding, in
addition to higher requirements for NLU and NLG
capabilities of language models.
REDCODER (Parvez et al., 2021) initially iden-
tified potential candidate codes from existing code
or abstract databases. The researchers retained
1.1 million unique lines of codes and abstracts as
retrieved data through multiple channels. The fi-
nal evaluation on the CodeXGLUE dataset demon-
strated excellent performance across multiple pro-
gramming languages and evaluation metrics. An-
other proposed enhancement was put forth by Zan
et al. (2022), who utilized private libraries to en-
hance the quality of code generation.
This in-
volved first identifying the most suitable private
libraries based on the API documentation through
the APIRetriever component, and then utilizing the
APIcoder for generation. This approach led to a no-
table improvement in the accuracy of the generated
content.
Liu et al. (2020) propose a novel attention-based
dynamic graph to complement the source code for
static graph representations and design a hybrid
message-passing GNN to capture local and global
structural information. This approach improves
the accuracy of code summarization and ultimately
yields superior performance over both mainstream
retrievers and generators. The RACE (Shi et al.,
2022) model employ a conventional RALM frame-
work to consolidate code in five programming lan-
guages within the MCMD dataset, resulting in a 6
to 38 percent enhancement over all baseline mod-
els.
8
Evaluation
This section provides a summary of the evaluation
approach and benchmarks for RALM. In Sections
6 and 7, we presented some evaluation criteria for
large language model tasks as well as baselines.
At the time of the initial proposal of the RALM
architecture, the majority of researchers employed
generalized benchmarks. However, as the RALM
architecture evolved, there was a growing number
of RALM-specific evaluation methods and base-
lines proposed. Table 3 demonstrates the details of
each evaluation model.
RAGAS (Es et al., 2023) employs the WikiEval
Dataset to assess the faithfulness, answer relevance,
and context relevance of RALMs. Faithfulness is
defined as the degree to which responses align with
the provided context. Answer relevance refers to
the extent to which generated responses address
the actual question posed. Context relevance is
gauged by the degree to which retrieved context
is centralized and devoid of irrelevant information.
Additionally, the researchers utilize the prompt gpt-
3.5-turbo-16k model to automate the evaluation
process. RGB (Chen et al., 2024) developed a
bilingual Chinese and English evaluation system
employing three evaluation metrics: accuracy, re-
jection rate, and error detection rate. These metrics
were utilized to assess the noise robustness, nega-
tive rejection, information integration, and counter-
factual robustness of the data sources, which were
articles processed by LM and retrieved through
Google’s API. CRUD-RAG (Lyu et al., 2024) con-
siders the impact of retrieval components and the
construction of external knowledge bases that have
not been previously considered by researchers. A
20
Table 3: Summary of evaluation methods in RALM.
Reference
RAGAS
(Es et al., 2023)
RGB
(Chen et al., 2024)
CRUD-RAG
(Lyu et al., 2024)
ARES
(Saad-Falcon et al., 2023)
MIRAGE
(Xiong et al., 2024)
RECALL
(Liu et al., 2023)
Dataset
WikiEval
LLM-generated
MMLU-Med
MedQA-US
MedMCQA
PubMedQA
BioASQ-Y/N
EventKG
UJ
Target
Retrieval Quality;
Generation Quality
Generation Quality
Context
Relevance
√
√
Faithfulness
√
√
√
√
√
√
Answer
Relevance
√
√
Noise
Robustness
√
Information
Integration
√
√
√
Negative
Rejection
√
Counterfactual
Robustness
√
√
√
Error
Correction
√
Summarization
√
dataset was generated using a large model to evalu-
ate the Create, Read, Update, and Delete (summa-
rization) capabilities of RALM through four eval-
uation metrics: ROUGE, BLEU, bertScore, and
RAGQuestEval. In addition, ARES (Saad-Falcon
et al., 2023) employs datasets generated by the LM,
but utilizes a lightweight LM to determine the qual-
ity of individual RALM components and utilizes
human-labeled data points for prediction-powered
inference. The RALM’s context is evaluated us-
ing the KILT and SuperGLUE benchmarks, with
Relevance, Answer Faithfulness, and Answer Rele-
vance being the relevant criteria.
In addition to the general assessment of RALM,
there has been some work focusing on the assess-
ment of specific details and domains. RECALL
(Liu et al., 2023) employs the EventKG and UJ
datasets to incorporate inaccurate information into
its existing data set. It then determines whether
RALM is susceptible to being misled by such
inaccurate information through two tasks: ques-
tion answering and text generation. Xiong et al.
(2024) concentrated on the medical domain and
proposed MIRAGE, which integrates data from
five datasets, including MMLU-Med, to evaluate
the zero-shot learning, multi-choice evaluation,
retrieval-augmented generation, and question-only
retrieval ideation capabilities of medical RALMs.
Ultimately, they also discovered the log-linear scal-
ing property and the "lost-in-the-middle" effect in
the medical domain.
9
Disscussion
This section is devoted to an analysis of the limi-
tations of existing RALM architectures and a de-
scription of potential future developments. Figure
7 summarizes the limitations of existing RALMs
and our proposed solutions.
9.1
Limitations
This section presents a summary and analysis of
some of the limitations of existing RALMs.
9.1.1
Poor Robustness
Robustness is a crucial aspect to be considered in
all systems. RALM systems, despite exhibiting
performance benefits in several domains, introduce
a multitude of uncertainties to the architecture due
to the incorporation of retrieval as a technique. As
elucidated by Hu et al. (2024), through exceedingly
simple prefix attacks, not only can the relevance
and accuracy of RALM output be diminished, but
even the retrieval strategy of the retriever can be al-
tered. Consequently, in addition to utilising various
21
Figure 7: Summary of limitations of current RALM models and future prospects.
retrieval enhancement techniques to enhance the
performance of LMs, researchers should also take
care to minimise the effects of factually inaccurate
data, information that is not relevant to problem
solving and even some harmful hints and prefixes
on LMs.
9.1.2
Poor Quality of Retrieval Results
A significant number of researchers (Yan et al.,
2024) (Asai et al., 2023) engaged in the endeav-
our of enhancing retrieval efficacy have asserted
that although their proposed models are demon-
strably beneficial in optimising the quality of the
output, there is as yet no assurance that the retrieval
outcomes can be entirely aligned with the LM. Par-
ticularly when using the Internet as a retrieval tool,
the quality of Internet sources can vary widely, and
merging this data without proper consideration can
introduce noise or misleading information into the
resulting output.
9.1.3
Overspending
While existing RALMs (Siriwardhana et al., 2023)
(Guu et al., 2020) (Borgeaud et al., 2022) can
greatly improve the performance of LMs in various
domains, some of them require extensive model
changes as well as complex pre-training and fine-
tuning operations, which greatly increases the time
and space overhead and also reduces the scalability
of RALMs. In addition, as the scale of retrieval
increases, so does the complexity of storing and
accessing the data sources. As a result, researchers
must weigh the benefits of modifying the model
against the costs.
9.1.4
Few Applications
Although many RALMs have greatly improved the
performance of LMs in various domains, there has
not been much improvement from an application
perspective, and RALMs are still doing some of
the routine work that was done in the early days
of LMs, e.g., question answering, summarizing
(Luo et al., 2023b) (Jiang et al., 2023b) (Alon et al.,
2022). Although there have been some very inter-
esting application directions recently, such as math
teaching (Levonian et al., 2023), slot filling (Glass
et al., 2021), etc., this is not enough. A technology
always needs to be actually used to fully prove its
value, and RALM is no exception.
9.2
Future Prospects
This section suggests some possible directions for
the future development of RALM, based mainly on
the limitations mentioned in Section 9.1.
9.2.1
Improve Robustness
Some scholars have mentioned possible ways to
improve model robustness in the future work sec-
tion of their papers, such as explicit self-reflection
and fine-grained attribution (Asai et al., 2023). In
22
contrast to these works, Hu et al. (2024) proposed
a method called Gradient Guided Prompt Perturba-
tion (GGPP), a way to perturb the RALM, which
was experimentally found to be effective in im-
proving the situation by utilizing the SAT probe
(Yuksekgonul et al., 2023) and activation (ACT)
classifier. A method is proposed to detect this per-
turbation by prompting the internal state of the
perturbed RALM. In addition, by proposing and
improving the evaluation method of RALM and
the related baseline can also help improve the ro-
bustness of the model, Chen et al. (2024) made a
series of evaluation system for RALM by focusing
on the robustness.
9.2.2
Improve Retrieval Quality
Improving the quality of retrieval can be considered
in two parts: improving the quality of the dataset
used for retrieval, and improving the performance
of the retrieval technique. Nowadays, many data
sets are given to LLM to generate relevant content,
and since LLM itself has "hallucination", certain
means must be adopted to ensure the accuracy of
the data, such as using human beings to supervise
the refinement (Chen et al., 2024). In addition, due
to the wide range of information sources on the
Internet, it is obviously not enough to rely solely
on search engines for screening, so it is necessary
to improve the retrieval technology, such as the use
of BM25 (Luo et al., 2023a) or TF-IDF (Lazaridou
et al., 2022) algorithms for further re-ranking.
9.2.3
Weigh Expenses and Benefits
Reducing the overhead can be considered from
three perspectives: first, some plug-and-play in-
termediate modules can be designed, e.g., CRAG
(Yan et al., 2024), Selfmem (Cheng et al., 2024),
AI agent (Peng et al., 2023), or some deployment
solutions, e.g., LangChain, Llama Index, so that
there is no need to make targeted improvements
for each model. Second, Internet retrieval can be
utilized to reduce the overhead of the retriever, but
attention needs to be paid to the data relevance men-
tioned earlier. Finally, In-context learning can be
employed to reduce the overhead associated with
improving LMs, e.g., ICRALM (Ram et al., 2023).
9.2.4
Expand Applications
In the contemporary era, the application of LLM
has been expanded to encompass a multitude of do-
mains, whereas the application direction of RALM
remains relatively limited. To address this limi-
tation, researchers must not only consider the ex-
isting application areas of LLM but also leverage
the distinctive strengths of RALM, which excels in
addressing problems closely related to knowledge
and experience. Additionally, they should integrate
RALM with other advanced technologies and uti-
lize it to overcome the challenges associated with
them. This paper presents several illustrative ex-
amples, including decision support, search engine,
and recommendation system.
10
Conclusion
The integration of RALMs represents a signifi-
cant advance in the capabilities of NLP systems.
This survey has provided an extensive review of
RALMs, highlighting their architecture, applica-
tions, and the challenges they face. RALMs en-
hance language models by retrieving and integrat-
ing external knowledge, leading to improved per-
formance across a variety of NLP tasks, including
translation, dialogue generation, and knowledge
graph completion.
Despite their successes, RALMs encounter sev-
eral limitations. Notably, their robustness against
adversarial inputs, the quality of retrieval results,
the computational costs associated with their de-
ployment, and a lack of diversity in application
domains have been identified as areas requiring fur-
ther attention. To address these, the research com-
munity has proposed several strategies, such as im-
proving the evaluation methods, refining retrieval
techniques, and exploring cost-effective solutions
that maintain a balance between performance and
efficiency.
In the future, the advancement of RALMs will
depend on the enhancement of their robustness, the
improvement of retrieval quality, and the expansion
of their application scope. By incorporating more
sophisticated techniques and integrating RALMs
with other AI technologies, these models can be
leveraged to address an even broader spectrum of
challenges. The ongoing research and development
in this field are expected to result in more resilient,
efficient, and versatile RALMs, thereby pushing
the boundaries of what is achievable in NLP and
beyond. As RALMs continue to evolve, they hold
the promise of enabling AI systems with deeper
understanding and more human-like language ca-
pabilities, thereby opening up new possibilities in
a wide range of fields.
23
References
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama
Ahmad, Ilge Akkaya, Florencia Leoni Aleman,
Diogo Almeida, Janko Altenschmidt, Sam Altman,
Shyamal Anadkat, et al. 2023. Gpt-4 technical report.
arXiv preprint arXiv:2303.08774.
Leonard Adolphs, Benjamin Boerschinger, Christian
Buck, Michelle Chen Huebscher, Massimiliano Cia-
ramita, Lasse Espeholt, Thomas Hofmann, Yannic
Kilcher, Sascha Rothe, Pier Giuseppe Sessa, et al.
2021.
Boosting search engines with interactive
agents. arXiv preprint arXiv:2109.00527.
Uri Alon, Frank Xu, Junxian He, Sudipta Sengupta, Dan
Roth, and Graham Neubig. 2022. Neuro-symbolic
language modeling with automaton-augmented re-
trieval.
In International Conference on Machine
Learning, pages 468–485. PMLR.
Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and
Hannaneh Hajishirzi. 2023. Self-rag: Learning to
retrieve, generate, and critique through self-reflection.
arXiv preprint arXiv:2310.11511.
Nathalie Aussenac-Gilles and Dagobert Sörgel. 2005.
Text analysis for ontology and terminology engineer-
ing. Applied Ontology, 1(1):35–46.
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-
ton. 2016.
Layer normalization.
arXiv preprint
arXiv:1607.06450.
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,
Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei
Huang, et al. 2023. Qwen technical report. arXiv
preprint arXiv:2309.16609.
Imon Banerjee, Yuan Ling, Matthew C Chen, Sadid A
Hasan, Curtis P Langlotz, Nathaniel Moradzadeh,
Brian Chapman, Timothy Amrhein, David Mong,
Daniel L Rubin, et al. 2019. Comparative effective-
ness of convolutional neural network (cnn) and recur-
rent neural network (rnn) architectures for radiology
text report classification. Artificial intelligence in
medicine, 97:79–88.
Sid Black, Stella Biderman, Eric Hallahan, Quentin
Anthony, Leo Gao, Laurence Golding, Horace He,
Connor Leahy, Kyle McDonell, Jason Phang, et al.
2022. Gpt-neox-20b: An open-source autoregressive
language model. arXiv preprint arXiv:2204.06745.
Andreas Blattmann, Robin Rombach, Kaan Oktay,
Jonas Müller, and Björn Ommer. 2022. Retrieval-
augmented diffusion models. Advances in Neural
Information Processing Systems, 35:15309–15324.
Sebastian Borgeaud, Arthur Mensch, Jordan Hoff-
mann, Trevor Cai, Eliza Rutherford, Katie Milli-
can, George Bm Van Den Driessche, Jean-Baptiste
Lespiau, Bogdan Damoc, Aidan Clark, et al. 2022.
Improving language models by retrieving from tril-
lions of tokens. In International conference on ma-
chine learning, pages 2206–2240. PMLR.
Leonid Boytsov, David Novak, Yury Malkov, and Eric
Nyberg. 2016. Off the beaten path: Let’s replace
term-based retrieval with k-nn search. In Proceed-
ings of the 25th ACM international on conference
on information and knowledge management, pages
1099–1108.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing
systems, 33:1877–1901.
Chen Chen, Yufei Wang, Bing Li, and Kwok-Yan Lam.
2022a. Knowledge is flat: A seq2seq generative
framework for various knowledge graph completion.
arXiv preprint arXiv:2209.07299.
Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun.
2024.
Benchmarking large language models in
retrieval-augmented generation. In Proceedings of
the AAAI Conference on Artificial Intelligence, vol-
ume 38, pages 17754–17762.
Jingwen Chen, Yingwei Pan, Yehao Li, Ting Yao,
Hongyang Chao, and Tao Mei. 2023. Retrieval aug-
mented convolutional encoder-decoder networks for
video captioning. ACM Transactions on Multime-
dia Computing, Communications and Applications,
19(1s):1–24.
Wenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, and
William W Cohen. 2022b.
Murag: Multimodal
retrieval-augmented generator for open question
answering over images and text.
arXiv preprint
arXiv:2210.02928.
Wenhu Chen, Hexiang Hu, Chitwan Saharia, and
William W Cohen. 2022c. Re-imagen: Retrieval-
augmented text-to-image generator. arXiv preprint
arXiv:2209.14491.
Xin Cheng, Di Luo, Xiuying Chen, Lemao Liu,
Dongyan Zhao, and Rui Yan. 2024. Lift yourself
up: Retrieval-augmented text generation with self-
memory. Advances in Neural Information Processing
Systems, 36.
Rewon Child,
Scott Gray,
Alec Radford,
and
Ilya
Sutskever.
2019.
Generating
long
se-
quences with sparse transformers. arXiv preprint
arXiv:1904.10509.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham, Hyung Won Chung, Charles Sutton, Sebas-
tian Gehrmann, et al. 2023. Palm: Scaling language
modeling with pathways. Journal of Machine Learn-
ing Research, 24(240):1–113.
Hyung Won Chung, Le Hou, Shayne Longpre, Barret
Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
2022. Scaling instruction-finetuned language models.
arXiv preprint arXiv:2210.11416.
24
Yann N Dauphin, Angela Fan, Michael Auli, and David
Grangier. 2017. Language modeling with gated con-
volutional networks. In International conference on
machine learning, pages 933–941. PMLR.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.
Andrew Drozdov, Shufan Wang, Razieh Rahimi, An-
drew McCallum, Hamed Zamani, and Mohit Iyyer.
2022. You can’t pick your neighbors, or can you?
when and how to rely on retrieval in the k nn-lm.
arXiv preprint arXiv:2210.15859.
Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi
Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin,
Percy S Liang, and Tatsunori B Hashimoto. 2024.
Alpacafarm: A simulation framework for methods
that learn from human feedback. Advances in Neural
Information Processing Systems, 36.
Shahul Es, Jithin James, Luis Espinosa-Anke, and
Steven Schockaert. 2023. Ragas: Automated eval-
uation of retrieval augmented generation.
arXiv
preprint arXiv:2309.15217.
Chao Feng, Xinyu Zhang, and Zichu Fei. 2023. Knowl-
edge solver: Teaching llms to search for domain
knowledge from knowledge graphs. arXiv preprint
arXiv:2309.03118.
Zhangyin Feng, Xiaocheng Feng, Dezhi Zhao, Maojin
Yang, and Bing Qin. 2024. Retrieval-generation syn-
ergy augmented large language models. In ICASSP
2024-2024 IEEE International Conference on Acous-
tics, Speech and Signal Processing (ICASSP), pages
11661–11665. IEEE.
Yifan Gao, Qingyu Yin, Zheng Li, Rui Meng, Tong
Zhao, Bing Yin, Irwin King, and Michael R Lyu.
2022. Retrieval-augmented multilingual keyphrase
generation with retriever-generator iterative training.
arXiv preprint arXiv:2205.10471.
Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia,
Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen
Wang. 2023. Retrieval-augmented generation for
large language models: A survey. arXiv preprint
arXiv:2312.10997.
Sreyan Ghosh, Sonal Kumar, Chandra Kiran Reddy
Evuru, Ramani Duraiswami, and Dinesh Manocha.
2024. Recap: retrieval-augmented audio captioning.
In ICASSP 2024-2024 IEEE International Confer-
ence on Acoustics, Speech and Signal Processing
(ICASSP), pages 1161–1165. IEEE.
Michael Glass, Gaetano Rossiello, Md Faisal Mahbub
Chowdhury, and Alfio Gliozzo. 2021. Robust re-
trieval augmented generation for zero-shot slot filling.
arXiv preprint arXiv:2108.13934.
Jianping Gou, Baosheng Yu, Stephen J Maybank, and
Dacheng Tao. 2021.
Knowledge distillation: A
survey. International Journal of Computer Vision,
129(6):1789–1819.
Edouard Grave, Armand Joulin, and Nicolas Usunier.
2016.
Improving neural language models with a
continuous cache. arXiv preprint arXiv:1612.04426.
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-
pat, and Mingwei Chang. 2020. Retrieval augmented
language model pre-training. In International confer-
ence on machine learning, pages 3929–3938. PMLR.
Kailash A Hambarde and Hugo Proenca. 2023. Infor-
mation retrieval: recent advances and beyond. IEEE
Access.
Jiaming Han, Renrui Zhang, Wenqi Shao, Peng Gao,
Peng Xu, Han Xiao, Kaipeng Zhang, Chris Liu,
Song Wen, Ziyu Guo, et al. 2023. Imagebind-llm:
Multi-modality instruction tuning. arXiv preprint
arXiv:2309.03905.
Hangfeng He, Hongming Zhang, and Dan Roth. 2022.
Rethinking with retrieval: Faithful large language
model inference. arXiv preprint arXiv:2301.00303.
Junxian He,
Graham Neubig,
and Taylor Berg-
Kirkpatrick. 2021. Efficient nearest neighbor lan-
guage models. arXiv preprint arXiv:2109.04212.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 770–
778.
Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh V Chawla,
Thomas Laurent, Yann LeCun, Xavier Bresson, and
Bryan Hooi. 2024. G-retriever: Retrieval-augmented
generation for textual graph understanding and ques-
tion answering. arXiv preprint arXiv:2402.07630.
Yingqing He, Menghan Xia, Haoxin Chen, Xiaodong
Cun, Yuan Gong, Jinbo Xing, Yong Zhang, Xintao
Wang, Chao Weng, Ying Shan, et al. 2023. Animate-
a-story: Storytelling with retrieval-augmented video
generation. arXiv preprint arXiv:2307.06940.
Jordan Hoffmann, Sebastian Borgeaud, Arthur Men-
sch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-
ford, Diego de Las Casas, Lisa Anne Hendricks,
Johannes Welbl, Aidan Clark, et al. 2022. Train-
ing compute-optimal large language models. arXiv
preprint arXiv:2203.15556.
Sebastian Hofstätter, Jiecao Chen, Karthik Raman, and
Hamed Zamani. 2023. Fid-light: Efficient and effec-
tive retrieval-augmented text generation. In Proceed-
ings of the 46th International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, pages 1437–1447.
25
Frederik Hogenboom, Flavius Frasincar, and Uzay Kay-
mak. 2010. An overview of approaches to extract
information from natural language corpora. Informa-
tion Foraging Lab, 69.
Zhibo Hu, Chen Wang, Yanfeng Shu, Liming Zhu, et al.
2024. Prompt perturbation in retrieval-augmented
generation based large language models.
arXiv
preprint arXiv:2402.07179.
Ziniu Hu, Ahmet Iscen, Chen Sun, Zirui Wang, Kai-
Wei Chang, Yizhou Sun, Cordelia Schmid, David A
Ross, and Alireza Fathi. 2023. Reveal: Retrieval-
augmented visual-language pre-training with multi-
source multimodal knowledge memory. In Proceed-
ings of the IEEE/CVF conference on computer vision
and pattern recognition, pages 23369–23379.
Xinyu Hua and Lu Wang. 2018. Neural argument gen-
eration augmented with externally retrieved evidence.
arXiv preprint arXiv:1805.10254.
Rongjie Huang, Jiawei Huang, Dongchao Yang, Yi Ren,
Luping Liu, Mingze Li, Zhenhui Ye, Jinglin Liu, Xi-
ang Yin, and Zhou Zhao. 2023. Make-an-audio: Text-
to-audio generation with prompt-enhanced diffusion
models. In International Conference on Machine
Learning, pages 13916–13932. PMLR.
Gautier Izacard, Mathilde Caron, Lucas Hosseini, Se-
bastian Riedel, Piotr Bojanowski, Armand Joulin,
and Edouard Grave. 2021. Unsupervised dense in-
formation retrieval with contrastive learning. arXiv
preprint arXiv:2112.09118.
Gautier Izacard and Edouard Grave. 2020a. Distilling
knowledge from reader to retriever for question an-
swering. arXiv preprint arXiv:2012.04584.
Gautier Izacard and Edouard Grave. 2020b.
Lever-
aging passage retrieval with generative models for
open domain question answering. arXiv preprint
arXiv:2007.01282.
Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas
Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-
Yu, Armand Joulin, Sebastian Riedel, and Edouard
Grave. 2022.
Atlas: Few-shot learning with re-
trieval augmented language models. arXiv preprint
arXiv:2208.03299.
Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan
Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea
Madotto, and Pascale Fung. 2023. Survey of halluci-
nation in natural language generation. ACM Comput-
ing Surveys, 55(12):1–38.
Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-
sch, Chris Bamford, Devendra Singh Chaplot, Diego
de las Casas, Florian Bressand, Gianna Lengyel, Guil-
laume Lample, Lucile Saulnier, et al. 2023a. Mistral
7b. arXiv preprint arXiv:2310.06825.
Xinke Jiang, Ruizhe Zhang, Yongxin Xu, Rihong Qiu,
Yue Fang, Zhiyuan Wang, Jinyi Tang, Hongxin Ding,
Xu Chu, Junfeng Zhao, et al. 2023b.
Think and
retrieval: A hypothesis knowledge graph enhanced
medical large language models.
arXiv preprint
arXiv:2312.15883.
Zhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing
Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang,
Jamie Callan, and Graham Neubig. 2023c.
Ac-
tive retrieval augmented generation. arXiv preprint
arXiv:2305.06983.
Zi-Hang Jiang, Weihao Yu, Daquan Zhou, Yunpeng
Chen, Jiashi Feng, and Shuicheng Yan. 2020. Con-
vbert: Improving bert with span-based dynamic con-
volution. Advances in Neural Information Process-
ing Systems, 33:12837–12848.
Minki Kang, Jin Myung Kwak, Jinheon Baek, and
Sung Ju Hwang. 2023. Knowledge graph-augmented
language models for knowledge-grounded dialogue
generation. arXiv preprint arXiv:2305.18846.
Vladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick
Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and
Wen-tau Yih. 2020.
Dense passage retrieval for
open-domain question answering. arXiv preprint
arXiv:2004.04906.
Urvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke
Zettlemoyer, and Mike Lewis. 2020.
Nearest
neighbor machine translation.
arXiv preprint
arXiv:2010.00710.
Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke
Zettlemoyer, and Mike Lewis. 2019. Generalization
through memorization: Nearest neighbor language
models. arXiv preprint arXiv:1911.00172.
Omar Khattab,
Keshav Santhanam,
Xiang Lisa
Li, David Hall, Percy Liang, Christopher Potts,
and Matei Zaharia. 2022.
Demonstrate-search-
predict: Composing retrieval and language mod-
els for knowledge-intensive nlp.
arXiv preprint
arXiv:2212.14024.
Mojtaba Komeili, Kurt Shuster, and Jason Weston. 2021.
Internet-augmented dialogue generation.
arXiv
preprint arXiv:2107.07566.
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-
field, Michael Collins, Ankur Parikh, Chris Alberti,
Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-
ton Lee, et al. 2019. Natural questions: a benchmark
for question answering research. Transactions of the
Association for Computational Linguistics, 7:453–
466.
Angeliki Lazaridou, Elena Gribovskaya, Wojciech
Stokowiec, and Nikolai Grigorev. 2022. Internet-
augmented language models through few-shot
prompting for open-domain question answering.
arXiv preprint arXiv:2203.05115.
Kenton Lee, Ming-Wei Chang, and Kristina Toutanova.
2019.
Latent retrieval for weakly supervised
open domain question answering. arXiv preprint
arXiv:1906.00300.
26
Zachary Levonian, Chenglu Li, Wangda Zhu, Anoushka
Gade, Owen Henkel, Millie-Ellen Postle, and Wanli
Xing. 2023. Retrieval-augmented generation to im-
prove math question-answering: Trade-offs between
groundedness and human preference. arXiv preprint
arXiv:2310.03184.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan
Ghazvininejad, Abdelrahman Mohamed, Omer Levy,
Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart: De-
noising sequence-to-sequence pre-training for natural
language generation, translation, and comprehension.
arXiv preprint arXiv:1910.13461.
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio
Petroni, Vladimir Karpukhin, Naman Goyal, Hein-
rich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-
täschel, et al. 2020. Retrieval-augmented generation
for knowledge-intensive nlp tasks. Advances in Neu-
ral Information Processing Systems, 33:9459–9474.
Bowen Li, Philip HS Torr, and Thomas Lukasiewicz.
2022a.
Memory-driven text-to-image generation.
arXiv preprint arXiv:2208.07022.
Huayang Li, Yixuan Su, Deng Cai, Yan Wang, and
Lemao Liu. 2022b. A survey on retrieval-augmented
text generation. arXiv preprint arXiv:2202.01110.
Sheng-Chieh Lin, Akari Asai, Minghan Li, Barlas Oguz,
Jimmy Lin, Yashar Mehdad, Wen-tau Yih, and Xilun
Chen. 2023a. How to train your dragon: Diverse
augmentation towards generalizable dense retrieval.
arXiv preprint arXiv:2302.07452.
Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi,
Maria Lomeli, Rich James, Pedro Rodriguez, Jacob
Kahn, Gergely Szilvasy, Mike Lewis, et al. 2023b.
Ra-dit: Retrieval-augmented dual instruction tuning.
arXiv preprint arXiv:2310.01352.
Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu
Wang, Shuohui Chen, Daniel Simig, Myle Ott, Na-
man Goyal, Shruti Bhosale, Jingfei Du, et al. 2022.
Few-shot learning with multilingual generative lan-
guage models. In Proceedings of the 2022 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 9019–9052.
Shangqing Liu, Yu Chen, Xiaofei Xie, Jingkai Siow, and
Yang Liu. 2020. Retrieval-augmented generation for
code summarization via hybrid gnn. arXiv preprint
arXiv:2006.05405.
Yi Liu, Lianzhe Huang, Shicheng Li, Sishuo Chen, Hao
Zhou, Fandong Meng, Jie Zhou, and Xu Sun. 2023.
Recall: A benchmark for llms robustness against
external counterfactual knowledge. arXiv preprint
arXiv:2311.08147.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap-
proach. arXiv preprint arXiv:1907.11692.
Yuxing Lu, Xiaohong Liu, Zongxin Du, Yuanxu Gao,
and Guangyu Wang. 2023a. Medkpl: a heteroge-
neous knowledge enhanced prompt learning frame-
work for transferable diagnosis. Journal of Biomedi-
cal Informatics, 143:104417.
Yuxing Lu, Xukai Zhao, and Jinzhuo Wang. 2023b.
Medical knowledge-enhanced prompt learning for di-
agnosis classification from clinical text. In Proceed-
ings of the 5th Clinical Natural Language Processing
Workshop, pages 278–288.
Hongyin Luo, Yung-Sung Chuang, Yuan Gong, Tian-
hua Zhang, Yoon Kim, Xixin Wu, Danny Fox, He-
len Meng, and James Glass. 2023a. Sail: Search-
augmented instruction learning.
arXiv preprint
arXiv:2305.15225.
Ziyang Luo, Can Xu, Pu Zhao, Xiubo Geng, Chongyang
Tao, Jing Ma, Qingwei Lin, and Daxin Jiang.
2023b.
Augmented large language models with
parametric knowledge guiding.
arXiv preprint
arXiv:2305.04757.
Yuanjie Lyu, Zhiyu Li, Simin Niu, Feiyu Xiong,
Bo Tang, Wenjin Wang, Hao Wu, Huanyong Liu,
Tong Xu, and Enhong Chen. 2024.
Crud-rag:
A comprehensive chinese benchmark for retrieval-
augmented generation of large language models.
arXiv preprint arXiv:2401.17043.
Aman Madaan, Niket Tandon, Peter Clark, and Yim-
ing Yang. 2022. Memory-assisted prompt editing
to improve gpt-3 after deployment. arXiv preprint
arXiv:2201.06009.
Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,
Daniel Khashabi, and Hannaneh Hajishirzi. 2022.
When not to trust language models: Investigating
effectiveness of parametric and non-parametric mem-
ories. arXiv preprint arXiv:2212.10511.
Yuning Mao, Pengcheng He, Xiaodong Liu, Ye-
long Shen, Jianfeng Gao, Jiawei Han, and Weizhu
Chen. 2020.
Generation-augmented retrieval for
open-domain question answering. arXiv preprint
arXiv:2009.08553.
Seyed Mahed Mousavi, Simone Alghisi, and Giuseppe
Riccardi. 2024. Is your llm outdated? benchmark-
ing llms & alignment algorithms for time-sensitive
knowledge. arXiv preprint arXiv:2404.08700.
Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff
Wu, Long Ouyang, Christina Kim, Christopher
Hesse, Shantanu Jain, Vineet Kosaraju, William
Saunders, et al. 2021. Webgpt: Browser-assisted
question-answering with human feedback, 2021.
URL https://arxiv. org/abs/2112.09332.
Daye Nam, Andrew Macvean, Vincent Hellendoorn,
Bogdan Vasilescu, and Brad Myers. 2024. Using an
llm to help with code understanding. In Proceedings
of the IEEE/ACM 46th International Conference on
Software Engineering, pages 1–13.
27
Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gus-
tavo Hernández Ábrego, Ji Ma, Vincent Y Zhao,
Yi Luan, Keith B Hall, Ming-Wei Chang, et al.
2021. Large dual encoders are generalizable retriev-
ers. arXiv preprint arXiv:2112.07899.
Md Rizwan Parvez, Wasi Uddin Ahmad, Saikat
Chakraborty, Baishakhi Ray, and Kai-Wei Chang.
2021. Retrieval augmented code generation and sum-
marization. arXiv preprint arXiv:2108.11601.
Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng,
Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou
Yu, Weizhu Chen, et al. 2023. Check your facts and
try again: Improving large language models with
external knowledge and automated feedback. arXiv
preprint arXiv:2302.12813.
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya
Sutskever, et al. 2018. Improving language under-
standing by generative pre-training.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. 2019. Language
models are unsupervised multitask learners. OpenAI
blog, 1(8):9.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J Liu. 2020. Exploring the lim-
its of transfer learning with a unified text-to-text
transformer. Journal of machine learning research,
21(140):1–67.
Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,
Amnon Shashua, Kevin Leyton-Brown, and Yoav
Shoham. 2023. In-context retrieval-augmented lan-
guage models. Transactions of the Association for
Computational Linguistics, 11:1316–1331.
Ori Ram, Gal Shachaf, Omer Levy, Jonathan Be-
rant, and Amir Globerson. 2021. Learning to re-
trieve passages without supervision. arXiv preprint
arXiv:2112.07708.
Prajit Ramachandran, Barret Zoph, and Quoc V Le.
2017.
Searching for activation functions.
arXiv
preprint arXiv:1710.05941.
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey
Chu, and Mark Chen. 2022.
Hierarchical text-
conditional image generation with clip latents. arXiv
preprint arXiv:2204.06125, 1(2):3.
Juan Ramos et al. 2003. Using tf-idf to determine word
relevance in document queries. In Proceedings of the
first instructional conference on machine learning,
volume 242, pages 29–48. Citeseer.
Stephen E Robertson, Steve Walker, Susan Jones,
Micheline M Hancock-Beaulieu, Mike Gatford, et al.
1995. Okapi at trec-3. Nist Special Publication Sp,
109:109.
Bernardino
Romera-Paredes,
Mohammadamin
Barekatain,
Alexander Novikov,
Matej Balog,
M Pawan Kumar, Emilien Dupont, Francisco JR
Ruiz, Jordan S Ellenberg, Pengming Wang, Omar
Fawzi, et al. 2024. Mathematical discoveries from
program search with large language models. Nature,
625(7995):468–475.
Md Rashad Al Hasan Rony, Ricardo Usbeck, and Jens
Lehmann. 2022.
Dialokg: Knowledge-structure
aware task-oriented dialogue generation.
arXiv
preprint arXiv:2204.09149.
Jon Saad-Falcon, Omar Khattab, Christopher Potts, and
Matei Zaharia. 2023. Ares: An automated evalua-
tion framework for retrieval-augmented generation
systems. arXiv preprint arXiv:2311.09476.
Devendra Singh Sachan, Mostofa Patwary, Mohammad
Shoeybi, Neel Kant, Wei Ping, William L Hamilton,
and Bryan Catanzaro. 2021. End-to-end training of
neural retrievers for open-domain question answering.
arXiv preprint arXiv:2101.00408.
Victor Sanh, Lysandre Debut, Julien Chaumond, and
Thomas Wolf. 2019. Distilbert, a distilled version
of bert: smaller, faster, cheaper and lighter. arXiv
preprint arXiv:1910.01108.
Keshav Santhanam, Omar Khattab, Jon Saad-Falcon,
Christopher Potts, and Matei Zaharia. 2021. Col-
bertv2:
Effective
and
efficient
retrieval
via
lightweight late interaction.
arXiv preprint
arXiv:2112.01488.
Apoorv Saxena, Adrian Kochsiek, and Rainer Gemulla.
2022. Sequence-to-sequence knowledge graph com-
pletion and question answering.
arXiv preprint
arXiv:2203.10321.
Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta
Raileanu, Maria Lomeli, Eric Hambro, Luke Zettle-
moyer, Nicola Cancedda, and Thomas Scialom. 2024.
Toolformer: Language models can teach themselves
to use tools. Advances in Neural Information Pro-
cessing Systems, 36.
Ivo Serra, Rosario Girardi, and Paulo Novais. 2013.
Parnt: a statistic based approach to extract non-
taxonomic relationships of ontologies from text. In
2013 10th International Conference on Informa-
tion Technology: New Generations, pages 561–566.
IEEE.
Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie
Huang, Nan Duan, and Weizhu Chen. 2023. Enhanc-
ing retrieval-augmented large language models with
iterative retrieval-generation synergy. arXiv preprint
arXiv:2305.15294.
Noam Shazeer. 2020. Glu variants improve transformer.
arXiv preprint arXiv:2002.05202.
Shelly Sheynin, Oron Ashual, Adam Polyak, Uriel
Singer, Oran Gafni, Eliya Nachmani, and Yaniv
28
Taigman. 2022.
Knn-diffusion:
Image gener-
ation via large-scale retrieval.
arXiv preprint
arXiv:2204.02849.
Ensheng Shi, Yanlin Wang, Wei Tao, Lun Du, Hongyu
Zhang, Shi Han, Dongmei Zhang, and Hongbin Sun.
2022. Race: Retrieval-augmented commit message
generation. arXiv preprint arXiv:2203.02700.
Freda Shi, Xinyun Chen, Kanishka Misra, Nathan
Scales, David Dohan, Ed H Chi, Nathanael Schärli,
and Denny Zhou. 2023a. Large language models
can be easily distracted by irrelevant context. In In-
ternational Conference on Machine Learning, pages
31210–31227. PMLR.
Weijia Shi, Sewon Min, Michihiro Yasunaga, Min-
joon Seo, Rich James, Mike Lewis, Luke Zettle-
moyer, and Wen-tau Yih. 2023b. Replug: Retrieval-
augmented black-box language models.
arXiv
preprint arXiv:2301.12652.
Devendra Singh, Siva Reddy, Will Hamilton, Chris
Dyer, and Dani Yogatama. 2021. End-to-end train-
ing of multi-document reader and retriever for open-
domain question answering. Advances in Neural
Information Processing Systems, 34:25968–25981.
Shamane Siriwardhana, Rivindu Weerasekera, Elliott
Wen, Tharindu Kaluarachchi, Rajib Rana, and
Suranga Nanayakkara. 2023. Improving the domain
adaptation of retrieval augmented generation (rag)
models for open domain question answering. Trans-
actions of the Association for Computational Linguis-
tics, 11:1–17.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: a simple way to prevent neural networks
from overfitting. The journal of machine learning
research, 15(1):1929–1958.
Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan,
Wen Bo, and Yunfeng Liu. 2024. Roformer: En-
hanced transformer with rotary position embedding.
Neurocomputing, 568:127063.
Wilson L Taylor. 1953.
“cloze procedure”: A new
tool for measuring readability. Journalism quarterly,
30(4):415–433.
James
Thorne,
Andreas
Vlachos,
Christos
Christodoulopoulos,
and
Arpit
Mittal.
2018.
Fever: a large-scale dataset for fact extraction and
verification. arXiv preprint arXiv:1803.05355.
David Thulke, Nico Daheim, Christian Dugast, and
Hermann Ney. 2021. Efficient retrieval augmented
generation from unstructured knowledge for task-
oriented dialog. arXiv preprint arXiv:2102.04643.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, et al. 2023a.
Llama:
Open and effi-
cient foundation language models. arXiv preprint
arXiv:2302.13971.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, et al. 2023b.
Llama 2: Open founda-
tion and fine-tuned chat models.
arXiv preprint
arXiv:2307.09288.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing
systems, 30.
Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and
Pierre-Antoine Manzagol. 2008. Extracting and com-
posing robust features with denoising autoencoders.
In Proceedings of the 25th international conference
on Machine learning, pages 1096–1103.
Boxin Wang, Wei Ping, Lawrence McAfee, Peng
Xu, Bo Li, Mohammad Shoeybi, and Bryan Catan-
zaro. 2023a. Instructretro: Instruction tuning post
retrieval-augmented pretraining.
arXiv preprint
arXiv:2310.07713.
Liang Wang, Nan Yang, and Furu Wei. 2023b. Learning
to retrieve in-context examples for large language
models. arXiv preprint arXiv:2307.07164.
Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan
Parvez, and Graham Neubig. 2023c. Learning to fil-
ter context for retrieval-augmented generation. arXiv
preprint arXiv:2311.08377.
BigScience Workshop, Teven Le Scao, Angela Fan,
Christopher Akiki, Ellie Pavlick, Suzana Ili´c, Daniel
Hesslow, Roman Castagné, Alexandra Sasha Luc-
cioni, François Yvon, et al. 2022. Bloom: A 176b-
parameter open-access multilingual language model.
arXiv preprint arXiv:2211.05100.
Guangzhi
Xiong,
Qiao
Jin,
Zhiyong
Lu,
and
Aidong Zhang. 2024.
Benchmarking retrieval-
augmented generation for medicine. arXiv preprint
arXiv:2402.13178.
Fangyuan Xu, Weijia Shi, and Eunsol Choi. 2023. Re-
comp: Improving retrieval-augmented lms with com-
pression and selective augmentation. arXiv preprint
arXiv:2310.04408.
Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling.
2024.
Corrective retrieval augmented generation.
arXiv preprint arXiv:2401.15884.
Haoyan Yang, Zhitao Li, Yong Zhang, Jianzong Wang,
Ning Cheng, Ming Li, and Jing Xiao. 2023. Prca:
Fitting black-box large language models for retrieval
question answering via pluggable reward-driven con-
textual adapter. arXiv preprint arXiv:2310.18347.
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-
gio, William W Cohen, Ruslan Salakhutdinov, and
Christopher D Manning. 2018. Hotpotqa: A dataset
for diverse, explainable multi-hop question answer-
ing. arXiv preprint arXiv:1809.09600.
29
Lirong Yao and Yazhuo Guan. 2018. An improved
lstm structure for natural language processing. In
2018 IEEE international conference of safety produce
informatization (IICSPI), pages 565–569. IEEE.
Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi,
Rich James, Jure Leskovec, Percy Liang, Mike Lewis,
Luke Zettlemoyer, and Wen-tau Yih. 2022. Retrieval-
augmented multimodal language modeling. arXiv
preprint arXiv:2211.12561.
Haoran Ye, Jiarui Wang, Zhiguang Cao, and Guojie
Song. 2024. Reevo: Large language models as hyper-
heuristics with reflective evolution. arXiv preprint
arXiv:2402.01145.
Chengxiang Yin, Jian Tang, Zhiyuan Xu, and Yanzhi
Wang. 2019. Memory augmented deep recurrent
neural network for video question answering. IEEE
transactions on neural networks and learning sys-
tems, 31(9):3159–3167.
Wenpeng Yin, Katharina Kann, Mo Yu, and Hinrich
Schütze. 2017. Comparative study of cnn and rnn
for natural language processing.
arXiv preprint
arXiv:1702.01923.
Dani Yogatama, Cyprien de Masson d’Autume, and
Lingpeng Kong. 2021. Adaptive semiparametric lan-
guage models. Transactions of the Association for
Computational Linguistics, 9:362–373.
Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan
Berant. 2023. Making retrieval-augmented language
models robust to irrelevant context. arXiv preprint
arXiv:2310.01558.
Donghan Yu and Yiming Yang. 2023.
Retrieval-
enhanced generative model for large-scale knowl-
edge graph completion. In Proceedings of the 46th
International ACM SIGIR Conference on Research
and Development in Information Retrieval, pages
2334–2338.
Wenhao Yu, Zhihan Zhang, Zhenwen Liang, Meng
Jiang, and Ashish Sabharwal. 2023. Improving lan-
guage models via plug-and-play retrieval feedback.
arXiv preprint arXiv:2305.14002.
Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D
Plumbley, and Wenwu Wang. 2024.
Retrieval-
augmented text-to-audio generation.
In ICASSP
2024-2024 IEEE International Conference on Acous-
tics, Speech and Signal Processing (ICASSP), pages
581–585. IEEE.
Mert Yuksekgonul, Varun Chandrasekaran, Erik Jones,
Suriya Gunasekar, Ranjita Naik, Hamid Palangi, Ece
Kamar, and Besmira Nushi. 2023. Attention satis-
fies: A constraint-satisfaction lens on factual errors of
language models. arXiv preprint arXiv:2309.15098.
Daoguang Zan, Bei Chen, Zeqi Lin, Bei Guan, Yongji
Wang, and Jian-Guang Lou. 2022.
When lan-
guage model meets private library. arXiv preprint
arXiv:2210.17236.
Biao Zhang and Rico Sennrich. 2019. Root mean square
layer normalization. Advances in Neural Information
Processing Systems, 32.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Artetxe, Moya Chen, Shuohui Chen, Christopher De-
wan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.
Opt: Open pre-trained transformer language models.
arXiv preprint arXiv:2205.01068.
Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren
Wang, Yunteng Geng, Fangcheng Fu, Ling Yang,
Wentao Zhang, and Bin Cui. 2024a.
Retrieval-
augmented generation for ai-generated content: A
survey. arXiv preprint arXiv:2402.19473.
Ruochen Zhao, Hailin Chen, Weishi Wang, Fangkai
Jiao, Xuan Long Do, Chengwei Qin, Bosheng
Ding, Xiaobao Guo, Minzhi Li, Xingxuan Li, et al.
2023. Retrieving multimodal information for aug-
mented generation:
A survey.
arXiv preprint
arXiv:2303.10868.
Weichen Zhao, Yuxing Lu, Ge Jiao, and Yuan Yang.
2024b.
Concentrated reasoning and unified re-
construction for multi-modal media manipulation.
In ICASSP 2024-2024 IEEE International Confer-
ence on Acoustics, Speech and Signal Processing
(ICASSP), pages 8190–8194. IEEE.
Weichen Zhao, Yuxing Lu, Ge Jiao, and Yuan Yang.
2024c. Dual-color granularity alignment for text-
based person search. In ICASSP 2024-2024 IEEE In-
ternational Conference on Acoustics, Speech and Sig-
nal Processing (ICASSP), pages 8075–8079. IEEE.
Zexuan Zhong, Tao Lei, and Danqi Chen. 2022. Train-
ing language models with memory augmentation.
arXiv preprint arXiv:2205.12674.
30
